

===== FILE: backend/app/api/__init__.py =====


===== FILE: backend/app/api/v1/api.py =====
     1	from __future__ import annotations
     2	from app.api.v1.endpoints import rfq as rfq_endpoint
     3	from fastapi import APIRouter
     4	
     5	from app.api.v1.endpoints import (
     6	    ai,
     7	    auth,
     8	    chat_stream,
     9	    chat_ws,
    10	    consult_invoke,
    11	    langgraph_sse,
    12	    memory,
    13	    system,
    14	    users,
    15	    sse_test,        # ⬅️ NEU
    16	)
    17	
    18	api_router = APIRouter()
    19	
    20	# SSE
    21	api_router.include_router(langgraph_sse.router, prefix="/langgraph", tags=["langgraph"])
    22	api_router.include_router(chat_stream.router, tags=["sse"])
    23	api_router.include_router(sse_test.router, prefix="/sse", tags=["sse"])  # ⬅️ NEU → /api/v1/sse/test
    24	
    25	# WebSocket (ohne extra Prefix → /api/v1/ai/ws)
    26	api_router.include_router(chat_ws.router, tags=["ws"])
    27	
    28	# Sync-Invoke (Debug)
    29	api_router.include_router(consult_invoke.router, tags=["test"])
    30	
    31	# REST
    32	api_router.include_router(ai.router, prefix="/ai")   # ⬅️ Prefix ergänzt → /api/v1/ai/beratung
    33	api_router.include_router(auth.router)
    34	api_router.include_router(memory.router)
    35	api_router.include_router(system.router)
    36	api_router.include_router(users.router)
    37	
    38	api_router.include_router(rfq_endpoint.router, prefix="/rfq", tags=["rfq"])


===== FILE: backend/app/api/v1/dependencies/auth.py =====
     1	import os
     2	import time
     3	import logging
     4	from typing import Optional, Tuple, Iterable
     5	import urllib.parse as urlparse
     6	
     7	import httpx
     8	from jose import jwt, jwk
     9	from jose.utils import base64url_decode
    10	from fastapi import WebSocket
    11	
    12	logger = logging.getLogger("auth")
    13	
    14	# ---- Config / ENV ------------------------------------------------------------
    15	
    16	def _csv(env: str) -> Iterable[str]:
    17	    raw = (os.getenv(env) or "").strip()
    18	    return [x.strip() for x in raw.split(",") if x.strip()]
    19	
    20	def _norm_url(u: str) -> str:
    21	    """Normalize for strict compare: lowercase scheme/host, strip trailing slash."""
    22	    if not u:
    23	        return ""
    24	    try:
    25	        p = urlparse.urlparse(u.strip())
    26	        scheme = (p.scheme or "https").lower()
    27	        host   = (p.hostname or "").lower()
    28	        port   = f":{p.port}" if p.port else ""
    29	        path   = (p.path or "").rstrip("/")
    30	        return f"{scheme}://{host}{port}{path}"
    31	    except Exception:
    32	        return u.strip().rstrip("/").lower()
    33	
    34	REALM_URL  = os.getenv("KEYCLOAK_REALM_URL", "https://auth.sealai.net/realms/sealAI")
    35	ISSUER_ENV = os.getenv("KEYCLOAK_ISSUER", REALM_URL)
    36	ALLOWED_ISSUERS = {ISSUER_ENV, REALM_URL, *_csv("KEYCLOAK_ALLOWED_ISSUERS")}
    37	ALLOWED_ISSUERS_NORM = {_norm_url(x) for x in ALLOWED_ISSUERS if x}
    38	
    39	JWKS_URL = (
    40	    os.getenv("KEYCLOAK_JWKS_URL")
    41	    or f"{_norm_url(ISSUER_ENV)}/protocol/openid-connect/certs"
    42	)
    43	
    44	ALLOWED_AUDIENCES = set(_csv("ALLOWED_AUDIENCES") or ["account", "nextauth", "sealai-backend-api"])
    45	
    46	JWKS_TTL_SEC = int(os.getenv("JWKS_TTL_SEC", "600"))
    47	CLOCK_SKEW   = int(os.getenv("WS_CLOCK_SKEW_LEEWAY", "120"))
    48	
    49	# ---- JWKS cache with TTL -----------------------------------------------------
    50	
    51	_JWKS_CACHE = {"data": None, "ts": 0.0}
    52	
    53	def _get_jwks_cached() -> dict:
    54	    now = time.time()
    55	    if _JWKS_CACHE["data"] and (now - _JWKS_CACHE["ts"] < JWKS_TTL_SEC):
    56	        return _JWKS_CACHE["data"]  # type: ignore[return-value]
    57	    logger.info("Fetching JWKS from %s", JWKS_URL)
    58	    with httpx.Client(timeout=10.0) as client:
    59	        r = client.get(JWKS_URL)
    60	        r.raise_for_status()
    61	        _JWKS_CACHE["data"] = r.json()
    62	        _JWKS_CACHE["ts"] = now
    63	        return _JWKS_CACHE["data"]  # type: ignore[return-value]
    64	
    65	def _jwks_clear():
    66	    _JWKS_CACHE["data"] = None
    67	    _JWKS_CACHE["ts"] = 0.0
    68	
    69	def _find_key(kid: str) -> Optional[dict]:
    70	    jwks = _get_jwks_cached()
    71	    for key in jwks.get("keys", []):
    72	        if key.get("kid") == kid:
    73	            return key
    74	    # miss? refresh once
    75	    _jwks_clear()
    76	    jwks = _get_jwks_cached()
    77	    for key in jwks.get("keys", []):
    78	        if key.get("kid") == kid:
    79	            return key
    80	    return None
    81	
    82	# ---- Token verification ------------------------------------------------------
    83	
    84	def _verify_rs256(token: str) -> dict:
    85	    try:
    86	        headers = jwt.get_unverified_header(token)
    87	        payload = jwt.get_unverified_claims(token)
    88	    except Exception as e:
    89	        raise ValueError(f"invalid token format: {e}")
    90	
    91	    kid = headers.get("kid")
    92	    if not kid:
    93	        raise ValueError("token header missing 'kid'")
    94	
    95	    key_dict = _find_key(kid)
    96	    if not key_dict:
    97	        raise ValueError("jwks key not found for kid")
    98	
    99	    # signature
   100	    signing_input, signature = token.rsplit(".", 1)
   101	    signature_bytes = base64url_decode(signature.encode())
   102	    public_key = jwk.construct(key_dict)
   103	    if not public_key.verify(signing_input.encode(), signature_bytes):
   104	        raise ValueError("signature verification failed")
   105	
   106	    # temporal claims (with skew)
   107	    now = int(time.time())
   108	    exp = int(payload.get("exp", 0) or 0)
   109	    nbf = int(payload.get("nbf", 0) or 0)
   110	    if nbf and now + CLOCK_SKEW < nbf:
   111	        raise ValueError("token not yet valid (nbf)")
   112	    if exp and now - CLOCK_SKEW >= exp:
   113	        raise ValueError("token expired")
   114	
   115	    # issuer (case/host normalized)
   116	    iss = payload.get("iss")
   117	    if _norm_url(iss or "") not in ALLOWED_ISSUERS_NORM:
   118	        raise ValueError(f"invalid issuer: {iss}")
   119	
   120	    # audience / azp fallback
   121	    aud = payload.get("aud")
   122	    aud_ok = False
   123	    if isinstance(aud, str):
   124	        aud_ok = aud in ALLOWED_AUDIENCES
   125	    elif isinstance(aud, (list, tuple, set)):
   126	        aud_ok = any(a in ALLOWED_AUDIENCES for a in aud)
   127	    if not aud_ok:
   128	        azp = payload.get("azp") or payload.get("client_id")
   129	        if not azp or azp not in ALLOWED_AUDIENCES:
   130	            raise ValueError(f"aud not allowed: {aud}")
   131	
   132	    return payload
   133	
   134	# ---- WS helpers --------------------------------------------------------------
   135	
   136	from fastapi import WebSocket
   137	
   138	def extract_bearer_or_query_token(websocket: WebSocket) -> Optional[str]:
   139	    auth = websocket.headers.get("authorization") or websocket.headers.get("Authorization")
   140	    if auth and auth.lower().startswith("bearer "):
   141	        return auth.split(" ", 1)[1].strip()
   142	    token = websocket.query_params.get("token")
   143	    if token:
   144	        return token.strip()
   145	    return None
   146	
   147	def _truthy(v: Optional[str]) -> bool:
   148	    if v is None:
   149	        return False
   150	    v = str(v).strip().strip('\'"').lower()
   151	    return v in ("1", "true", "yes", "on")
   152	
   153	def _allowed_origins_set() -> set[str]:
   154	    raw = (os.getenv("ALLOWED_ORIGIN", "") or "").strip().strip('\'"')
   155	    return {o.strip() for o in raw.split(",") if o.strip()}
   156	
   157	def check_origin_allowed(origin: Optional[str]) -> Tuple[bool, str]:
   158	    if _truthy(os.getenv("ALLOW_WS_ORIGIN_ANY", "0")):
   159	        return True, "ALLOW_WS_ORIGIN_ANY=1"
   160	    if not origin and not _truthy(os.getenv("WS_REQUIRE_ORIGIN", "0")):
   161	        return True, "no Origin header (allowed)"
   162	    if not origin:
   163	        return False, "missing Origin header"
   164	    allowed = _allowed_origins_set()
   165	    if not allowed:
   166	        return False, "ALLOWED_ORIGIN not configured"
   167	    if "*" in allowed or "any" in allowed:
   168	        return True, "ALLOWED_ORIGIN=*"
   169	    if origin in allowed:
   170	        return True, "origin ok"
   171	    return False, f"origin '{origin}' not in {sorted(allowed)}"
   172	
   173	def token_allows_origin(payload: dict, origin: Optional[str]) -> Tuple[bool, str]:
   174	    if not _truthy(os.getenv("WS_ENFORCE_TOKEN_ORIGIN", "0")):
   175	        return True, "token-origin check disabled"
   176	    if not origin:
   177	        return True, "skip claim check (no origin)"
   178	    claim = payload.get("allowed-origins")
   179	    if not claim:
   180	        return True, "no allowed-origins claim; skipping"
   181	    if isinstance(claim, list) and origin in claim:
   182	        return True, "allowed-origins claim ok"
   183	    return False, f"origin '{origin}' not in token allowed-origins"
   184	
   185	def verify_token_or_raise(token: str) -> dict:
   186	    try:
   187	        return _verify_rs256(token)
   188	    except Exception as e:
   189	        logger.warning("JWT verify failed: %s", e)
   190	        raise
   191	
   192	async def guard_websocket(websocket: WebSocket) -> dict:
   193	    origin = websocket.headers.get("origin")
   194	
   195	    ok, _ = check_origin_allowed(origin)
   196	    if not ok:
   197	        await websocket.close(code=1008)
   198	        raise RuntimeError("forbidden origin")
   199	
   200	    token = extract_bearer_or_query_token(websocket)
   201	    if not token:
   202	        await websocket.close(code=1008)
   203	        raise RuntimeError("missing bearer token")
   204	
   205	    try:
   206	        payload = verify_token_or_raise(token)
   207	    except Exception:
   208	        await websocket.close(code=1008)
   209	        raise
   210	
   211	    ok2, _ = token_allows_origin(payload, origin)
   212	    if not ok2:
   213	        await websocket.close(code=1008)
   214	        raise RuntimeError("token origin mismatch")
   215	
   216	    websocket.scope["user"] = payload
   217	    return payload


===== FILE: backend/app/api/v1/dependencies/__init__.py =====


===== FILE: backend/app/api/v1/endpoints/ai.py =====
     1	# backend/app/api/v1/endpoints/ai.py
     2	from __future__ import annotations
     3	
     4	import os
     5	import re
     6	from typing import Optional
     7	
     8	from fastapi import APIRouter, HTTPException, Request
     9	from pydantic import BaseModel, Field
    10	from redis import Redis
    11	
    12	# Nur die Consult-Funktion nutzen; Checkpointer holen wir aus app.state
    13	from app.services.langgraph.graph.consult.io import invoke_consult as _invoke_consult
    14	
    15	# ─────────────────────────────────────────────────────────────
    16	# ENV / Redis STM (Short-Term Memory)
    17	# ─────────────────────────────────────────────────────────────
    18	REDIS_URL = os.getenv("REDIS_URL", "redis://redis:6379/0")
    19	STM_PREFIX = os.getenv("STM_PREFIX", "chat:stm")
    20	STM_TTL_SEC = int(os.getenv("STM_TTL_SEC", "604800"))  # 7 Tage
    21	
    22	def _stm_key(thread_id: str) -> str:
    23	    return f"{STM_PREFIX}:{thread_id}"
    24	
    25	def _get_redis() -> Redis:
    26	    return Redis.from_url(REDIS_URL, decode_responses=True)
    27	
    28	def _set_stm(thread_id: str, key: str, value: str) -> None:
    29	    r = _get_redis()
    30	    skey = _stm_key(thread_id)
    31	    r.hset(skey, key, value)
    32	    r.expire(skey, STM_TTL_SEC)
    33	
    34	def _get_stm(thread_id: str, key: str) -> Optional[str]:
    35	    r = _get_redis()
    36	    skey = _stm_key(thread_id)
    37	    v = r.hget(skey, key)
    38	    return v if (isinstance(v, str) and v.strip()) else None
    39	
    40	# ─────────────────────────────────────────────────────────────
    41	# Intent: “merke dir … / remember …” (optional)
    42	# ─────────────────────────────────────────────────────────────
    43	RE_REMEMBER_NUM  = re.compile(r"\b(merke\s*dir|merk\s*dir|remember)\b[^0-9\-+]*?(-?\d+(?:[.,]\d+)?)", re.I)
    44	RE_REMEMBER_FREE = re.compile(r"\b(merke\s*dir|merk\s*dir|remember)\b[:\s]+(.+)$", re.I)
    45	RE_ASK_NUMBER    = re.compile(r"\b(welche\s+zahl\s+meinte\s+ich|what\s+number\s+did\s+i\s+mean)\b", re.I)
    46	RE_ASK_FREE      = re.compile(r"\b(woran\s+erinn?erst\s+du\s+dich|what\s+did\s+you\s+remember)\b", re.I)
    47	
    48	def _normalize_num_str(s: str) -> str:
    49	    return (s or "").replace(",", ".")
    50	
    51	def _maybe_handle_memory_intent(text: str, thread_id: str) -> Optional[str]:
    52	    t = (text or "").strip()
    53	    if not t:
    54	        return None
    55	
    56	    m = RE_REMEMBER_NUM.search(t)
    57	    if m:
    58	        raw = m.group(2)
    59	        norm = _normalize_num_str(raw)
    60	        _set_stm(thread_id, "last_number", norm)
    61	        return f"Alles klar – ich habe mir **{raw}** gemerkt."
    62	
    63	    m2 = RE_REMEMBER_FREE.search(t)
    64	    if m2 and not m:
    65	        val = (m2.group(2) or "").strip()
    66	        if val:
    67	            _set_stm(thread_id, "last_note", val)
    68	            return "Notiert. 👍"
    69	
    70	    if RE_ASK_NUMBER.search(t):
    71	        v = _get_stm(thread_id, "last_number")
    72	        return f"Du meintest **{v}**." if v else "Ich habe dazu noch keine Zahl gespeichert."
    73	
    74	    if RE_ASK_FREE.search(t):
    75	        v = _get_stm(thread_id, "last_note")
    76	        return f"Ich habe mir gemerkt: “{v}”." if v else "Ich habe dazu noch nichts gespeichert."
    77	
    78	    return None
    79	
    80	# ─────────────────────────────────────────────────────────────
    81	# API
    82	# ─────────────────────────────────────────────────────────────
    83	router = APIRouter()  # KEIN prefix hier – der übergeordnete Router hängt '/ai' an.
    84	
    85	class ChatRequest(BaseModel):
    86	    chat_id: str = Field(default="default", description="Konversations-ID")
    87	    input_text: str = Field(..., description="Nutzertext")
    88	
    89	class ChatResponse(BaseModel):
    90	    text: str
    91	
    92	@router.post("/beratung", response_model=ChatResponse)
    93	async def beratung(request: Request, payload: ChatRequest) -> ChatResponse:
    94	    """
    95	    Einstieg in den Consult-Flow. Nutzt (falls vorhanden) den Checkpointer aus app.state.
    96	    Zusätzlich: einfache STM-Merkfunktion (merke dir … / welche Zahl …?).
    97	    """
    98	    user_text = (payload.input_text or "").strip()
    99	    if not user_text:
   100	        raise HTTPException(status_code=400, detail="input_text empty")
   101	
   102	    thread_id = f"api:{payload.chat_id}"
   103	
   104	    # 1) Memory-Intents kurz-circuited beantworten
   105	    mem = _maybe_handle_memory_intent(user_text, thread_id)
   106	    if mem:
   107	        return ChatResponse(text=mem)
   108	
   109	    # 2) Consult-Flow aufrufen (mit optionalem Checkpointer)
   110	    checkpointer = getattr(request.app.state, "swarm_checkpointer", None)
   111	    out = _invoke_consult(user_text, thread_id=thread_id, checkpointer=checkpointer)
   112	    return ChatResponse(text=out)


===== FILE: backend/app/api/v1/endpoints/auth.py =====
     1	from fastapi import APIRouter
     2	from fastapi.responses import RedirectResponse
     3	from urllib.parse import urlencode
     4	
     5	router = APIRouter()
     6	
     7	@router.get("/login", tags=["Auth"])
     8	def login_redirect():
     9	    """
    10	    Leitet den Benutzer zum Keycloak-Login weiter.
    11	    Die Parameter müssen mit der Konfiguration deines Keycloak-Clients übereinstimmen.
    12	    """
    13	    keycloak_base_url = "https://auth.sealai.net/realms/sealAI/protocol/openid-connect/auth"
    14	    # Ersetze diese Werte mit deinen konfigurierten Angaben:
    15	    client_id = "nextauth"  # oder "sealai-backend", je nachdem, was du in Keycloak als Client definiert hast
    16	    redirect_uri = "https://sealai.net/api/auth/callback/keycloak"  # muss zu deinen Keycloak-Redirect-URIs passen
    17	    response_type = "code"
    18	    scope = "openid"
    19	
    20	    params = {
    21	        "client_id": client_id,
    22	        "response_type": response_type,
    23	        "redirect_uri": redirect_uri,
    24	        "scope": scope
    25	    }
    26	
    27	    # Erzeuge die vollständige URL
    28	    url = f"{keycloak_base_url}?{urlencode(params)}"
    29	    return RedirectResponse(url)


===== FILE: backend/app/api/v1/endpoints/chat_stream.py =====
     1	# backend/app/api/v1/endpoints/chat_stream.py
     2	from __future__ import annotations
     3	
     4	import asyncio
     5	import contextlib
     6	import json
     7	import os
     8	from typing import Any, Dict, List, Optional, Tuple, AsyncGenerator
     9	
    10	from fastapi import APIRouter, Request
    11	from langchain_core.messages import HumanMessage
    12	from langchain_core.messages.ai import AIMessageChunk
    13	from starlette.responses import StreamingResponse
    14	
    15	router = APIRouter()
    16	
    17	COALESCE_MIN_CHARS = int(os.getenv("SSE_COALESCE_MIN_CHARS", os.getenv("WS_COALESCE_MIN_CHARS", "32")))
    18	COALESCE_MAX_LAT_MS = float(os.getenv("SSE_COALESCE_MAX_LAT_MS", os.getenv("WS_COALESCE_MAX_LAT_MS", "80")))
    19	IDLE_TIMEOUT_SEC = int(os.getenv("SSE_IDLE_TIMEOUT_SEC", os.getenv("WS_STREAM_IDLE_TIMEOUT_SEC", "20")))
    20	FIRST_TOKEN_TIMEOUT_MS = int(os.getenv("SSE_FIRST_TOKEN_TIMEOUT_MS", os.getenv("WS_STREAM_FIRST_TOKEN_TIMEOUT_MS", "900")))
    21	FLUSH_ENDINGS: Tuple[str, ...] = (". ", "? ", "! ", "\n\n", ":", ";", "…", "–", ", ")
    22	
    23	
    24	def _piece_from_llm_chunk(chunk: Any) -> Optional[str]:
    25	    if isinstance(chunk, AIMessageChunk):
    26	        return chunk.content or ""
    27	    txt = getattr(chunk, "content", None)
    28	    if isinstance(txt, str):
    29	        return txt
    30	    ak = getattr(chunk, "additional_kwargs", None)
    31	    if isinstance(ak, dict):
    32	        for k in ("delta", "content", "text", "token"):
    33	            v = ak.get(k)
    34	            if isinstance(v, str) and v:
    35	                return v
    36	    if isinstance(chunk, dict):
    37	        for k in ("delta", "content", "text", "token"):
    38	            v = chunk.get(k)
    39	            if isinstance(v, str) and v:
    40	                return v
    41	    return None
    42	
    43	
    44	def _sse(payload: Dict[str, Any]) -> str:
    45	    return f"data: {json.dumps(payload, ensure_ascii=False)}\n\n"
    46	
    47	
    48	def _sse_event(event: str, payload: Dict[str, Any]) -> str:
    49	    return f"event: {event}\n" + _sse(payload)
    50	
    51	
    52	# Pfad geändert, um Konflikt mit langgraph_sse zu vermeiden
    53	@router.post("/langgraph/chat/stream2")
    54	async def sse_stream(request: Request, payload: Dict[str, Any]) -> StreamingResponse:
    55	    chat_id = str(payload.get("chat_id") or "default")
    56	    user_input = str(payload.get("input") or payload.get("input_text") or "")
    57	
    58	    async def publisher() -> AsyncGenerator[bytes, None]:
    59	        yield _sse({"phase": "starting"}).encode("utf-8")
    60	
    61	        llm = getattr(request.app.state, "llm", None)
    62	
    63	        if llm is None or not user_input.strip():
    64	            g_sync = getattr(request.app.state, "graph_sync", None)
    65	            if g_sync is None:
    66	                yield _sse_event("done", {}).encode("utf-8")
    67	                return
    68	            try:
    69	                res = g_sync.invoke({"messages": [HumanMessage(content=user_input)]})
    70	                text = ""
    71	                for m in res.get("messages", []):
    72	                    if isinstance(m, AIMessageChunk) and m.content:
    73	                        text = m.content
    74	                    else:
    75	                        txt = getattr(m, "content", None)
    76	                        if isinstance(txt, str) and txt:
    77	                            text = txt
    78	                text = text or ""
    79	            except Exception:
    80	                text = ""
    81	            if text:
    82	                yield _sse({"delta": text}).encode("utf-8")
    83	            yield _sse_event("done", {}).encode("utf-8")
    84	            return
    85	
    86	        loop = asyncio.get_event_loop()
    87	        last_flush = loop.time()
    88	        buf: List[str] = []
    89	
    90	        async def flush():
    91	            nonlocal buf, last_flush
    92	            if not buf:
    93	                return
    94	            chunk = "".join(buf)
    95	            buf.clear()
    96	            last_flush = loop.time()
    97	            yield _sse({"delta": chunk}).encode("utf-8")
    98	
    99	        async def _idle_guard():
   100	            await asyncio.sleep(IDLE_TIMEOUT_SEC)
   101	            raise asyncio.TimeoutError("idle timeout")
   102	
   103	        agen = llm.astream([HumanMessage(content=user_input)])
   104	
   105	        try:
   106	            first = await asyncio.wait_for(agen.__anext__(), timeout=FIRST_TOKEN_TIMEOUT_MS / 1000.0)
   107	        except Exception:
   108	            with contextlib.suppress(Exception):
   109	                await agen.aclose()
   110	            yield _sse_event("done", {}).encode("utf-8")
   111	            return
   112	
   113	        p = _piece_from_llm_chunk(first)
   114	        if p:
   115	            buf.append(p)
   116	            async for out in flush():
   117	                yield out
   118	
   119	        guard = asyncio.create_task(_idle_guard())
   120	        try:
   121	            async for chunk in agen:
   122	                if guard.done():
   123	                    break
   124	                piece = _piece_from_llm_chunk(chunk)
   125	                if not piece:
   126	                    continue
   127	                buf.append(piece)
   128	
   129	                guard.cancel()
   130	                guard = asyncio.create_task(_idle_guard())
   131	
   132	                enough = sum(len(x) for x in buf) >= COALESCE_MIN_CHARS
   133	                natural = any("".join(buf).endswith(e) for e in FLUSH_ENDINGS)
   134	                too_old = (loop.time() - last_flush) * 1000.0 >= COALESCE_MAX_LAT_MS
   135	                if enough or natural or too_old:
   136	                    async for out in flush():
   137	                        yield out
   138	            async for out in flush():
   139	                yield out
   140	        finally:
   141	            guard.cancel()
   142	            with contextlib.suppress(Exception):
   143	                await agen.aclose()
   144	
   145	        yield _sse_event("done", {}).encode("utf-8")
   146	
   147	    return StreamingResponse(publisher(), media_type="text/event-stream")


===== FILE: backend/app/api/v1/endpoints/chat_ws.py =====
     1	# backend/app/api/v1/endpoints/chat_ws.py
     2	# backend/app/api/v1/endpoints/chat_ws.py
     3	from __future__ import annotations
     4	
     5	import os
     6	import re
     7	import json
     8	import asyncio
     9	from typing import Any, Dict, Iterable, List, Optional, Tuple
    10	import redis
    11	
    12	from fastapi import APIRouter, WebSocket, WebSocketDisconnect
    13	from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
    14	from langchain_core.messages.ai import AIMessageChunk
    15	
    16	from app.api.v1.dependencies.auth import guard_websocket  # ⬅️ Token & Origin prüfen
    17	from app.services.langgraph.llm_factory import get_llm as make_llm
    18	from app.services.langgraph.redis_lifespan import get_redis_checkpointer
    19	from app.services.langgraph.prompt_registry import get_agent_prompt
    20	from app.services.langgraph.graph.intent_router import classify_intent  # ⬅️ NEU
    21	
    22	from app.services.langgraph.graph.consult.memory_utils import (
    23	    read_history as stm_read_history,
    24	    write_message as stm_write_message,
    25	)
    26	
    27	from app.services.langgraph.tools import long_term_memory as ltm
    28	
    29	router = APIRouter()
    30	
    31	COALESCE_MIN_CHARS      = int(os.getenv("WS_COALESCE_MIN_CHARS", "24"))
    32	COALESCE_MAX_LAT_MS     = float(os.getenv("WS_COALESCE_MAX_LAT_MS", "60"))
    33	IDLE_TIMEOUT_SEC        = int(os.getenv("WS_IDLE_TIMEOUT_SEC", "20"))
    34	FIRST_TOKEN_TIMEOUT_MS  = int(os.getenv("WS_FIRST_TOKEN_TIMEOUT_MS", "5000"))
    35	WS_INPUT_MAX_CHARS      = int(os.getenv("WS_INPUT_MAX_CHARS", "4000"))
    36	WS_RATE_LIMIT_PER_MIN   = int(os.getenv("WS_RATE_LIMIT_PER_MIN", "30"))
    37	MICRO_CHUNK_CHARS       = int(os.getenv("WS_MICRO_CHUNK_CHARS", "0"))
    38	EMIT_FINAL_TEXT         = os.getenv("WS_EMIT_FINAL_TEXT", "0") == "1"
    39	DEBUG_EVENTS            = os.getenv("WS_DEBUG_EVENTS", "1") == "1"
    40	DEFAULT_ROUTE           = os.getenv("WS_DEFAULT_ROUTE", "supervisor").strip().lower()
    41	
    42	FLUSH_ENDINGS: Tuple[str, ...] = (". ", "? ", "! ", "\n\n", ":", ";", "…", ", ", ") ", "] ", " }")
    43	
    44	def _env_stream_nodes() -> set[str]:
    45	    raw = os.getenv("WS_STREAM_NODES", "*").strip()
    46	    if not raw or raw in {"*", "all"}:
    47	        return {"*"}
    48	    return {x.strip().lower() for x in raw.split(",") if x.strip()}
    49	
    50	STREAM_NODES = _env_stream_nodes()
    51	
    52	def _truthy(v: Optional[str]) -> bool:
    53	    if v is None: return False
    54	    v = str(v).strip().strip('\'"').lower()
    55	    return v in ("1","true","yes","on")
    56	
    57	def _get_rl_redis(app) -> Optional[redis.Redis]:
    58	    # singleton redis client für Rate-Limit
    59	    client = getattr(app.state, "redis_rl", None)
    60	    if client is not None:
    61	        return client
    62	    url = os.getenv("REDIS_URL")
    63	    if not url:
    64	        return None
    65	    try:
    66	        client = redis.Redis.from_url(url, decode_responses=True)
    67	        app.state.redis_rl = client
    68	        return client
    69	    except Exception:
    70	        return None
    71	
    72	def _piece_from_llm_chunk(chunk: Any) -> Optional[str]:
    73	    if isinstance(chunk, AIMessageChunk):
    74	        return chunk.content or ""
    75	    txt = getattr(chunk, "content", None)
    76	    if isinstance(txt, str) and txt:
    77	        return txt
    78	    ak = getattr(chunk, "additional_kwargs", None)
    79	    if isinstance(ak, dict):
    80	        for k in ("delta", "content", "text", "token"):
    81	            v = ak.get(k)
    82	            if isinstance(v, str) and v:
    83	                return v
    84	    if isinstance(chunk, dict):
    85	        for k in ("delta", "content", "text", "token"):
    86	            v = chunk.get(k)
    87	            if isinstance(v, str) and v:
    88	                return v
    89	    return None
    90	
    91	def _iter_text_from_chunk(chunk) -> Iterable[str]:
    92	    if isinstance(chunk, dict):
    93	        c = chunk.get("content")
    94	        if isinstance(c, str) and c:
    95	            yield c; return
    96	        d = chunk.get("delta")
    97	        if isinstance(d, str) and d:
    98	            yield d; return
    99	    content = getattr(chunk, "content", None)
   100	    if isinstance(content, str) and content:
   101	        yield content; return
   102	    if isinstance(content, list):
   103	        for part in content:
   104	            if isinstance(part, str):
   105	                yield part
   106	            elif isinstance(part, dict) and isinstance(part.get("text"), str):
   107	                yield part["text"]
   108	    ak = getattr(chunk, "additional_kwargs", None)
   109	    if isinstance(ak, dict):
   110	        for k in ("delta", "content", "text", "token"):
   111	            v = ak.get(k)
   112	            if isinstance(v, str) and v:
   113	                yield v
   114	
   115	_BOUNDARY_RX = re.compile(r"[ \n\t.,;:!?…)\]}]")
   116	
   117	def _micro_chunks(s: str) -> Iterable[str]:
   118	    n = MICRO_CHUNK_CHARS
   119	    if n <= 0 or len(s) <= n:
   120	        yield s; return
   121	    i = 0; L = len(s)
   122	    while i < L:
   123	        j = min(i + n, L); k = j
   124	        if j < L:
   125	            m = _BOUNDARY_RX.search(s, j, min(L, j + 40))
   126	            if m: k = m.end()
   127	        yield s[i:k]; i = k
   128	
   129	def _is_relevant_node(ev: Dict) -> bool:
   130	    if "*" in STREAM_NODES or "all" in STREAM_NODES:
   131	        return True
   132	    meta = ev.get("metadata") or {}; run  = ev.get("run") or {}
   133	    node = str(meta.get("langgraph_node") or "").lower()
   134	    run_name = str(run.get("name") or meta.get("run_name") or "").lower()
   135	    return (node in STREAM_NODES) or (run_name in STREAM_NODES)
   136	
   137	def _last_ai_text_from_result_like(obj: Dict[str, Any]) -> str:
   138	    if not isinstance(obj, dict):
   139	        return ""
   140	    msgs = obj.get("messages")
   141	    if isinstance(msgs, list):
   142	        for m in reversed(msgs):
   143	            if isinstance(m, AIMessage):
   144	                c = getattr(m, "content", "")
   145	                if isinstance(c, str) and c:
   146	                    return c.strip()
   147	    resp = obj.get("response")
   148	    if isinstance(resp, str) and resp.strip():
   149	        return resp.strip()
   150	    for key in ("output", "state", "final_state"):
   151	        sub = obj.get(key)
   152	        if isinstance(sub, dict):
   153	            t = _last_ai_text_from_result_like(sub)
   154	            if t:
   155	                return t
   156	    return ""
   157	
   158	# --- Heuristik: RWDR-/Maß-Erkennung toleranter; erlaubt x, ×, / und optionalen Bindestrich vor der letzten Zahl
   159	CONSULT_RX  = re.compile(
   160	    r"\b(rwdr|wellendichtring|bauform\s*[A-Z0-9]{1,4}|\d{1,3}\s*[x×/]\s*\d{1,3}\s*[x×/\-]?\s*\d{1,3})\b",
   161	    re.I
   162	)
   163	GREETING_RX = re.compile(r"^(hi|hallo|hey|guten\s+tag|moin|servus)\b", re.I)
   164	REMEMBER_RX = re.compile(r"^\s*(?:!remember|remember|merke(?:\s*dir)?|speicher(?:e)?)\s*[:\-]?\s*(.+)$", re.I)
   165	
   166	def _is_greeting_only(text: str) -> bool:
   167	    """Nur reiner Gruß (z. B. 'Hallo', 'Moin!'). Gruß + Fachinhalt zählt NICHT als reiner Gruß."""
   168	    t = (text or "").strip()
   169	    m = GREETING_RX.match(t)
   170	    if not m:
   171	        return False
   172	    rest = t[m.end():].strip(" .,!?:;—–-")
   173	    return rest == ""
   174	
   175	GRAPH_BUILDER = os.getenv("GRAPH_BUILDER", "supervisor").lower()
   176	
   177	def _ensure_graph(app) -> None:
   178	    if getattr(app.state, "graph_async", None) is not None or getattr(app.state, "graph_sync", None) is not None:
   179	        return
   180	    if GRAPH_BUILDER == "supervisor":
   181	        from app.services.langgraph.supervisor_graph import build_supervisor_graph as build_graph
   182	    else:
   183	        from app.services.langgraph.graph.consult.build import build_consult_graph as build_graph
   184	    saver = None
   185	    try:
   186	        saver = get_redis_checkpointer(app)
   187	    except Exception:
   188	        saver = None
   189	    g = build_graph()
   190	    try:
   191	        compiled = g.compile(checkpointer=saver) if saver else g.compile()
   192	    except Exception:
   193	        compiled = g.compile()
   194	    app.state.graph_async = compiled
   195	    app.state.graph_sync  = compiled
   196	
   197	def _choose_subprotocol(ws: WebSocket) -> Optional[str]:
   198	    raw = ws.headers.get("sec-websocket-protocol")
   199	    if not raw:
   200	        return None
   201	    return raw.split(",")[0].strip() or None
   202	
   203	async def _send_json_safe(ws: WebSocket, payload: Dict) -> bool:
   204	    try:
   205	        await ws.send_json(payload); return True
   206	    except WebSocketDisconnect:
   207	        return False
   208	    except Exception:
   209	        return False
   210	
   211	async def _stream_llm_direct(ws: WebSocket, llm, *, user_input: str, thread_id: str):
   212	    """LLM-Direktmodus – jetzt IMMER mit Supervisor-Systemprompt als erste Message."""
   213	    def cancelled() -> bool:
   214	        flags = getattr(ws.app.state, "ws_cancel_flags", {})
   215	        return bool(flags.get(thread_id))
   216	
   217	    history = stm_read_history(thread_id, limit=80)
   218	    if cancelled():
   219	        return
   220	
   221	    loop = asyncio.get_event_loop()
   222	    buf: List[str] = []; accum: List[str] = []; last_flush = [loop.time()]
   223	
   224	    async def flush():
   225	        if not buf or cancelled():
   226	            return
   227	        chunk = "".join(buf); buf.clear(); last_flush[0] = loop.time()
   228	        accum.append(chunk)
   229	        await _send_json_safe(ws, {"event": "token", "delta": chunk, "thread_id": thread_id})
   230	
   231	    sys_msg = SystemMessage(content=get_agent_prompt("supervisor"))
   232	
   233	    agen = llm.astream([sys_msg] + history + [HumanMessage(content=user_input)])
   234	    try:
   235	        first = await asyncio.wait_for(agen.__anext__(), timeout=FIRST_TOKEN_TIMEOUT_MS / 1000.0)
   236	    except asyncio.TimeoutError:
   237	        try:
   238	            if not cancelled():
   239	                resp = await llm.ainvoke([sys_msg] + history + [HumanMessage(content=user_input)])
   240	                text = getattr(resp, "content", "") or ""
   241	            else:
   242	                text = ""
   243	        except Exception:
   244	            text = ""
   245	        try: await agen.aclose()
   246	        except Exception: pass
   247	        if text and not cancelled():
   248	            await _send_json_safe(ws, {"event": "token", "delta": text, "thread_id": thread_id})
   249	            try: stm_write_message(thread_id=thread_id, role="assistant", content=text)
   250	            except Exception: pass
   251	        if EMIT_FINAL_TEXT and not cancelled():
   252	            await _send_json_safe(ws, {"event": "final", "text": text, "thread_id": thread_id})
   253	        await _send_json_safe(ws, {"event": "done", "thread_id": thread_id})
   254	        return
   255	    except Exception:
   256	        try: await agen.aclose()
   257	        except Exception: pass
   258	        return
   259	
   260	    if cancelled():
   261	        try: await agen.aclose()
   262	        except Exception: pass
   263	        return
   264	
   265	    txt = (_piece_from_llm_chunk(first) or "")
   266	    if txt and not cancelled():
   267	        for seg in _micro_chunks(txt):
   268	            buf.append(seg); await flush()
   269	
   270	    try:
   271	        async for chunk in agen:
   272	            if cancelled(): break
   273	            for piece in _iter_text_from_chunk(chunk):
   274	                if not piece or cancelled(): continue
   275	                for seg in _micro_chunks(piece):
   276	                    buf.append(seg)
   277	                    enough  = sum(len(x) for x in buf) >= COALESCE_MIN_CHARS
   278	                    natural = any("".join(buf).endswith(e) for e in FLUSH_ENDINGS)
   279	                    too_old = (loop.time() - last_flush[0]) * 1000.0 >= COALESCE_MAX_LAT_MS
   280	                    if enough or natural or too_old:
   281	                        await flush()
   282	        await flush()
   283	    finally:
   284	        try: await agen.aclose()
   285	        except Exception: pass
   286	
   287	    if cancelled():
   288	        return
   289	
   290	    final_text = ("".join(accum)).strip()
   291	    if final_text:
   292	        try: stm_write_message(thread_id=thread_id, role="assistant", content=final_text)
   293	        except Exception: pass
   294	    if EMIT_FINAL_TEXT:
   295	        await _send_json_safe(ws, {"event": "final", "text": final_text, "thread_id": thread_id})
   296	    await _send_json_safe(ws, {"event": "done", "thread_id": thread_id})
   297	
   298	async def _stream_supervised(ws: WebSocket, *, app, user_input: str, thread_id: str, params_patch: Optional[Dict]=None):
   299	    def cancelled() -> bool:
   300	        flags = getattr(ws.app.state, "ws_cancel_flags", {})
   301	        return bool(flags.get(thread_id))
   302	
   303	    if cancelled():
   304	        return
   305	
   306	    try:
   307	        _ensure_graph(app)
   308	    except Exception as e:
   309	        if EMIT_FINAL_TEXT and not cancelled():
   310	            await _send_json_safe(ws, {"event": "final", "text": "", "thread_id": thread_id, "error": f"graph_build_failed: {e!r}"})
   311	        await _send_json_safe(ws, {"event": "done", "thread_id": thread_id})
   312	        return
   313	
   314	    g_async = getattr(app.state, "graph_async", None)
   315	    g_sync  = getattr(app.state, "graph_sync", None)
   316	
   317	    history = stm_read_history(thread_id, limit=80)
   318	    sys_msg = SystemMessage(content=get_agent_prompt("supervisor"))
   319	    initial: Dict[str, Any] = {"messages": [sys_msg] + history + [HumanMessage(content=user_input)], "chat_id": thread_id, "input": user_input}
   320	    if isinstance(params_patch, dict) and params_patch:
   321	        initial["params"] = params_patch
   322	
   323	    cfg = {"configurable": {"thread_id": thread_id, "checkpoint_ns": getattr(app.state, "checkpoint_ns", None)}}
   324	
   325	    loop = asyncio.get_event_loop()
   326	    buf: List[str] = []; last_flush = [loop.time()]; streamed_any = False
   327	    final_tail: str = ""; accum: List[str] = []
   328	
   329	    async def flush():
   330	        nonlocal streamed_any
   331	        if not buf or cancelled(): return
   332	        chunk = "".join(buf); buf.clear(); last_flush[0] = loop.time()
   333	        streamed_any = True; accum.append(chunk)
   334	        if not await _send_json_safe(ws, {"event": "token", "delta": chunk, "thread_id": thread_id}):
   335	            raise WebSocketDisconnect()
   336	
   337	    def _emit_ui_event_if_any(ev_data: Any) -> bool:
   338	        if not isinstance(ev_data, dict):
   339	            return False
   340	        ui_ev = ev_data.get("ui_event")
   341	        if isinstance(ui_ev, dict):
   342	            # zusätzlich "event": "ui_action" mitsenden, damit das Frontend eindeutig routen kann
   343	            payload = {**ui_ev, "event": "ui_action", "thread_id": thread_id}
   344	            return asyncio.create_task(_send_json_safe(ws, payload)) is not None
   345	        for key in ("output", "state", "final_state", "result"):
   346	            sub = ev_data.get(key)
   347	            if isinstance(sub, dict) and isinstance(sub.get("ui_event"), dict):
   348	                u = {**sub["ui_event"], "event": "ui_action", "thread_id": thread_id}
   349	                return asyncio.create_task(_send_json_safe(ws, u)) is not None
   350	        return False
   351	
   352	    if g_async is not None and not cancelled():
   353	        for ver in ("v2", "v1"):
   354	            try:
   355	                async for ev in g_async.astream_events(initial, config=cfg, version=ver):
   356	                    if cancelled(): return
   357	
   358	                    if DEBUG_EVENTS:
   359	                        await _send_json_safe(ws, {"event": "dbg", "meta": ev.get("metadata"), "name": ev.get("event")})
   360	
   361	                    ev_name = ev.get("event") if isinstance(ev, dict) else None
   362	
   363	                    if ev_name in ("on_chat_model_stream", "on_llm_stream") and _is_relevant_node(ev):
   364	                        chunk = (ev.get("data") or {}).get("chunk") if isinstance(ev.get("data"), dict) else None
   365	                        if not chunk: continue
   366	                        for piece in _iter_text_from_chunk(chunk):
   367	                            if not piece or cancelled(): continue
   368	                            for seg in _micro_chunks(piece):
   369	                                buf.append(seg)
   370	                                enough  = sum(len(x) for x in buf) >= COALESCE_MIN_CHARS
   371	                                natural = any("".join(buf).endswith(e) for e in FLUSH_ENDINGS)
   372	                                too_old = (loop.time() - last_flush[0]) * 1000.0 >= COALESCE_MAX_LAT_MS
   373	                                if enough or natural or too_old:
   374	                                    await flush()
   375	
   376	                    if ev_name in ("on_node_end", "on_chain_end", "on_graph_end"):
   377	                        _emit_ui_event_if_any(ev.get("data"))
   378	
   379	                    if ev_name in ("on_chain_end", "on_graph_end"):
   380	                        data   = ev.get("data") or {}
   381	                        output = data.get("output") if isinstance(data, dict) else {}
   382	                        output = output or (data.get("state") if isinstance(data, dict) else {})
   383	                        output = output or (data.get("final_state") if isinstance(data, dict) else {})
   384	                        t = _last_ai_text_from_result_like(output) or ""
   385	                        if t: final_tail = t
   386	                await flush(); break
   387	            except Exception:
   388	                continue
   389	
   390	    if cancelled():
   391	        return
   392	
   393	    if final_tail:
   394	        if not streamed_any:
   395	            accum.append(final_tail)
   396	            if not await _send_json_safe(ws, {"event": "token", "delta": final_tail, "thread_id": thread_id}):
   397	                return
   398	        assistant_text = final_tail
   399	    elif not streamed_any:
   400	        try:
   401	            if g_sync is not None:
   402	                def _run_sync(): return g_sync.invoke(initial, config=cfg)
   403	                result = await asyncio.get_event_loop().run_in_executor(None, _run_sync)
   404	            else:
   405	                result = await g_async.ainvoke(initial, config=cfg)  # type: ignore
   406	            final_text = _last_ai_text_from_result_like(result) or ""
   407	            assistant_text = final_text
   408	            if final_text and not cancelled():
   409	                accum.append(final_text)
   410	                if not await _send_json_safe(ws, {"event": "token", "delta": final_text, "thread_id": thread_id}):
   411	                    return
   412	        except Exception:
   413	            assistant_text = ""
   414	    else:
   415	        assistant_text = "".join(accum)
   416	
   417	    if cancelled():
   418	        return
   419	
   420	    final_text = (assistant_text or "".join(accum)).strip()
   421	    try:
   422	        if final_text:
   423	            stm_write_message(thread_id=thread_id, role="assistant", content=final_text)
   424	    except Exception:
   425	        pass
   426	
   427	    if EMIT_FINAL_TEXT:
   428	        await _send_json_safe(ws, {"event": "final", "text": final_text, "thread_id": thread_id})
   429	    await _send_json_safe(ws, {"event": "done", "thread_id": thread_id})
   430	
   431	@router.websocket("/ai/ws")
   432	async def ws_chat(ws: WebSocket):
   433	    # ⛔ Erst authentifizieren/origin prüfen, dann accept()
   434	    try:
   435	        _ = await guard_websocket(ws)
   436	    except Exception:
   437	        return
   438	
   439	    await ws.accept(subprotocol=_choose_subprotocol(ws))
   440	
   441	    app = ws.app
   442	    if not getattr(app.state, "llm", None):
   443	        app.state.llm = make_llm(streaming=True)
   444	    try:
   445	        ltm.prewarm_ltm()
   446	    except Exception:
   447	        pass
   448	    if not hasattr(app.state, "ws_cancel_flags"):
   449	        app.state.ws_cancel_flags = {}
   450	
   451	    try:
   452	        while True:
   453	            raw = await ws.receive_text()
   454	            # 1) Rohgrößen-Guard (DoS/Fehleingaben)
   455	            if isinstance(raw, str) and WS_INPUT_MAX_CHARS > 0 and len(raw) > (WS_INPUT_MAX_CHARS * 2):
   456	                await _send_json_safe(ws, {
   457	                    "event": "error",
   458	                    "code": "input_oversize",
   459	                    "message": f"payload too large (>{WS_INPUT_MAX_CHARS*2} chars)",
   460	                })
   461	                await _send_json_safe(ws, {"event": "done", "thread_id": "ws"})
   462	                continue
   463	            try:
   464	                data = json.loads(raw)
   465	            except Exception:
   466	                await _send_json_safe(ws, {"event": "error", "message": "invalid_json"}); continue
   467	
   468	            typ = (data.get("type") or "").strip().lower()
   469	            if typ == "ping":
   470	                await _send_json_safe(ws, {"event": "pong", "ts": data.get("ts")}); continue
   471	            if typ == "cancel":
   472	                tid = (data.get("thread_id") or f"api:{(data.get('chat_id') or 'default').strip()}").strip()
   473	                app.state.ws_cancel_flags[tid] = True
   474	                await _send_json_safe(ws, {"event": "done", "thread_id": tid}); continue
   475	
   476	            chat_id    = (data.get("chat_id") or "").strip() or "default"
   477	            thread_id  = f"api:{chat_id}"
   478	            payload    = ws.scope.get("user") or {}
   479	            user_id    = str(payload.get("sub") or payload.get("email") or chat_id)
   480	
   481	            # 2) Rate-Limit (pro User/Thread), 30 req/min Default
   482	            rl = _get_rl_redis(app)
   483	            if rl and WS_RATE_LIMIT_PER_MIN > 0:
   484	                key = f"ws:ratelimit:{user_id}:{chat_id}"
   485	                try:
   486	                    cur = rl.incr(key)
   487	                    if cur == 1:
   488	                        rl.expire(key, 60)  # Fenster 60s
   489	                    if cur > WS_RATE_LIMIT_PER_MIN:
   490	                        await _send_json_safe(ws, {
   491	                            "event": "error",
   492	                            "code": "rate_limited",
   493	                            "message": "Too many requests, slow down.",
   494	                            "retry_after_sec": int(rl.ttl(key) or 60)
   495	                        })
   496	                        await _send_json_safe(ws, {"event": "done", "thread_id": thread_id})
   497	                        continue
   498	                except Exception:
   499	                    pass
   500	
   501	            params_patch = data.get("params") or data.get("params_patch")
   502	            if not isinstance(params_patch, dict):
   503	                params_patch = None
   504	
   505	            user_input = (data.get("input") or data.get("text") or data.get("query") or "").strip()
   506	            # 3) Input-Längen-Guard
   507	            if user_input and WS_INPUT_MAX_CHARS > 0 and len(user_input) > WS_INPUT_MAX_CHARS:
   508	                await _send_json_safe(ws, {
   509	                    "event": "error",
   510	                    "code": "input_too_long",
   511	                    "message": f"input exceeds {WS_INPUT_MAX_CHARS} chars"
   512	                })
   513	                await _send_json_safe(ws, {"event": "done", "thread_id": thread_id})
   514	                continue
   515	
   516	            if not user_input and not params_patch:
   517	                await _send_json_safe(ws, {"event": "error", "message": "missing input", "thread_id": thread_id}); continue
   518	
   519	            try: app.state.ws_cancel_flags.pop(thread_id, None)
   520	            except Exception: pass
   521	
   522	            if user_input:
   523	                try: stm_write_message(thread_id=thread_id, role="user", content=user_input)
   524	                except Exception: pass
   525	
   526	            m = REMEMBER_RX.match(user_input or "")
   527	            if m:
   528	                note = m.group(1).strip(); ok = False
   529	                try:
   530	                    _ = ltm.upsert_memory(user=thread_id, chat_id=thread_id, text=note, kind="note"); ok = True
   531	                except Exception: ok = False
   532	                msg = "✅ Gespeichert." if ok else "⚠️ Konnte nicht speichern."
   533	                await _send_json_safe(ws, {"event": "token", "delta": msg, "thread_id": thread_id})
   534	                if EMIT_FINAL_TEXT:
   535	                    await _send_json_safe(ws, {"event": "final", "text": msg, "thread_id": thread_id})
   536	                await _send_json_safe(ws, {"event": "done", "thread_id": thread_id})
   537	                try: stm_write_message(thread_id=thread_id, role="assistant", content=msg)
   538	                except Exception: pass
   539	                continue
   540	
   541	            # Routing per Intent-Router:
   542	            # 1) Immer Supervisor, wenn Formular-Parameter gepatcht werden
   543	            # 2) Bei klaren Consult-Hinweisen (Maße/RWDR) -> Supervisor
   544	            # 3) Sonst Intent-Router: chitchat -> LLM, consult -> Supervisor
   545	            if params_patch:
   546	                route = "supervisor"; reason = "params_patch"
   547	            elif user_input and CONSULT_RX.search(user_input):
   548	                route = "supervisor"; reason = "consult_rx"
   549	            else:
   550	                history_msgs = stm_read_history(thread_id, limit=40)
   551	                sys_msg = SystemMessage(content=get_agent_prompt("supervisor"))
   552	                msgs = [sys_msg] + history_msgs + [HumanMessage(content=user_input or "")]
   553	                try:
   554	                    intent = classify_intent(app.state.llm, msgs)
   555	                    if intent == "chitchat":
   556	                        route = "llm"; reason = "intent:chitchat"
   557	                    else:
   558	                        route = "supervisor"; reason = f"intent:{intent}"
   559	                except Exception:
   560	                    route = "llm"; reason = "intent_error_fallback"
   561	
   562	            await _send_json_safe(ws, {"phase": "starting", "thread_id": thread_id, "route_guess": route, "reason": reason})
   563	
   564	            if route == "supervisor":
   565	                await _stream_supervised(ws, app=app, user_input=(user_input or "📝 form patch"), thread_id=thread_id, params_patch=params_patch)
   566	            else:
   567	                await _stream_llm_direct(ws, app.state.llm, user_input=(user_input or ""), thread_id=thread_id)
   568	
   569	            try: app.state.ws_cancel_flags.pop(thread_id, None)
   570	            except Exception: pass
   571	
   572	    except WebSocketDisconnect:
   573	        return
   574	    except Exception as e:
   575	        if EMIT_FINAL_TEXT:
   576	            await _send_json_safe(ws, {"event": "final", "text": "", "error": f"ws_internal_error: {e!r}"})
   577	        await _send_json_safe(ws, {"event": "done", "thread_id": "ws"})


===== FILE: backend/app/api/v1/endpoints/consult_invoke.py =====
     1	from __future__ import annotations
     2	
     3	from fastapi import APIRouter, Request, HTTPException, status
     4	from pydantic import BaseModel, Field
     5	from typing import Optional
     6	
     7	from app.services.langgraph.graph.consult.io import invoke_consult
     8	from app.services.langgraph.redis_lifespan import get_redis_checkpointer
     9	
    10	router = APIRouter(prefix="/test", tags=["test"])  # wird unter /api/v1 gemountet
    11	
    12	class ConsultInvokeIn(BaseModel):
    13	    text: str = Field(..., description="Nutzereingabe")
    14	    chat_id: str = Field(..., description="Thread/Chat ID")
    15	
    16	class ConsultInvokeOut(BaseModel):
    17	    text: str
    18	
    19	@router.post("/consult/invoke", response_model=ConsultInvokeOut)
    20	async def consult_invoke_endpoint(payload: ConsultInvokeIn, request: Request):
    21	    text = (payload.text or "").strip()
    22	    if not text:
    23	        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="text empty")
    24	
    25	    chat_id = f"api:{payload.chat_id.strip() or 'test'}"
    26	    try:
    27	        saver = None
    28	        try:
    29	            saver = get_redis_checkpointer(request.app)
    30	        except Exception:
    31	            saver = None
    32	        out = invoke_consult(text, thread_id=chat_id, checkpointer=saver)
    33	        return {"text": out}
    34	    except Exception as e:
    35	        raise HTTPException(status_code=500, detail=f"invoke_failed: {e}")


===== FILE: backend/app/api/v1/endpoints/__init__.py =====


===== FILE: backend/app/api/v1/endpoints/langgraph_sse.py =====
     1	from __future__ import annotations
     2	
     3	import json
     4	import re
     5	from typing import AsyncGenerator, Dict, Optional
     6	
     7	from fastapi import APIRouter, Depends, HTTPException
     8	from starlette.responses import StreamingResponse
     9	
    10	router = APIRouter()
    11	
    12	try:
    13	    from app.api.v1.deps import get_current_user_optional as _get_user_opt  # type: ignore
    14	except Exception:  # pragma: no cover
    15	    async def _get_user_opt() -> Optional[dict]:
    16	        return None
    17	
    18	def _sse(event_dict: Dict, event_type: str = "data") -> bytes:
    19	    return f"event: {event_type}\ndata: {json.dumps(event_dict, ensure_ascii=False)}\n\n".encode("utf-8")
    20	
    21	_re_dims = re.compile(r"\b(\d+)\s*[x×X]\s*(\d+)\s*[x×X]\s*(\d+)\b")
    22	_re_rpm = re.compile(r"\b(rpm|U/min)\b", re.I)
    23	
    24	def _parse_missing(text: str) -> list[str]:
    25	    missing: list[str] = []
    26	    if not _re_dims.search(text):
    27	        missing += ["Welle (mm)", "Gehäuse (mm)", "Breite (mm)"]
    28	    if not re.search(r"\b(Öl|Medium|Hydrauliköl)\b", text, re.I):
    29	        missing.append("Medium")
    30	    if not re.search(r"(Tmax|Temperatur|max\s*°C|°C)", text, re.I):
    31	        missing.append("Temperatur max (°C)")
    32	    if not re.search(r"(Druck\s*\d+|Druck\s*\d+[.,]?\d*\s*bar|\b\d+\s*bar\b)", text, re.I):
    33	        missing.append("Druck (bar)")
    34	    if not _re_rpm.search(text):
    35	        missing.append("Drehzahl (U/min)")
    36	    if "Drehzahl (U/min)" not in missing and not re.search(r"\bv\s*=\s*\d+[.,]?\d*\s*m/s\b", text, re.I):
    37	        missing.append("Relativgeschwindigkeit (m/s)")
    38	    return missing
    39	
    40	def _route_guess(text: str) -> str:
    41	    if re.search(r"\b(BA|RWDR|Dichtung|Ring)\b", text, re.I):
    42	        return "consult"
    43	    if re.search(r"^(hi|hallo|guten\s*tag|moin)\b", text.strip(), re.I):
    44	        return "chitchat"
    45	    return "consult"
    46	
    47	async def _event_stream(chat_id: str, input_text: str) -> AsyncGenerator[bytes, None]:
    48	    route = _route_guess(input_text)
    49	    yield _sse({"phase": "starting", "thread_id": f"api:{chat_id}", "route_guess": route, "reason": "heuristic"})
    50	    if route == "chitchat":
    51	        for chunk in ["G", "uten Tag", "! Wie kann", " ich Ihnen helfen", "?"]:
    52	            yield _sse({"delta": chunk})
    53	        yield _sse({"final": {"text": "Guten Tag! Wie kann ich Ihnen helfen?"}})
    54	        return
    55	    missing = _parse_missing(input_text)
    56	    if missing:
    57	        msg = (
    58	            "Mir fehlen noch folgende Angaben, um fortzufahren: "
    59	            + ", ".join(f"**{m}**" for m in missing)
    60	            + ".\nKannst du mir diese bitte nennen?\n\n"
    61	              "Du kannst die Werte gern in einer Zeile schicken, z. B.: "
    62	              "`Welle 25, Gehäuse 47, Breite 7, Medium Öl, Tmax 80, Druck 2 bar, n 1500, v=0,5 m/s`"
    63	        )
    64	        yield _sse({"final": {"text": msg}})
    65	        return
    66	    dims = _re_dims.search(input_text)
    67	    d_w, d_h, d_b = dims.groups() if dims else ("?", "?", "?")
    68	    werkstoff = "FKM" if re.search(r"\b(9\d|1\d{2})\s*°?C\b|>\s*80\s*°?C", input_text) else "NBR"
    69	    text = (
    70	        "🔎 **Meine Empfehlung – präzise und transparent:**\n\n"
    71	        f"**Typ:** BA {d_w}x{d_h}x{d_b}\n"
    72	        f"**Werkstoff:** {werkstoff}\n\n"
    73	        "**Vorteile:** hohe Temperatur- und Medienbeständigkeit\n"
    74	        "**Einschränkungen:** drucklose Anwendung vorausgesetzt\n\n"
    75	        "**Begründung:** Angaben vollständig; Auswahl gemäß Öl, Temperatur und Drehzahl.\n\n"
    76	        "Möchten Sie ein **Angebot** anfordern, eine **Zeichnung/Datenblatt** erhalten oder **Alternativen** vergleichen?"
    77	    )
    78	    yield _sse({"final": {"text": text}})
    79	
    80	@router.post("/chat/stream", response_class=StreamingResponse)
    81	async def chat_stream(payload: dict, user=Depends(_get_user_opt)) -> StreamingResponse:
    82	    chat_id = str(payload.get("chat_id") or "").strip()
    83	    input_text = str(payload.get("input_text") or "").strip()
    84	    if not chat_id or not input_text:
    85	        raise HTTPException(status_code=400, detail="chat_id und input_text sind Pflichtfelder.")
    86	    return StreamingResponse(_event_stream(chat_id, input_text), media_type="text/event-stream")


===== FILE: backend/app/api/v1/endpoints/memory.py =====
     1	# backend/app/api/v1/endpoints/memory.py
     2	from __future__ import annotations
     3	
     4	import logging
     5	import time
     6	import uuid
     7	from typing import Any, Dict, Optional, List
     8	
     9	from fastapi import APIRouter, Depends, HTTPException, Query
    10	from fastapi.responses import JSONResponse
    11	from qdrant_client import QdrantClient, models as qmodels
    12	
    13	from app.core.config import settings
    14	from app.services.auth.dependencies import get_current_request_user
    15	from app.services.memory.memory_core import (
    16	    ltm_export_all,
    17	    ltm_delete_all,
    18	    ensure_ltm_collection,
    19	    _get_qdrant_client,
    20	)
    21	
    22	router = APIRouter(prefix="/memory", tags=["memory"])
    23	logger = logging.getLogger(__name__)
    24	
    25	
    26	def _ltm_collection() -> str:
    27	    """Resolve the Qdrant collection name for LTM (Long-Term-Memory)."""
    28	    return (settings.qdrant_collection_ltm or f"{settings.qdrant_collection}-ltm").strip()
    29	
    30	
    31	# ----------------------------------------------------------------------
    32	# Create Memory Item
    33	# ----------------------------------------------------------------------
    34	@router.post("", summary="Lege einen LTM-Eintrag in Qdrant an")
    35	async def create_memory_item(
    36	    payload: Dict[str, Any],
    37	    username: str = Depends(get_current_request_user),
    38	) -> JSONResponse:
    39	    """
    40	    Erwartet JSON:
    41	    {
    42	      "text": "…Pflicht…",
    43	      "kind": "note|preference|fact|…",
    44	      "chat_id": "optional"
    45	    }
    46	    """
    47	    if not settings.ltm_enable:
    48	        return JSONResponse({"ltm_enabled": False}, status_code=200)
    49	
    50	    text = (payload.get("text") or "").strip()
    51	    if not text:
    52	        raise HTTPException(status_code=400, detail="Field 'text' is required and must be non-empty")
    53	
    54	    kind = (payload.get("kind") or "note").strip()
    55	    chat_id = (payload.get("chat_id") or None) or None
    56	
    57	    point_id = uuid.uuid4().hex
    58	    q_payload: Dict[str, Any] = {
    59	        "user": username,  # WICHTIG: Schlüssel = 'user' (wird für Filter verwendet!)
    60	        "chat_id": chat_id,
    61	        "kind": kind,
    62	        "text": text,
    63	        "created_at": time.time(),
    64	    }
    65	    # Zusatzfelder übernehmen (ohne Pflichtfelder zu überschreiben)
    66	    for k, v in payload.items():
    67	        if k not in q_payload:
    68	            q_payload[k] = v
    69	
    70	    try:
    71	        client: QdrantClient = _get_qdrant_client()
    72	        ensure_ltm_collection(client)
    73	
    74	        # Dummy-Vektor, da nur Payload benötigt wird
    75	        client.upsert(
    76	            collection_name=_ltm_collection(),
    77	            points=[
    78	                qmodels.PointStruct(
    79	                    id=point_id,
    80	                    vector=[0.0],
    81	                    payload=q_payload,
    82	                )
    83	            ],
    84	            wait=True,
    85	        )
    86	
    87	        logger.info(f"[LTM] create_memory_item user={username} chat_id={chat_id} id={point_id}")
    88	        return JSONResponse(
    89	            {"id": point_id, "ltm_enabled": True, "success": True},
    90	            status_code=200,
    91	        )
    92	
    93	    except Exception as exc:
    94	        logger.exception(f"[LTM] Fehler beim Speichern für user={username}, chat_id={chat_id}: {exc}")
    95	        raise HTTPException(status_code=500, detail="Speichern fehlgeschlagen") from exc
    96	
    97	
    98	# ----------------------------------------------------------------------
    99	# Export Memory Items
   100	# ----------------------------------------------------------------------
   101	@router.get("/export", summary="Exportiere Long-Term-Memory (Qdrant) des aktuellen Nutzers")
   102	async def export_memory(
   103	    chat_id: Optional[str] = Query(default=None, description="Optional: nur Einträge dieses Chats exportieren"),
   104	    limit: int = Query(default=10000, ge=1, le=20000),
   105	    username: str = Depends(get_current_request_user),
   106	) -> JSONResponse:
   107	    if not settings.ltm_enable:
   108	        return JSONResponse({"items": [], "count": 0, "ltm_enabled": False}, status_code=200)
   109	
   110	    try:
   111	        items: List[Dict[str, Any]] = ltm_export_all(user=username, chat_id=chat_id, limit=limit)
   112	        logger.info(f"[LTM] export_memory user={username} chat_id={chat_id} count={len(items)}")
   113	        return JSONResponse(
   114	            {"items": items, "count": len(items), "ltm_enabled": True, "success": True},
   115	            status_code=200,
   116	        )
   117	    except Exception as exc:
   118	        logger.exception(f"[LTM] Fehler beim Export für user={username}, chat_id={chat_id}: {exc}")
   119	        raise HTTPException(status_code=500, detail="Export fehlgeschlagen") from exc
   120	
   121	
   122	# ----------------------------------------------------------------------
   123	# Delete Memory Items
   124	# ----------------------------------------------------------------------
   125	@router.delete("", summary="Lösche Long-Term-Memory des aktuellen Nutzers (optional pro Chat)")
   126	async def delete_memory(
   127	    chat_id: Optional[str] = Query(default=None, description="Optional: nur Einträge dieses Chats löschen"),
   128	    username: str = Depends(get_current_request_user),
   129	) -> JSONResponse:
   130	    if not settings.ltm_enable:
   131	        return JSONResponse({"deleted": 0, "ltm_enabled": False}, status_code=200)
   132	
   133	    try:
   134	        deleted = ltm_delete_all(user=username, chat_id=chat_id)
   135	        logger.info(f"[LTM] delete_memory user={username} chat_id={chat_id} deleted={deleted}")
   136	        return JSONResponse(
   137	            {"deleted": deleted, "ltm_enabled": True, "success": True},
   138	            status_code=200,
   139	        )
   140	    except Exception as exc:
   141	        logger.exception(f"[LTM] Fehler beim Löschen für user={username}, chat_id={chat_id}: {exc}")
   142	        raise HTTPException(status_code=500, detail="Löschen fehlgeschlagen") from exc


===== FILE: backend/app/api/v1/endpoints/rfq.py =====
     1	from __future__ import annotations
     2	import os
     3	from fastapi import APIRouter, HTTPException, Query
     4	from fastapi.responses import FileResponse
     5	
     6	router = APIRouter()
     7	
     8	@router.get("/download")
     9	def rfq_download(path: str = Query(..., description="Server-Pfad zur PDF")):
    10	    if not os.path.isfile(path):
    11	        raise HTTPException(404, "Datei nicht gefunden")
    12	    return FileResponse(path, filename=os.path.basename(path), media_type="application/pdf")


===== FILE: backend/app/api/v1/endpoints/sse_test.py =====
     1	# backend/app/api/v1/endpoints/sse_test.py
     2	from __future__ import annotations
     3	
     4	import asyncio
     5	from fastapi import APIRouter
     6	from fastapi.responses import StreamingResponse
     7	
     8	router = APIRouter()
     9	
    10	async def _ticker():
    11	    for i in range(1, 6):
    12	        yield f"data: tick {i}\n\n".encode("utf-8")
    13	        await asyncio.sleep(0.5)
    14	    yield b"event: done\ndata: {}\n\n"
    15	
    16	@router.get("/sse/test")
    17	async def sse_test():
    18	    return StreamingResponse(
    19	        _ticker(),
    20	        media_type="text/event-stream",
    21	        headers={
    22	            "Cache-Control": "no-cache, no-transform",
    23	            "Connection": "keep-alive",
    24	            "X-Accel-Buffering": "no",
    25	        },
    26	    )


===== FILE: backend/app/api/v1/endpoints/system.py =====
     1	from __future__ import annotations
     2	
     3	from typing import Optional, Dict, Any
     4	from pydantic import BaseModel, Field
     5	from fastapi import APIRouter, Request
     6	import json
     7	
     8	from app.services.langgraph.graph.consult.io import invoke_consult
     9	from app.services.langgraph.redis_lifespan import get_redis_checkpointer
    10	
    11	router = APIRouter()  # Prefix und Tags werden im übergeordneten Router gesetzt
    12	
    13	class _ConsultInvokeIn(BaseModel):
    14	    text: str = Field(..., min_length=1)
    15	    chat_id: Optional[str] = None
    16	
    17	@router.post("/test/consult/invoke", tags=["test"])
    18	async def test_consult_invoke(body: _ConsultInvokeIn, request: Request) -> Dict[str, Any]:
    19	    thread_id = f"api:{body.chat_id or 'test'}"
    20	    try:
    21	        saver = get_redis_checkpointer(request.app)
    22	    except Exception:
    23	        saver = None
    24	
    25	    out = invoke_consult(body.text, thread_id=thread_id, checkpointer=saver)
    26	
    27	    try:
    28	        parsed = json.loads(out)
    29	        payload = {"json": parsed} if isinstance(parsed, (dict, list)) else {"text": out}
    30	    except Exception:
    31	        payload = {"text": out}
    32	
    33	    return {"final": payload}


===== FILE: backend/app/api/v1/endpoints/users.py =====
     1	from fastapi import APIRouter
     2	from fastapi.responses import JSONResponse
     3	
     4	router = APIRouter()
     5	
     6	@router.get("/ping", summary="👥 User Ping", response_class=JSONResponse)
     7	async def users_ping():
     8	    return {"pong": True, "module": "users"}


===== FILE: backend/app/api/v1/__init__.py =====


===== FILE: backend/app/api/v1/schemas/chat.py =====
     1	from pydantic import BaseModel
     2	from typing import Dict, Optional, List
     3	from datetime import datetime
     4	
     5	class ChatRequest(BaseModel):
     6	    chat_id: str
     7	    input_text: str
     8	
     9	class ChatResponse(BaseModel):
    10	    response: str
    11	
    12	# ───────────────────────────────────────────────────────
    13	# Für den Beratungs-Workflow via /beratung
    14	# ───────────────────────────────────────────────────────
    15	class BeratungsRequest(BaseModel):
    16	    frage: str
    17	    chat_id: str
    18	
    19	class BeratungsResponse(BaseModel):
    20	    antworten: Dict[str, str]
    21	
    22	class BeratungsverlaufResponse(BaseModel):
    23	    id: int
    24	    session_id: str
    25	    frage: Optional[str]
    26	    parameter: Optional[dict]
    27	    antworten: Optional[List[str]]
    28	    timestamp: datetime


===== FILE: backend/app/api/v1/schemas/form.py =====
     1	from pydantic import BaseModel, Field
     2	from datetime import datetime
     3	
     4	class FormData(BaseModel):
     5	    shaft_diameter: float = Field(..., description="Wellen-Ø in mm")
     6	    housing_diameter: float = Field(..., description="Gehäuse-Ø in mm")
     7	
     8	class FormResult(BaseModel):
     9	    id: str
    10	    username: str
    11	    radial_clearance: float
    12	    tolerance_fit: str
    13	    result_text: str
    14	    created_at: datetime
    15	
    16	    class Config:
    17	        from_attributes = True


===== FILE: backend/app/core/config.py =====
     1	# backend/app/core/config.py
     2	from typing import Optional
     3	from pydantic_settings import BaseSettings, SettingsConfigDict
     4	from functools import lru_cache
     5	import structlog
     6	
     7	# Strukturiertes Logging (JSON)
     8	structlog.configure(
     9	    processors=[
    10	        structlog.processors.TimeStamper(fmt="iso"),
    11	        structlog.stdlib.add_log_level,
    12	        structlog.processors.JSONRenderer(),
    13	    ],
    14	    context_class=dict,
    15	    logger_factory=structlog.stdlib.LoggerFactory(),
    16	    wrapper_class=structlog.stdlib.BoundLogger,
    17	    cache_logger_on_first_use=True,
    18	)
    19	
    20	class Settings(BaseSettings):
    21	    # Datenbank / SQLAlchemy
    22	    postgres_user: str
    23	    postgres_password: str
    24	    postgres_host: str
    25	    postgres_port: int
    26	    postgres_db: str
    27	    database_url: str
    28	    debug_sql: bool = False
    29	
    30	    # Neu: Postgres-Sync-URL (basierend auf database_url oder explizit)
    31	    POSTGRES_SYNC_URL: str
    32	
    33	    # OpenAI / LLM / LangChain
    34	    openai_api_key: str
    35	    openai_model: str = "gpt-4.1-mini"
    36	
    37	    # Embeddings
    38	    embedding_model: str = "BAAI/bge-base-en-v1.5"
    39	
    40	    # Qdrant (RAG & LTM)
    41	    qdrant_url: str
    42	    qdrant_collection: str
    43	    qdrant_collection_ltm: Optional[str] = None
    44	    qdrant_api_key: Optional[str] = None
    45	    rag_k: int = 4
    46	    qdrant_filter_metadata: Optional[dict] = None
    47	    debug_qdrant: bool = False
    48	
    49	    # Redis Memory & Sessions
    50	    redis_host: str = "redis"
    51	    redis_port: int = 6379
    52	    redis_url: str
    53	    redis_db: int = 0
    54	    redis_ttl: int = 60 * 60 * 24  # 24h
    55	
    56	    # Explizite REDIS_URL (Fallback für andere Komponenten)
    57	    REDIS_URL: str = "redis://redis:6379/0"
    58	
    59	    # Auth / Keycloak / NextAuth
    60	    nextauth_url: str
    61	    nextauth_secret: str
    62	    keycloak_issuer: str
    63	    keycloak_jwks_url: str
    64	    keycloak_client_id: str
    65	    keycloak_client_secret: str
    66	    keycloak_expected_azp: str
    67	
    68	    # LangChain Tracing etc.
    69	    langchain_tracing_v2: bool = True
    70	    langchain_endpoint: Optional[str] = "https://api.smith.langchain.com"
    71	    langchain_api_key: Optional[str] = None
    72	    langchain_project: Optional[str] = "sealai"
    73	
    74	    # Feature-Flags
    75	    ltm_enable: bool = True
    76	
    77	    model_config = SettingsConfigDict(
    78	        env_file=".env",
    79	        extra="ignore",
    80	    )
    81	
    82	    # Wichtig: nicht mehr aus ENV lesen.
    83	    # Der Backend-Issuer entspricht immer dem Keycloak-Issuer.
    84	    @property
    85	    def backend_keycloak_issuer(self) -> str:
    86	        return self.keycloak_issuer
    87	
    88	
    89	@lru_cache(maxsize=1)
    90	def get_settings() -> Settings:
    91	    return Settings()
    92	
    93	
    94	settings = get_settings()


===== FILE: backend/app/database.py =====
     1	# 📁 backend/app/database.py
     2	
     3	from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
     4	from sqlalchemy.orm import sessionmaker, declarative_base
     5	# Best Practice: Einzige Quelle für Einstellungen ist app.core.config
     6	from app.core.config import settings
     7	
     8	# SQLAlchemy Base
     9	Base = declarative_base()
    10	
    11	# Datenbank-URL aus Core-Config ziehen
    12	DATABASE_URL = settings.database_url
    13	
    14	# Engine mit optionalem SQL-Debug aus den Settings
    15	engine = create_async_engine(
    16	    DATABASE_URL,
    17	    future=True,
    18	    echo=settings.debug_sql,   # gibt SQL-Statements bei Bedarf aus
    19	)
    20	
    21	# Session-Factory
    22	AsyncSessionLocal = sessionmaker(
    23	    bind=engine,
    24	    class_=AsyncSession,
    25	    expire_on_commit=False
    26	)
    27	
    28	# FastAPI-Dependency für DB-Sessions
    29	async def get_db() -> AsyncSession:
    30	    async with AsyncSessionLocal() as session:
    31	        yield session


===== FILE: backend/app/__init__.py =====


===== FILE: backend/app/main.py =====
     1	from __future__ import annotations
     2	
     3	import logging
     4	import os
     5	from fastapi import FastAPI
     6	from fastapi.middleware.cors import CORSMiddleware
     7	from fastapi.responses import PlainTextResponse
     8	
     9	from app.api.v1.api import api_router
    10	from app.services.langgraph.graph.consult.build import build_consult_graph
    11	
    12	# Bevorzugte LLM-Factory (nutzt das zentrale LLM für WS/SSE)
    13	try:
    14	    from app.services.langgraph.llm_factory import get_llm as _make_llm  # hat meist streaming=True
    15	except Exception:  # Fallback nur, falls Modul nicht vorhanden
    16	    _make_llm = None  # type: ignore
    17	
    18	# Zweite Option: LLM-Factory aus der Consult-Config
    19	try:
    20	    from app.services.langgraph.graph.consult.config import create_llm as _create_llm_cfg
    21	except Exception:
    22	    _create_llm_cfg = None  # type: ignore
    23	
    24	# RAG-Orchestrator für Warmup
    25	try:
    26	    from app.services.rag import rag_orchestrator as ro  # enthält prewarm(), hybrid_retrieve, …
    27	except Exception:
    28	    ro = None  # type: ignore
    29	
    30	# ---- Access-Log-Filter: /health stummschalten ----
    31	class _HealthSilencer(logging.Filter):
    32	    def filter(self, record: logging.LogRecord) -> bool:
    33	        try:
    34	            msg = record.getMessage()
    35	        except Exception:
    36	            return True
    37	        return "/health" not in msg
    38	
    39	logging.getLogger("uvicorn.access").addFilter(_HealthSilencer())
    40	# ---------------------------------------------------
    41	
    42	log = logging.getLogger("uvicorn.error")
    43	
    44	
    45	def _init_llm():
    46	    """
    47	    Initialisiert ein Chat LLM für Streaming-Endpoints.
    48	    Robust gegen unterschiedliche Factory-Signaturen/Module.
    49	    """
    50	    # 1) Primär: zentrale LLM-Factory
    51	    if _make_llm:
    52	        try:
    53	            return _make_llm(streaming=True)  # neue Signatur
    54	        except TypeError:
    55	            # ältere Signatur ohne streaming-Param
    56	            return _make_llm()
    57	
    58	    # 2) Fallback: Consult-Config Factory
    59	    if _create_llm_cfg:
    60	        try:
    61	            return _create_llm_cfg(streaming=True)
    62	        except TypeError:
    63	            return _create_llm_cfg()
    64	
    65	    return None
    66	
    67	
    68	def create_app() -> FastAPI:
    69	    app = FastAPI(title="SealAI Backend", version=os.getenv("APP_VERSION", "dev"))
    70	
    71	    app.add_middleware(
    72	        CORSMiddleware,
    73	        allow_origins=os.getenv("CORS_ALLOW_ORIGINS", "*").split(","),
    74	        allow_credentials=True,
    75	        allow_methods=["*"],
    76	        allow_headers=["*"],
    77	    )
    78	
    79	    # Health für LB/Compose
    80	    @app.get("/health")
    81	    async def _health() -> PlainTextResponse:
    82	        return PlainTextResponse("ok")
    83	
    84	    # API v1
    85	    app.include_router(api_router, prefix="/api/v1")
    86	
    87	    @app.on_event("startup")
    88	    async def _startup():
    89	        # 1) LLM für Streaming-Endpoints initialisieren
    90	        try:
    91	            app.state.llm = _init_llm()
    92	            if app.state.llm is None:
    93	                raise RuntimeError("No LLM factory available")
    94	            log.info("LLM initialized for streaming endpoints.")
    95	        except Exception as e:
    96	            app.state.llm = None
    97	            log.warning("LLM init failed: %s", e)
    98	
    99	        # 2) RAG Warmup (Embedding, Reranker, Redis, Qdrant) – verhindert langen ersten Request
   100	        try:
   101	            if ro and hasattr(ro, "prewarm"):
   102	                ro.prewarm()
   103	                log.info("RAG prewarm completed.")
   104	            else:
   105	                log.info("RAG prewarm skipped (no ro.prewarm available).")
   106	        except Exception as e:
   107	            log.warning("RAG prewarm failed: %s", e)
   108	
   109	        # 3) Sync-Fallback-Graph (ohne Checkpointer) vorbereiten
   110	        try:
   111	            app.state.graph_sync = build_consult_graph().compile()
   112	            log.info("Consult graph compiled for sync fallback.")
   113	        except Exception as e:
   114	            app.state.graph_sync = None
   115	            log.warning("Graph compile failed: %s", e)
   116	
   117	        log.info("Startup: no prebuilt async graph (lazy build in chat_ws).")
   118	
   119	    return app
   120	
   121	
   122	app = create_app()


===== FILE: backend/app/models/beratungsergebnis.py =====
     1	from sqlalchemy import Column, Integer, String, DateTime, JSON
     2	from datetime import datetime
     3	
     4	# ✅ Gemeinsame Base aus app.database verwenden (keine eigene deklarieren)
     5	from app.database import Base
     6	
     7	class Beratungsergebnis(Base):
     8	    __tablename__ = "beratungsergebnisse"
     9	
    10	    id = Column(Integer, primary_key=True, index=True)
    11	    username = Column(String, index=True)
    12	    session_id = Column(String, index=True)
    13	    frage = Column(String)
    14	    parameter = Column(JSON)
    15	    antworten = Column(JSON)
    16	    timestamp = Column(DateTime, default=datetime.utcnow)


===== FILE: backend/app/models/chat_message.py =====
     1	# 📁 backend/app/models/chat_message.py
     2	
     3	from sqlalchemy import Column, Integer, String, Text, DateTime, func
     4	from app.database import Base
     5	
     6	class ChatMessage(Base):
     7	    __tablename__ = "chat_messages"
     8	
     9	    id = Column(Integer, primary_key=True, index=True)
    10	    username = Column(String, index=True)
    11	    session_id = Column(String, index=True)
    12	    role = Column(String)  # "user" oder "assistant"
    13	    content = Column(Text)
    14	    timestamp = Column(DateTime(timezone=True), server_default=func.now())


===== FILE: backend/app/models/form_result.py =====
     1	# 📁 backend/app/models/form_result.py
     2	from __future__ import annotations
     3	from sqlalchemy import Column, String, Float, DateTime
     4	from sqlalchemy.sql import func
     5	from app.database import Base
     6	
     7	class FormResult(Base):
     8	    __tablename__ = "form_results"
     9	
    10	    id = Column(String, primary_key=True, index=True)
    11	    username = Column(String, index=True)
    12	    radial_clearance = Column(Float, nullable=False)
    13	    tolerance_fit = Column(String, nullable=False)
    14	    result_text = Column(String, nullable=False)
    15	    created_at = Column(DateTime(timezone=True), server_default=func.now())


===== FILE: backend/app/models/long_term_memory.py =====
     1	# backend/app/models/long_term_memory.py
     2	
     3	from sqlalchemy import Column, Integer, String, Text, DateTime, func
     4	from app.database import Base
     5	
     6	class LongTermMemory(Base):
     7	    __tablename__ = "long_term_memory"
     8	    id = Column(Integer, primary_key=True, autoincrement=True)
     9	    user_id = Column(String, index=True, nullable=False)
    10	    key = Column(String, index=True, nullable=False)
    11	    value = Column(Text, nullable=False)
    12	    created_at = Column(DateTime(timezone=True), server_default=func.now())


===== FILE: backend/app/models/postgres_logger.py =====
     1	# 📁 backend/app/services/memory/postgres_logger.py
     2	
     3	from app.models.chat_message import ChatMessage
     4	from sqlalchemy.ext.asyncio import AsyncSession
     5	from sqlalchemy import select
     6	from typing import Literal
     7	
     8	async def log_message_to_db(
     9	    db: AsyncSession,
    10	    username: str,
    11	    session_id: str,
    12	    role: Literal["user", "assistant"],
    13	    content: str,
    14	):
    15	    message = ChatMessage(
    16	        username=username,
    17	        session_id=session_id,
    18	        role=role,
    19	        content=content
    20	    )
    21	    db.add(message)
    22	    await db.commit()
    23	
    24	async def get_messages_for_session(
    25	    db: AsyncSession,
    26	    username: str,
    27	    session_id: str
    28	):
    29	    result = await db.execute(
    30	        select(ChatMessage)
    31	        .where(ChatMessage.username == username)
    32	        .where(ChatMessage.session_id == session_id)
    33	        .order_by(ChatMessage.timestamp)
    34	    )
    35	    return result.scalars().all()


===== FILE: backend/app/redis_init.py =====
     1	import asyncio
     2	import os
     3	from langgraph.checkpoint.redis.aio import AsyncRedisSaver
     4	
     5	async def main():
     6	    saver = AsyncRedisSaver(redis_url=os.environ["REDIS_URL"])
     7	    await saver.asetup()      # idempotent
     8	    print("✅  RediSearch-Index angelegt/aktualisiert")
     9	
    10	asyncio.run(main())


===== FILE: backend/app/services/auth/dependencies.py =====
     1	# 📁 backend/app/services/auth/dependencies.py
     2	"""
     3	Auth-Dependencies für FastAPI-/WebSocket-Endpoints.
     4	
     5	* prüft Bearer-Token (Keycloak / OIDC)
     6	* liefert den User-Namen für Endpoints (HTTP & WS)
     7	* WS: akzeptiert neben "Authorization: Bearer <token>" auch ?token= / ?access_token=
     8	"""
     9	
    10	from __future__ import annotations
    11	
    12	from fastapi import Depends, HTTPException, WebSocket, status, Header
    13	
    14	from app.core.config import settings              # <-- korrekter Pfad!
    15	from app.services.auth.token import verify_access_token
    16	
    17	
    18	# --------------------------------------------------------------------------- #
    19	# 1) HTTP-Dependency – für normale FastAPI-Routes
    20	# --------------------------------------------------------------------------- #
    21	async def get_current_request_user(  # noqa: D401 (FastAPI-Namenskonvention)
    22	    authorization: str | None = Header(default=None),
    23	) -> str:
    24	    """
    25	    Liefert den `preferred_username` aus dem gültigen JWT,
    26	    sonst → 401 UNAUTHORIZED.
    27	    """
    28	    if not authorization or not authorization.startswith("Bearer "):
    29	        raise HTTPException(
    30	            status_code=status.HTTP_401_UNAUTHORIZED,
    31	            detail="Authorization header fehlt oder ungültig",
    32	        )
    33	
    34	    token = authorization.removeprefix("Bearer ").strip()
    35	    payload = verify_access_token(token)
    36	    return payload.get("preferred_username", "anonymous")
    37	
    38	
    39	# --------------------------------------------------------------------------- #
    40	# 2) WebSocket-Dependency – für Chat-Streaming
    41	#    (unterstützt Header *oder* Query-Parameter ?token=/ ?access_token=)
    42	# --------------------------------------------------------------------------- #
    43	async def get_current_ws_user(websocket: WebSocket) -> str:
    44	    """
    45	    Prüft beim WS-Handshake das Access Token.
    46	    Bevorzugt `Authorization: Bearer <token>`, fällt aber auf Query-Parameter
    47	    `?token=` oder `?access_token=` zurück (praktisch, da Browser-WS keine
    48	    Custom-Header setzen kann).
    49	    """
    50	    # 1) Versuche Authorization-Header
    51	    auth_header = websocket.headers.get("Authorization", "")
    52	    token: str | None = None
    53	    if auth_header.startswith("Bearer "):
    54	        token = auth_header.removeprefix("Bearer ").strip()
    55	
    56	    # 2) Fallback: Query-Parameter (z. B. ws://.../ws?token=xxx)
    57	    if not token:
    58	        qp = websocket.query_params
    59	        token = qp.get("token") or qp.get("access_token")
    60	
    61	    if not token:
    62	        await websocket.close(code=1008)  # Policy violation
    63	        raise HTTPException(
    64	            status_code=status.HTTP_401_UNAUTHORIZED,
    65	            detail="Kein Token gefunden (weder Authorization-Header noch Query-Param).",
    66	        )
    67	
    68	    payload = verify_access_token(token)
    69	    return payload.get("preferred_username", "anonymous")


===== FILE: backend/app/services/auth/__init__.py =====


===== FILE: backend/app/services/auth/jwt_utils.py =====
     1	# 📁 backend/app/services/auth/jwt_utils.py
     2	from fastapi import HTTPException, status
     3	
     4	def extract_username_from_payload(payload: dict) -> str:
     5	    """
     6	    Extrahiert den Nutzernamen aus dem bereits verifizierten JWT-Payload.
     7	    """
     8	    try:
     9	        return (
    10	            payload.get("preferred_username")
    11	            or payload.get("email")
    12	            or payload["sub"]
    13	        )
    14	    except KeyError:
    15	        raise HTTPException(
    16	            status_code=status.HTTP_401_UNAUTHORIZED,
    17	            detail="Username claim missing in token"
    18	        )


===== FILE: backend/app/services/auth/token.py =====
     1	"""
     2	Token-Utilities
     3	===============
     4	Verifiziert Keycloak-JWTs für das Backend (REST & WebSocket).
     5	Logging reduziert (keine sensiblen Daten), Algorithmen strikt auf RS256.
     6	"""
     7	
     8	from __future__ import annotations
     9	from app.core.config import settings
    10	import functools
    11	from typing import Any, Final
    12	import httpx
    13	from jose import jwt, JWTError
    14	import logging
    15	import base64
    16	import re
    17	import json
    18	
    19	log = logging.getLogger("uvicorn.error")
    20	
    21	REALM_ISSUER: Final[str] = settings.backend_keycloak_issuer
    22	JWKS_URL: Final[str] = settings.keycloak_jwks_url
    23	ALLOWED_AUDS: Final[set[str]] = {"nextauth", "sealai-backend-api"}
    24	ALLOWED_ALGS: Final[tuple[str, ...]] = ("RS256",)  # ✅ fixiert
    25	
    26	@functools.lru_cache(maxsize=1)
    27	def _get_jwks() -> dict[str, Any]:
    28	    resp = httpx.get(JWKS_URL, timeout=5.0, verify=True)
    29	    resp.raise_for_status()
    30	    return resp.json()
    31	
    32	def _get_key(kid: str) -> dict[str, Any]:
    33	    for key in _get_jwks()["keys"]:
    34	        if key["kid"] == kid:
    35	            return key
    36	    raise JWTError(f"kid {kid!r} not found in JWKS")
    37	
    38	def _jwk_to_pem(jwk: dict[str, Any]) -> str:
    39	    x5c = jwk.get("x5c")
    40	    if not x5c:
    41	        raise JWTError("x5c field missing in JWKS")
    42	    cert = x5c[0]
    43	    cert_str = "-----BEGIN CERTIFICATE-----\n"
    44	    cert_str += "\n".join(cert[i:i+64] for i in range(0, len(cert), 64))
    45	    cert_str += "\n-----END CERTIFICATE-----\n"
    46	    return cert_str
    47	
    48	def _safe_get_unverified_header(token: str) -> dict:
    49	    try:
    50	        header_b64 = token.split(".")[0]
    51	        header_b64 = re.sub(r'[^A-Za-z0-9_\-]', '', header_b64)
    52	        padded = header_b64 + "=" * (-len(header_b64) % 4)
    53	        decoded = base64.urlsafe_b64decode(padded).decode("utf-8")
    54	        return json.loads(decoded)
    55	    except Exception as exc:
    56	        log.debug("JWT header decode failed: %r", exc)
    57	        raise JWTError("Header decode fail") from exc
    58	
    59	def verify_access_token(token: str) -> dict[str, Any]:
    60	    """
    61	    * Gibt den vollständigen Claim-Dict zurück, wenn alles passt
    62	    * Löst ValueError aus, wenn der Token ungültig ist
    63	    """
    64	    try:
    65	        header = _safe_get_unverified_header(token)
    66	        kid = header.get("kid")
    67	        alg = header.get("alg")
    68	
    69	        if alg not in ALLOWED_ALGS:
    70	            raise JWTError(f"unsupported alg {alg!r}; allowed={ALLOWED_ALGS}")
    71	
    72	        jwk = _get_key(kid)
    73	        public_key_pem = _jwk_to_pem(jwk)
    74	
    75	        claims: dict[str, Any] = jwt.decode(
    76	            token,
    77	            public_key_pem,
    78	            algorithms=list(ALLOWED_ALGS),  # ✅ strikt
    79	            issuer=REALM_ISSUER,
    80	            options={"verify_aud": False},  # aud separat prüfen
    81	        )
    82	
    83	        # Audience/Client-Checks
    84	        aud = claims.get("aud")
    85	        aud_ok = (
    86	            (isinstance(aud, str)  and aud in ALLOWED_AUDS)
    87	            or (isinstance(aud, list) and any(a in ALLOWED_AUDS for a in aud))
    88	            or (claims.get("azp") in ALLOWED_AUDS)
    89	            or (claims.get("client_id") in ALLOWED_AUDS)
    90	        )
    91	        if not aud_ok:
    92	            raise JWTError(
    93	                f"audience not allowed (aud={aud!r}, azp={claims.get('azp')!r}, client_id={claims.get('client_id')!r})"
    94	            )
    95	
    96	        # Minimal-log (kein PEM/keine Claims)
    97	        log.debug("JWT verified (kid=%s, alg=%s, iss ok, aud ok)", kid, alg)
    98	        return claims
    99	
   100	    except Exception as exc:
   101	        log.warning("JWT verify failed: %s", exc)
   102	        raise ValueError(str(exc)) from exc


===== FILE: backend/app/services/__init__.py =====


===== FILE: backend/app/services/langgraph/agents/__init__.py =====


===== FILE: backend/app/services/langgraph/agents/material_agent.py =====
     1	import os
     2	from jinja2 import Environment, FileSystemLoader
     3	from langchain_openai import ChatOpenAI
     4	from langchain_core.messages import SystemMessage
     5	
     6	PROMPT_FILE = os.path.join(os.path.dirname(__file__), "..", "prompts", "material_agent.jinja2")
     7	
     8	def get_prompt(context=None):
     9	    env = Environment(loader=FileSystemLoader(os.path.dirname(PROMPT_FILE)))
    10	    template = env.get_template("material_agent.jinja2")
    11	    return template.render(context=context)
    12	
    13	class MaterialAgent:
    14	    name = "material_agent"
    15	
    16	    def __init__(self, context=None):
    17	        self.system_prompt = get_prompt(context)
    18	        self.llm = ChatOpenAI(
    19	            model=os.getenv("OPENAI_MODEL", "gpt-4.1-mini"),
    20	            api_key=os.getenv("OPENAI_API_KEY"),
    21	            base_url=os.getenv("OPENAI_BASE_URL") or None,
    22	            temperature=float(os.getenv("OPENAI_TEMPERATURE", "0")),
    23	            streaming=True,
    24	        )
    25	
    26	    def invoke(self, state):
    27	        messages = [SystemMessage(content=self.system_prompt)] + state["messages"]
    28	        return {"messages": self.llm.invoke(messages)}
    29	
    30	def get_material_agent(context=None):
    31	    return MaterialAgent(context)


===== FILE: backend/app/services/langgraph/agents/profile_agent.py =====
     1	# backend/app/services/langgraph/agents/profile_agent.py
     2	from __future__ import annotations
     3	
     4	from typing import Any, Dict, List
     5	
     6	from langchain_core.messages import SystemMessage, AIMessage, HumanMessage
     7	from app.services.langgraph.llm_factory import get_llm
     8	from app.services.langgraph.prompting import render_template
     9	
    10	def _last_user_text(messages: List[Any]) -> str:
    11	    for m in reversed(messages or []):
    12	        c = getattr(m, "content", None)
    13	        if isinstance(c, str) and c.strip():
    14	            return c.strip()
    15	    return ""
    16	
    17	class ProfileAgent:
    18	    name = "profile_agent"
    19	
    20	    def __init__(self) -> None:
    21	        self.llm = get_llm()
    22	
    23	    def invoke(self, state: Dict[str, Any]) -> Dict[str, Any]:
    24	        msgs = state.get("messages") or []
    25	        query = _last_user_text(msgs)
    26	        system_prompt = render_template("profile_agent.jinja2", query=query)
    27	        response = self.llm.invoke([SystemMessage(content=system_prompt)] + msgs)
    28	        ai = response if isinstance(response, AIMessage) else AIMessage(content=getattr(response, "content", "") or "")
    29	        return {"messages": [ai]}
    30	
    31	def get_profile_agent() -> ProfileAgent:
    32	    return ProfileAgent()


===== FILE: backend/app/services/langgraph/domains/base.py =====
     1	# -*- coding: utf-8 -*-
     2	from __future__ import annotations
     3	import os
     4	import yaml
     5	from dataclasses import dataclass
     6	from typing import Dict, Any, Tuple, List, Optional, Callable
     7	
     8	
     9	@dataclass
    10	class DomainSpec:
    11	    id: str
    12	    name: str
    13	    base_dir: str            # Ordner der Domain (für Prompts/Schema)
    14	    schema_file: str         # relativer Pfad
    15	    calculator: Callable[[dict], Dict[str, Any]]  # compute(params) -> {'calculated': ..., 'flags': ...}
    16	    ask_order: List[str]     # Reihenfolge der Nachfragen (falls fehlt)
    17	
    18	    def template_dir(self) -> str:
    19	        return os.path.join(self.base_dir, "prompts")
    20	
    21	    def schema_path(self) -> str:
    22	        return os.path.join(self.base_dir, self.schema_file)
    23	
    24	_REGISTRY: Dict[str, DomainSpec] = {}
    25	
    26	def register_domain(spec: DomainSpec) -> None:
    27	    _REGISTRY[spec.id] = spec
    28	    # Domain-Prompts dem Jinja-Loader bekannt machen
    29	
    30	def get_domain(domain_id: str) -> Optional[DomainSpec]:
    31	    return _REGISTRY.get(domain_id)
    32	
    33	def list_domains() -> List[str]:
    34	    return list(_REGISTRY.keys())
    35	
    36	# -------- YAML Schema Laden & Validieren (leichtgewichtig) ----------
    37	def load_schema(spec: DomainSpec) -> Dict[str, Any]:
    38	    with open(spec.schema_path(), "r", encoding="utf-8") as f:
    39	        return yaml.safe_load(f) or {}
    40	
    41	def validate_params(spec: DomainSpec, params: Dict[str, Any]) -> Tuple[List[str], List[str]]:
    42	    """
    43	    Gibt (errors, warnings) zurück.
    44	    YAML-Schema Felder:
    45	      fields:
    46	        <name>:
    47	          required: bool
    48	          type: str ('int'|'float'|'str'|'enum')
    49	          min: float
    50	          max: float
    51	          enum: [..]
    52	          ask_if: optional (Dependency-Hinweis, nur Info)
    53	    """
    54	    schema = load_schema(spec)
    55	    fields = schema.get("fields", {})
    56	    errors: List[str] = []
    57	    warnings: List[str] = []
    58	
    59	    def _typename(x):
    60	        if isinstance(x, bool):   # bool ist auch int in Python
    61	            return "bool"
    62	        if isinstance(x, int):
    63	            return "int"
    64	        if isinstance(x, float):
    65	            return "float"
    66	        if isinstance(x, str):
    67	            return "str"
    68	        return type(x).__name__
    69	
    70	    for key, rule in fields.items():
    71	        req = bool(rule.get("required", False))
    72	        if req and (key not in params or params.get(key) in (None, "")):
    73	            errors.append(f"Pflichtfeld fehlt: {key}")
    74	            continue
    75	        if key not in params or params.get(key) in (None, ""):
    76	            continue
    77	
    78	        val = params.get(key)
    79	        typ = rule.get("type")
    80	        if typ == "enum":
    81	            allowed = rule.get("enum", [])
    82	            if val not in allowed:
    83	                errors.append(f"{key}: ungültiger Wert '{val}', erlaubt: {allowed}")
    84	        elif typ == "int":
    85	            if not isinstance(val, int):
    86	                # ints können als float ankommen (LLM) – tolerant casten
    87	                try:
    88	                    params[key] = int(float(val))
    89	                except Exception:
    90	                    errors.append(f"{key}: erwartet int, erhalten {_typename(val)}")
    91	            else:
    92	                # ok
    93	                pass
    94	        elif typ == "float":
    95	            if isinstance(val, (int, float)):
    96	                params[key] = float(val)
    97	            else:
    98	                try:
    99	                    params[key] = float(str(val).replace(",", "."))
   100	                except Exception:
   101	                    errors.append(f"{key}: erwartet float, erhalten {_typename(val)}")
   102	        elif typ == "str":
   103	            if not isinstance(val, str):
   104	                params[key] = str(val)
   105	
   106	        # Ranges
   107	        if isinstance(params.get(key), (int, float)):
   108	            v = float(params[key])
   109	            if "min" in rule and v < float(rule["min"]):
   110	                errors.append(f"{key}: {v} < min {rule['min']}")
   111	            if "max" in rule and v > float(rule["max"]):
   112	                warnings.append(f"{key}: {v} > empfohlene Obergrenze {rule['max']}")
   113	
   114	    return errors, warnings


===== FILE: backend/app/services/langgraph/domains/hydraulics_rod/calculator.py =====
     1	# Hydraulik – Stangendichtung: deterministische Checks
     2	from typing import Dict, Any
     3	
     4	def _to_float(v, default=None):
     5	    try:
     6	        if v is None or v == "" or v == "unknown":
     7	            return default
     8	        return float(v)
     9	    except Exception:
    10	        return default
    11	
    12	def compute(params: Dict[str, Any]) -> Dict[str, Any]:
    13	    # Pflicht-/Kernparameter
    14	    p_bar   = _to_float(params.get("druck_bar"))
    15	    t_max   = _to_float(params.get("temp_max_c"))
    16	    speed   = _to_float(params.get("geschwindigkeit_m_s"))  # optional
    17	    bore    = _to_float(params.get("nut_d_mm"))              # ✅ Nut-Ø D (mm)
    18	    rod     = _to_float(params.get("stange_mm"))             # ✅ Stangen-Ø (mm)
    19	
    20	    flags = {}
    21	    warnings = []
    22	    reqs = []
    23	
    24	    # Extrusionsrisiko grob ab ~160–200 bar (ohne Stützring / je nach Spalt)
    25	    if p_bar is not None and p_bar >= 160:
    26	        flags["extrusion_risk"] = True
    27	        reqs.append("Stütz-/Back-up-Ring prüfen (≥160 bar).")
    28	
    29	    if t_max is not None and t_max > 100:
    30	        warnings.append(f"Hohe Temperatur ({t_max:.0f} °C) – Werkstoffwahl prüfen.")
    31	
    32	    if speed is not None and speed > 0.6:
    33	        warnings.append(f"Hohe Stangengeschwindigkeit ({speed:.2f} m/s) – Reibung/Stick-Slip beachten.")
    34	
    35	    # Plausibilitäts-Hinweis (Spaltmaß sehr klein)
    36	    if bore and rod and bore - rod < 2.0:
    37	        warnings.append("Sehr kleiner Spalt zwischen Bohrung und Stange (< 2 mm).")
    38	
    39	    return {
    40	        "calculated": {
    41	            "druck_bar": p_bar,
    42	            "temp_max_c": t_max,
    43	            "geschwindigkeit_m_s": speed,
    44	            "bohrung_mm": bore,
    45	            "stange_mm": rod,
    46	        },
    47	        "flags": flags,
    48	        "warnings": warnings,
    49	        "requirements": reqs,
    50	    }


===== FILE: backend/app/services/langgraph/domains/hydraulics_rod/__init__.py =====
     1	# -*- coding: utf-8 -*-
     2	import os
     3	from typing import Dict, Any
     4	from app.services.langgraph.domains.base import DomainSpec, register_domain
     5	from .calculator import compute as hyd_rod_compute
     6	
     7	def register() -> None:
     8	    base_dir = os.path.dirname(os.path.abspath(__file__))
     9	    spec = DomainSpec(
    10	        id="hydraulics_rod",
    11	        name="Hydraulik – Stangendichtung",
    12	        base_dir=base_dir,
    13	        schema_file="schema.yaml",
    14	        calculator=hyd_rod_compute,
    15	        ask_order=[
    16	            "falltyp", "stange_mm", "nut_d_mm", "nut_b_mm", "druck_bar",
    17	            "geschwindigkeit_m_s", "medium", "temp_max_c"
    18	        ],
    19	    )
    20	    register_domain(spec)


===== FILE: backend/app/services/langgraph/domains/hydraulics_rod/prompts/extract_params.jinja2 =====
     1	{# Hydraulik Stange – Param-Extraktion #}
     2	Antworte ausschließlich mit einem kompakten JSON in einer Zeile, ohne Markdown/Erklärung.
     3	
     4	Bekannte Parameter:
     5	{{ existing_params_json }}
     6	
     7	Dialog:
     8	{% for m in messages %}
     9	- {{ m.type|lower }}: {{ m.content }}
    10	{% endfor %}
    11	
    12	Setze nur wenn sicher:
    13	- falltyp: "ersatz"|"neu"|"optimierung"
    14	- stange_mm, nut_d_mm, nut_b_mm (Zahlen, mm)
    15	- druck_bar (Zahl)
    16	- geschwindigkeit_m_s (Zahl)
    17	- medium (String)
    18	- temp_max_c (Zahl)
    19	
    20	Beispiel:
    21	{"falltyp":"ersatz","stange_mm":50,"nut_d_mm":60,"nut_b_mm":7,"druck_bar":180,"geschwindigkeit_m_s":0.25,"medium":"Hydrauliköl","temp_max_c":80}


===== FILE: backend/app/services/langgraph/domains/hydraulics_rod/prompts/__init__.py =====


===== FILE: backend/app/services/langgraph/domains/hydraulics_rod/prompts/material.jinja2 =====
     1	Du bist Materialexperte für Hydraulik-Stangendichtungen.
     2	- Parameter: {{ params }}
     3	- Berechnungen/Flags: {{ derived }}
     4	
     5	Empfiehl 1–3 Werkstoffe/Kombis (z. B. PU, PTFE+Bronze, NBR Stützring).
     6	Begründe kurz.
     7	Antwort JSON einzeilig:
     8	{"materiale":[{"werkstoff":"PU","begruendung":"..."}]}


===== FILE: backend/app/services/langgraph/domains/hydraulics_rod/prompts/profile.jinja2 =====
     1	Du bist Profil-Agent für Hydraulik-Stangendichtungen.
     2	- Parameter: {{ params }}
     3	- Berechnungen/Flags: {{ derived }}
     4	
     5	Schlage 1–2 Profilfamilien vor (z. B. U-Ring PU, PTFE-Kombiring mit O-Ring-Vorspann).
     6	Kurz begründen.
     7	Antwort (einzeilig):
     8	{"profile":[{"profil":"PU-U-Ring","begruendung":"..."}]}


===== FILE: backend/app/services/langgraph/domains/hydraulics_rod/prompts/type.jinja2 =====
     1	Du bist Typ/Serien-Agent (Hydraulik Stange).
     2	- Parameter: {{ params }}
     3	- Abgeleitete Werte/Flags: {{ derived }}
     4	- Profil/Werkstoff (falls vorhanden): {{ prior }}
     5	
     6	Nenne 1–2 konkrete Typen/Serien (Herstellerneutral beschreiben).
     7	Antwort (einzeilig):
     8	{"typen":[{"typ":"PU U-Ring für {{params.stange_mm}} mm / Nut {{params.nut_d_mm}}x{{params.nut_b_mm}}","begruendung":"..."}]}


===== FILE: backend/app/services/langgraph/domains/hydraulics_rod/schema.yaml =====
     1	fields:
     2	  falltyp:
     3	    required: true
     4	    type: enum
     5	    enum: ["ersatz", "neu", "optimierung"]
     6	  stange_mm:
     7	    required: true
     8	    type: float
     9	    min: 4
    10	    max: 400
    11	  nut_d_mm:
    12	    required: true
    13	    type: float
    14	    min: 6
    15	    max: 500
    16	  nut_b_mm:
    17	    required: true
    18	    type: float
    19	    min: 2
    20	    max: 50
    21	  druck_bar:
    22	    required: true
    23	    type: float
    24	    min: 0
    25	    max: 500
    26	  geschwindigkeit_m_s:
    27	    required: false
    28	    type: float
    29	    min: 0
    30	    max: 15
    31	  medium:
    32	    required: true
    33	    type: str
    34	  temp_max_c:
    35	    required: true
    36	    type: float
    37	    min: -60
    38	    max: 200


===== FILE: backend/app/services/langgraph/domains/__init__.py =====
     1	# -*- coding: utf-8 -*-
     2	# Stellt sicher, dass Domains beim Import registriert werden.
     3	from .rwdr import register as register_rwdr
     4	from .hydraulics_rod import register as register_hydraulics_rod
     5	
     6	def register_all_domains() -> None:
     7	    register_rwdr()
     8	    register_hydraulics_rod()


===== FILE: backend/app/services/langgraph/domains/rwdr/calculator.py =====
     1	# backend/app/services/langgraph/domains/rwdr/calculator.py
     2	from __future__ import annotations
     3	from typing import Dict, Any
     4	import math
     5	
     6	
     7	def _to_float(x, default=0.0):
     8	    try:
     9	        if x is None:
    10	            return default
    11	        if isinstance(x, (int, float)):
    12	            return float(x)
    13	        s = str(x).replace(" ", "").replace(".", "").replace(",", ".")
    14	        return float(s)
    15	    except Exception:
    16	        return default
    17	
    18	
    19	def compute(params: Dict[str, Any]) -> Dict[str, Any]:
    20	    p = params or {}
    21	    out = {"calculated": {}, "flags": {}, "warnings": [], "requirements": []}
    22	
    23	    d_mm = _to_float(p.get("wellen_mm"))
    24	    rpm = _to_float(p.get("drehzahl_u_min"))
    25	    t_max = _to_float(p.get("temp_max_c"))
    26	    press_bar = _to_float(p.get("druck_bar"))
    27	    medium = (p.get("medium") or "").lower()
    28	    bauform = (p.get("bauform") or "").upper()
    29	
    30	    # Umfangsgeschwindigkeit [m/s]
    31	    v = 0.0
    32	    if d_mm > 0 and rpm > 0:
    33	        v = math.pi * (d_mm / 1000.0) * (rpm / 60.0)
    34	    v = round(v, 3)
    35	
    36	    # Beide Keys setzen (Deutsch+Englisch), damit Templates/Alt-Code beides finden
    37	    out["calculated"]["umfangsgeschwindigkeit_m_s"] = v
    38	    out["calculated"]["surface_speed_m_s"] = v
    39	
    40	    # Flags
    41	    if press_bar > 2.0:
    42	        out["flags"]["requires_pressure_stage"] = True
    43	    if v >= 20.0:
    44	        out["flags"]["speed_high"] = True
    45	    if t_max >= 120.0:
    46	        out["flags"]["temp_very_high"] = True
    47	
    48	    # Material-Guidance (Whitelist/Blacklist)
    49	    whitelist, blacklist = set(), set()
    50	
    51	    # RWDR Bauform BA: Standard ist Elastomer-Lippe (NBR/FKM). PTFE nur Spezialprofile.
    52	    if bauform.startswith("BA"):
    53	        blacklist.add("PTFE")
    54	        if any(k in medium for k in ("hydraulik", "öl", "oel", "oil")):
    55	            if t_max <= 100:
    56	                whitelist.update(["NBR", "FKM"])   # NBR präferiert, FKM ok
    57	            else:
    58	                whitelist.add("FKM")
    59	                blacklist.add("NBR")
    60	        else:
    61	            whitelist.update(["FKM", "NBR"])
    62	
    63	    # Druckrestriktion für PTFE (Standard-RWDR): ab ~0.5 bar vermeiden
    64	    if press_bar > 0.5:
    65	        blacklist.add("PTFE")
    66	
    67	    # Chemie / sehr hohe Temp → PTFE als mögliche Alternative zulassen
    68	    if any(k in medium for k in ("chem", "lösemittel", "loesemittel", "solvent")) or t_max > 180:
    69	        whitelist.add("PTFE")
    70	
    71	    out["calculated"]["material_whitelist"] = sorted(whitelist) if whitelist else []
    72	    out["calculated"]["material_blacklist"] = sorted(blacklist) if blacklist else []
    73	
    74	    # Anforderungen (menschlich lesbar)
    75	    if whitelist:
    76	        out["requirements"].append("Bevorzuge Materialien: " + ", ".join(sorted(whitelist)))
    77	    if blacklist:
    78	        out["requirements"].append("Vermeide Materialien: " + ", ".join(sorted(blacklist)))
    79	    if out["flags"].get("requires_pressure_stage"):
    80	        out["requirements"].append("Druckstufe oder Drucktaugliches Profil erforderlich (>2 bar).")
    81	    if out["flags"].get("speed_high"):
    82	        out["requirements"].append("Hohe Umfangsgeschwindigkeit (>= 20 m/s) berücksichtigen.")
    83	
    84	    return out


===== FILE: backend/app/services/langgraph/domains/rwdr/__init__.py =====
     1	# -*- coding: utf-8 -*-
     2	import os
     3	from typing import Dict, Any
     4	from app.services.langgraph.domains.base import DomainSpec, register_domain
     5	from .calculator import compute as rwdr_compute
     6	
     7	def register() -> None:
     8	    base_dir = os.path.dirname(os.path.abspath(__file__))
     9	    spec = DomainSpec(
    10	        id="rwdr",
    11	        name="Radialwellendichtring",
    12	        base_dir=base_dir,
    13	        schema_file="schema.yaml",
    14	        calculator=rwdr_compute,
    15	        ask_order=[
    16	            "falltyp", "bauform", "wellen_mm", "gehause_mm", "breite_mm",
    17	            "medium", "temp_max_c", "druck_bar", "drehzahl_u_min"
    18	        ],
    19	    )
    20	    register_domain(spec)


===== FILE: backend/app/services/langgraph/domains/rwdr/prompts/extract_params.jinja2 =====
     1	{# RWDR: Parameter-Extraktion #}
     2	Du extrahierst strukturierte Parameter aus dem Dialog. Antworte ausschließlich mit einem kompakten JSON-Objekt **in einer Zeile**, ohne Markdown oder Erklärungen.
     3	
     4	Bekannte Parameter (nur ergänzen, überschreiben nur wenn klar genannt):
     5	{{ existing_params_json }}
     6	
     7	Dialog (neueste zuerst):
     8	{% for m in messages %}
     9	- {{ m.type|lower }}: {{ m.content }}
    10	{% endfor %}
    11	
    12	Setze nur, wenn sicher:
    13	- falltyp: "ersatz"|"neu"|"optimierung"
    14	- bauform: z.B. "BA","BASL","B1","B2"
    15	- wellen_mm, gehause_mm, breite_mm (Zahlen, mm)
    16	- medium: String (z. B. "Hydrauliköl ISO VG 46")
    17	- temp_max_c: Zahl (°C)
    18	- druck_bar: Zahl (bar)
    19	- drehzahl_u_min: Zahl (U/min)
    20	- optionals: umgebung, prioritaet, besondere_anforderungen, bekannte_probleme
    21	
    22	Antwortformat (einzeiliges JSON):
    23	{"falltyp":"ersatz","bauform":"BA","wellen_mm":45,"gehause_mm":62,"breite_mm":7,"medium":"Hydrauliköl","temp_max_c":80,"druck_bar":0,"drehzahl_u_min":1200}


===== FILE: backend/app/services/langgraph/domains/rwdr/prompts/__init__.py =====


===== FILE: backend/app/services/langgraph/domains/rwdr/prompts/material.jinja2 =====
     1	Du bist Materialexperte für RWDR.
     2	Eingaben:
     3	- Parameter: {{ params }}
     4	- Berechnungen/Flags: {{ derived }}
     5	
     6	Aufgabe:
     7	- Empfiehl 1–3 plausible Werkstoffe (z. B. NBR, FKM, PTFE …) passend zu Medium, Temperatur, Umfangsgeschwindigkeit, Druck.
     8	- Kurze Begründung je Werkstoff.
     9	Antworte als JSON (einzeilig):
    10	{"materiale":[{"werkstoff":"FKM","begruendung":"..."}]}


===== FILE: backend/app/services/langgraph/domains/rwdr/prompts/profile.jinja2 =====
     1	Du bist Profil-Agent für RWDR.
     2	Gegeben:
     3	- Parameter: {{ params }}
     4	- Berechnungen/Flags: {{ derived }}
     5	
     6	Aufgabe:
     7	- Schlage 1–2 Profile/Bauformen (z. B. BA, BASL, B1/B2) vor.
     8	- Kurze Begründung.
     9	Antwort (einzeiliges JSON):
    10	{"profile":[{"bauform":"BA","begruendung":"..."}]}


===== FILE: backend/app/services/langgraph/domains/rwdr/prompts/type.jinja2 =====
     1	Du bist Typ/Serien-Agent für RWDR.
     2	Gegeben:
     3	- Parameter: {{ params }}
     4	- Berechnungen/Flags: {{ derived }}
     5	- Ausgewähltes Profil/Werkstoffe (falls vorhanden): {{ prior }}
     6	
     7	Aufgabe:
     8	- Benenne 1–2 konkrete Typangaben (z. B. "BA 45x62x7") basierend auf Abmessungen + Profil.
     9	Antwort (einzeiliges JSON):
    10	{"typen":[{"typ":"BA {{params.wellen_mm}}x{{params.gehause_mm}}x{{params.breite_mm}}","begruendung":"..."}]}


===== FILE: backend/app/services/langgraph/domains/rwdr/schema.yaml =====
     1	# RWDR Param-Schema (leichtgewichtig)
     2	fields:
     3	  falltyp:
     4	    required: true
     5	    type: enum
     6	    enum: ["ersatz", "neu", "optimierung"]
     7	  bauform:
     8	    required: false
     9	    type: str
    10	  wellen_mm:
    11	    required: true
    12	    type: float
    13	    min: 1
    14	    max: 500
    15	  gehause_mm:
    16	    required: true
    17	    type: float
    18	    min: 1
    19	    max: 800
    20	  breite_mm:
    21	    required: true
    22	    type: float
    23	    min: 1
    24	    max: 50
    25	  medium:
    26	    required: true
    27	    type: str
    28	  temp_max_c:
    29	    required: true
    30	    type: float
    31	    min: -60
    32	    max: 250
    33	  druck_bar:
    34	    required: true
    35	    type: float
    36	    min: 0
    37	    max: 25
    38	  drehzahl_u_min:
    39	    required: true
    40	    type: int
    41	    min: 1
    42	    max: 30000
    43	  umgebung:
    44	    required: false
    45	    type: str
    46	  prioritaet:
    47	    required: false
    48	    type: str
    49	  besondere_anforderungen:
    50	    required: false
    51	    type: str
    52	  bekannte_probleme:
    53	    required: false
    54	    type: str


===== FILE: backend/app/services/langgraph/examples/__init__.py =====


===== FILE: backend/app/services/langgraph/examples/rfq_export_example.json =====
     1	{
     2	  "params": {
     3	    "shaft_d": 16,
     4	    "housing_d": 22,
     5	    "width": 4,
     6	    "medium": "water",
     7	    "pressure_bar": 1.5,
     8	    "temp_max_c": 80,
     9	    "speed_rpm": 450
    10	  },
    11	  "derived": {
    12	    "v_m_s": 0.377,
    13	    "dn_value": 7200,
    14	    "pv_indicator_bar_ms": 0.5655
    15	  },
    16	  "candidates": [
    17	    {
    18	      "doc_id": "rwdr-0001",
    19	      "vendor_id": "VEND_A",
    20	      "title": "RWDR TC 16x22x4 FKM",
    21	      "profile": "TC",
    22	      "material": "FKM",
    23	      "paid_tier": "gold",
    24	      "contract_valid_until": "2026-12-31",
    25	      "active": true,
    26	      "score": 0.92,
    27	      "url": "https://partner-a.example/rwdr-tc-16x22x4"
    28	    }
    29	  ],
    30	  "sources": ["Partner A Datenblatt RWDR TC 16x22x4 v1.3"],
    31	  "legal_notice": "Verbindliche Eignungszusage obliegt dem Hersteller."
    32	}


===== FILE: backend/app/services/langgraph/graph/consult/build.py =====
     1	from __future__ import annotations
     2	
     3	import logging
     4	from typing import Any, Dict, List
     5	from langgraph.graph import StateGraph, END
     6	
     7	from .state import ConsultState
     8	from .utils import normalize_messages
     9	from .domain_router import detect_domain
    10	from .domain_runtime import compute_domain
    11	
    12	from .nodes.intake import intake_node
    13	from .nodes.ask_missing import ask_missing_node
    14	from .nodes.validate import validate_node
    15	from .nodes.recommend import recommend_node
    16	from .nodes.explain import explain_node
    17	from .nodes.calc_agent import calc_agent_node  # ⬅ NEU: Kalkulations-Agent
    18	
    19	# RAG + Answer-Validation
    20	from .nodes.rag import run_rag_node
    21	from .nodes.validate_answer import validate_answer
    22	
    23	from .heuristic_extract import pre_extract_params
    24	from .extract import extract_params_with_llm
    25	from .config import create_llm
    26	
    27	log = logging.getLogger("uvicorn.error")
    28	
    29	
    30	# ---------------------- helpers ----------------------
    31	def _join_user_text(msgs: List) -> str:
    32	    out: List[str] = []
    33	    for m in msgs:
    34	        role = (getattr(m, "type", "") or getattr(m, "role", "")).lower()
    35	        content = getattr(m, "content", "")
    36	        if isinstance(m, dict):
    37	            role = (m.get("type") or m.get("role") or "").lower()
    38	            content = m.get("content")
    39	        if role in ("human", "user") and isinstance(content, str) and content.strip():
    40	            out.append(content.strip())
    41	    return "\n".join(out)
    42	
    43	
    44	def _merge_seed_first(seed: Dict[str, Any], llm_out: Dict[str, Any]) -> Dict[str, Any]:
    45	    out = dict(llm_out or {})
    46	    for k, v in (seed or {}).items():
    47	        if v not in (None, "", []):
    48	            out[k] = v
    49	    return out
    50	
    51	
    52	def _compact_param_summary(domain: str, params: Dict[str, Any]) -> str:
    53	    p = params or {}
    54	    parts: List[str] = []
    55	
    56	    if domain == "rwdr":
    57	        parts.append("RWDR")
    58	        if p.get("abmessung"):
    59	            parts.append(str(p["abmessung"]))
    60	        elif p.get("wellen_mm") and p.get("gehause_mm") and p.get("breite_mm"):
    61	            parts.append(f'{p["wellen_mm"]}x{p["gehause_mm"]}x{p["breite_mm"]}')
    62	    elif domain == "hydraulics_rod":
    63	        parts.append("Hydraulik Stangendichtung")
    64	
    65	    if p.get("medium"):
    66	        parts.append(str(p["medium"]))
    67	    if p.get("temp_max_c") or p.get("tmax_c"):
    68	        parts.append(f'Tmax {int(p.get("temp_max_c") or p.get("tmax_c"))} °C')
    69	    if p.get("druck_bar"):
    70	        parts.append(f'Druck {p["druck_bar"]} bar')
    71	    if p.get("drehzahl_u_min"):
    72	        parts.append(f'{int(p["drehzahl_u_min"])} U/min')
    73	    if p.get("relativgeschwindigkeit_ms"):
    74	        parts.append(f'v≈{float(p["relativgeschwindigkeit_ms"]):.2f} m/s')
    75	
    76	    bl = p.get("material_blacklist") or p.get("vermeide_materialien")
    77	    wl = p.get("material_whitelist") or p.get("bevorzugte_materialien")
    78	    if bl:
    79	        parts.append(f'Vermeide: {bl}')
    80	    if wl:
    81	        parts.append(f'Bevorzugt: {wl}')
    82	
    83	    return ", ".join(parts)
    84	
    85	
    86	# ---------------------- nodes ----------------------
    87	def _extract_node(state: Dict[str, Any]) -> Dict[str, Any]:
    88	    msgs = normalize_messages(state.get("messages", []))
    89	    params = dict(state.get("params") or {})
    90	    user_text = _join_user_text(msgs)
    91	
    92	    heur = pre_extract_params(user_text)
    93	    seed = {**params, **{k: v for k, v in heur.items() if v not in (None, "", [])}}
    94	
    95	    llm = create_llm(streaming=False)
    96	    llm_params = extract_params_with_llm(llm, msgs, seed)
    97	    final_params = _merge_seed_first(seed, llm_params)
    98	    return {**state, "params": final_params, "phase": "extract"}
    99	
   100	
   101	def _domain_router_node(state: Dict[str, Any]) -> Dict[str, Any]:
   102	    msgs = normalize_messages(state.get("messages", []))
   103	    params = dict(state.get("params") or {})
   104	    try:
   105	        domain = detect_domain(None, msgs, params) or "rwdr"
   106	        domain = domain.strip().lower()
   107	    except Exception:
   108	        domain = "rwdr"
   109	    return {**state, "domain": domain, "phase": "domain_router"}
   110	
   111	
   112	def _compute_node(state: Dict[str, Any]) -> Dict[str, Any]:
   113	    domain = (state.get("domain") or "rwdr").strip().lower()
   114	    params = dict(state.get("params") or {})
   115	    derived = compute_domain(domain, params) or {}
   116	
   117	    # kompatible Aliasse nachziehen (ohne doppelt zu rechnen)
   118	    alias_map = {
   119	        "tmax_c": params.get("temp_max_c"),
   120	        "temp_c": params.get("temp_max_c"),
   121	        "druck": params.get("druck_bar"),
   122	        "pressure_bar": params.get("druck_bar"),
   123	        "n_u_min": params.get("drehzahl_u_min"),
   124	        "rpm": params.get("drehzahl_u_min"),
   125	        "v_ms": params.get("relativgeschwindigkeit_ms"),
   126	    }
   127	    for k, v in alias_map.items():
   128	        if k not in params and v not in (None, "", []):
   129	            params[k] = v
   130	
   131	    return {**state, "params": params, "derived": derived, "phase": "compute"}
   132	
   133	
   134	def _prepare_query_node(state: Dict[str, Any]) -> Dict[str, Any]:
   135	    if (state.get("query") or "").strip():
   136	        return {**state, "phase": "prepare_query"}
   137	
   138	    msgs = normalize_messages(state.get("messages", []))
   139	    params = dict(state.get("params") or {})
   140	    domain = (state.get("domain") or "rwdr").strip().lower()
   141	
   142	    user_text = _join_user_text(msgs)
   143	    param_str = _compact_param_summary(domain, params)
   144	
   145	    prefix = "RWDR" if domain == "rwdr" else "Hydraulik"
   146	    query = ", ".join([s for s in [prefix, user_text, param_str] if s])
   147	
   148	    new_state = dict(state)
   149	    new_state["query"] = query
   150	    return {**new_state, "phase": "prepare_query"}
   151	
   152	
   153	def _respond_node(state: Dict[str, Any]) -> Dict[str, Any]:
   154	    return {**state, "phase": "respond"}
   155	
   156	
   157	# ---------- routing helpers ----------
   158	def _ask_or_ok(state: Dict[str, Any]) -> str:
   159	    p = state.get("params") or {}
   160	
   161	    def has(v: Any) -> bool:
   162	        if v is None: return False
   163	        if isinstance(v, (list, dict)) and not v: return False
   164	        if isinstance(v, str) and not v.strip(): return False
   165	        return True
   166	
   167	    base_ok = has(p.get("temp_max_c")) and has(p.get("druck_bar"))
   168	    rel_ok  = has(p.get("relativgeschwindigkeit_ms")) or (has(p.get("wellen_mm")) and has(p.get("drehzahl_u_min")))
   169	
   170	    msgs = normalize_messages(state.get("messages", []))
   171	    user = (_join_user_text(msgs) or "").lower()
   172	    info_triggers = (
   173	        "was weißt du", "was weisst du", "what do you know", "info",
   174	        "rag", "rag:", "kyrolon 79x", "ptfe", "datenblatt", "sds"
   175	    )
   176	    if not (base_ok and rel_ok) and any(t in user for t in info_triggers):
   177	        return "info"
   178	
   179	    return "ok" if (base_ok and rel_ok) else "ask"
   180	
   181	
   182	def _after_rag(state: Dict[str, Any]) -> str:
   183	    p = state.get("params") or {}
   184	
   185	    def has(v: Any) -> bool:
   186	        if v is None: return False
   187	        if isinstance(v, (list, dict)) and not v: return False
   188	        if isinstance(v, str) and not v.strip(): return False
   189	        return True
   190	
   191	    base_ok = has(p.get("temp_max_c")) and has(p.get("druck_bar"))
   192	    rel_ok  = has(p.get("relativgeschwindigkeit_ms")) or (has(p.get("wellen_mm")) and has(p.get("drehzahl_u_min")))
   193	    docs    = state.get("retrieved_docs") or state.get("docs") or []
   194	    ctx_ok  = bool(docs) or bool(state.get("context"))
   195	
   196	    return "recommend" if (base_ok and rel_ok and ctx_ok) else "explain"
   197	
   198	
   199	# ---------------------- graph ----------------------
   200	def build_graph() -> StateGraph:
   201	    log.info("[ConsultGraph] Initialisierung…")
   202	    g = StateGraph(ConsultState)
   203	
   204	    g.add_node("intake", intake_node)
   205	    g.add_node("extract", _extract_node)
   206	    g.add_node("domain_router", _domain_router_node)
   207	    g.add_node("compute", _compute_node)
   208	    g.add_node("calc_agent", calc_agent_node)          # ⬅ NEU im Graph
   209	    g.add_node("ask_missing", ask_missing_node)
   210	    g.add_node("validate", validate_node)
   211	    g.add_node("prepare_query", _prepare_query_node)
   212	    g.add_node("rag", run_rag_node)
   213	    g.add_node("recommend", recommend_node)
   214	    g.add_node("validate_answer", validate_answer)
   215	    g.add_node("explain", explain_node)
   216	    g.add_node("respond", _respond_node)
   217	
   218	    g.set_entry_point("intake")
   219	    g.add_edge("intake", "extract")
   220	    g.add_edge("extract", "domain_router")
   221	    g.add_edge("domain_router", "compute")
   222	    g.add_edge("compute", "calc_agent")                # ⬅ compute → calc_agent
   223	    g.add_edge("calc_agent", "ask_missing")            # ⬅ calc_agent → ask_missing
   224	
   225	    g.add_conditional_edges("ask_missing", _ask_or_ok, {
   226	        "ask":  "respond",
   227	        "ok":   "validate",
   228	        "info": "prepare_query",
   229	    })
   230	
   231	    g.add_edge("validate", "prepare_query")
   232	    g.add_edge("prepare_query", "rag")
   233	    g.add_conditional_edges("rag", _after_rag, {
   234	        "recommend": "recommend",
   235	        "explain":   "explain",
   236	    })
   237	    g.add_edge("recommend", "validate_answer")
   238	    g.add_edge("validate_answer", "explain")
   239	    g.add_edge("explain", "respond")
   240	    g.add_edge("respond", END)
   241	
   242	    log.info("[ConsultGraph] erfolgreich erstellt.")
   243	    return g
   244	
   245	
   246	def build_consult_graph() -> StateGraph:
   247	    return build_graph()


===== FILE: backend/app/services/langgraph/graph/consult/config.py =====
     1	from __future__ import annotations
     2	import os
     3	from langchain_openai import ChatOpenAI
     4	from ...llm_factory import get_llm  # zentrale Factory nutzen
     5	
     6	OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
     7	OPENAI_TEMPERATURE = float(os.getenv("OPENAI_TEMPERATURE", "0"))
     8	OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
     9	OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL") or None
    10	
    11	REDIS_URL = os.getenv("REDIS_URL", "redis://redis:6379/0")
    12	CHECKPOINT_NS = os.getenv("CHECKPOINT_NS", "sealai.consult.v2")
    13	
    14	ENABLED_DOMAINS = [
    15	    d.strip()
    16	    for d in os.getenv("DOMAINS_ENABLED", "rwdr,hydraulics_rod").split(",")
    17	    if d.strip()
    18	]
    19	
    20	LTM_TOP_K = int(os.getenv("LTM_TOP_K", "4"))
    21	LTM_TIMEOUT_MS = int(os.getenv("LTM_SEARCH_TIMEOUT_MS", "300"))
    22	
    23	def create_llm(*, streaming: bool = True) -> ChatOpenAI:
    24	    # Vereinheitlicht auf zentrale Factory
    25	    return get_llm(streaming=streaming)


===== FILE: backend/app/services/langgraph/graph/consult/domain_router.py =====
     1	# backend/app/services/langgraph/graph/consult/domain_router.py
     2	from __future__ import annotations
     3	import json
     4	from typing import List
     5	from langchain_openai import ChatOpenAI
     6	from app.services.langgraph.llm_router import get_router_llm, get_router_fallback_llm
     7	from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage
     8	from app.services.langgraph.prompting import render_template, messages_for_template, strip_json_fence
     9	from .config import ENABLED_DOMAINS
    10	
    11	def detect_domain(llm: ChatOpenAI, msgs: List[AnyMessage], params: dict) -> str:
    12	    router = llm or get_router_llm()
    13	    prompt = render_template(
    14	        "domain_router.jinja2",
    15	        messages=messages_for_template(msgs),
    16	        params_json=json.dumps(params, ensure_ascii=False),
    17	        enabled_domains=ENABLED_DOMAINS,
    18	    )
    19	    # 1st pass
    20	    resp = router.invoke([HumanMessage(content=prompt)])
    21	    domain, conf = None, 0.0
    22	    try:
    23	        data = json.loads(strip_json_fence(resp.content or ""))
    24	        domain = str((data.get("domain") or "")).strip().lower()
    25	        conf = float(data.get("confidence") or 0.0)
    26	    except Exception:
    27	        domain, conf = None, 0.0
    28	
    29	    # Fallback, wenn unsicher
    30	    if (domain not in ENABLED_DOMAINS) or (conf < 0.70):
    31	        fb = get_router_fallback_llm()
    32	        try:
    33	            resp2 = fb.invoke([HumanMessage(content=prompt)])
    34	            data2 = json.loads(strip_json_fence(resp2.content or ""))
    35	            d2 = str((data2.get("domain") or "")).strip().lower()
    36	            c2 = float(data2.get("confidence") or 0.0)
    37	            if (d2 in ENABLED_DOMAINS) and (c2 >= conf):
    38	                domain, conf = d2, c2
    39	        except Exception:
    40	            pass
    41	
    42	    # Heuristische Fallbacks – nur Nutzertext
    43	    if (domain not in ENABLED_DOMAINS) or (conf < 0.40):
    44	        utter = ""
    45	        for m in reversed(msgs or []):
    46	            if hasattr(m, "content") and getattr(m, "content"):
    47	                if isinstance(m, HumanMessage):
    48	                    utter = (m.content or "").lower().strip()
    49	                    break
    50	        if "wellendichtring" in utter or "rwdr" in utter:
    51	            domain = "rwdr"
    52	        elif "stangendichtung" in utter or "kolbenstange" in utter or "hydraulik" in utter:
    53	            domain = "hydraulics_rod"
    54	        elif (params.get("bauform") or "").upper().startswith("BA"):
    55	            domain = "rwdr"
    56	        elif ENABLED_DOMAINS:
    57	            domain = ENABLED_DOMAINS[0]
    58	        else:
    59	            domain = "rwdr"
    60	    return domain


===== FILE: backend/app/services/langgraph/graph/consult/domain_runtime.py =====
     1	# backend/app/services/langgraph/graph/consult/domain_runtime.py
     2	from __future__ import annotations
     3	import importlib
     4	import logging
     5	from typing import Any, Dict, List
     6	from .state import Parameters, Derived
     7	
     8	log = logging.getLogger(__name__)
     9	
    10	def compute_domain(domain: str, params: Parameters) -> Derived:
    11	    try:
    12	        mod = importlib.import_module(f"app.services.langgraph.domains.{domain}.calculator")
    13	        compute = getattr(mod, "compute")
    14	        out = compute(params)  # type: ignore
    15	        return {
    16	            "calculated": dict(out.get("calculated", {})),
    17	            "flags": dict(out.get("flags", {})),
    18	            "warnings": list(out.get("warnings", [])),
    19	            "requirements": list(out.get("requirements", [])),
    20	        }
    21	    except Exception as e:
    22	        log.warning("Domain compute failed (%s): %s", domain, e)
    23	        return {"calculated": {}, "flags": {}, "warnings": [], "requirements": []}
    24	
    25	def missing_by_domain(domain: str, p: Parameters) -> List[str]:
    26	    # ✅ Hydraulik-Stange nutzt stange_mm / nut_d_mm / nut_b_mm
    27	    if domain == "hydraulics_rod":
    28	        req = [
    29	            "falltyp",
    30	            "stange_mm",
    31	            "nut_d_mm",
    32	            "nut_b_mm",
    33	            "medium",
    34	            "temp_max_c",
    35	            "druck_bar",
    36	            "geschwindigkeit_m_s",
    37	        ]
    38	    else:
    39	        req = [
    40	            "falltyp",
    41	            "wellen_mm",
    42	            "gehause_mm",
    43	            "breite_mm",
    44	            "medium",
    45	            "temp_max_c",
    46	            "druck_bar",
    47	            "drehzahl_u_min",
    48	        ]
    49	
    50	    def _is_missing(key: str, val: Any) -> bool:
    51	        if val is None or val == "" or val == "unknown":
    52	            return True
    53	        if key == "druck_bar":
    54	            try: float(val); return False
    55	            except Exception: return True
    56	        if key in ("wellen_mm", "gehause_mm", "breite_mm", "drehzahl_u_min", "geschwindigkeit_m_s",
    57	                   "stange_mm", "nut_d_mm", "nut_b_mm"):
    58	            try: return float(val) <= 0
    59	            except Exception: return True
    60	        if key == "temp_max_c":
    61	            try: float(val); return False
    62	            except Exception: return True
    63	        return False
    64	
    65	    return [k for k in req if _is_missing(k, p.get(k))]
    66	
    67	def anomaly_messages(domain: str, params: Parameters, derived: Derived) -> List[str]:
    68	    msgs: List[str] = []
    69	    flags = (derived.get("flags") or {})
    70	    if flags.get("requires_pressure_stage") and not flags.get("pressure_stage_ack"):
    71	        msgs.append("Ein Überdruck >2 bar ist für Standard-Radialdichtringe kritisch. Dürfen Druckstufenlösungen geprüft werden?")
    72	    if flags.get("speed_high"):
    73	        msgs.append("Die Drehzahl/Umfangsgeschwindigkeit ist hoch – ist sie dauerhaft oder nur kurzzeitig (Spitzen)?")
    74	    if flags.get("temp_very_high"):
    75	        msgs.append("Die Temperatur ist sehr hoch. Handelt es sich um Dauer- oder Spitzentemperaturen?")
    76	    if domain == "hydraulics_rod" and flags.get("extrusion_risk") and not flags.get("extrusion_risk_ack"):
    77	        msgs.append("Bei dem Druck besteht Extrusionsrisiko. Darf eine Stütz-/Back-up-Ring-Lösung geprüft werden?")
    78	    return msgs


===== FILE: backend/app/services/langgraph/graph/consult/extract.py =====
     1	# backend/app/services/langgraph/graph/consult/extract.py
     2	from __future__ import annotations
     3	import json
     4	import logging
     5	import re
     6	from typing import Dict, Any, List
     7	
     8	from langchain_openai import ChatOpenAI
     9	from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage
    10	
    11	from app.services.langgraph.prompting import (
    12	    render_template,
    13	    messages_for_template,
    14	    strip_json_fence,
    15	)
    16	from app.services.langgraph.prompt_registry import get_agent_prompt
    17	
    18	from .state import Parameters
    19	from .utils import msgs_text, apply_heuristics_from_text
    20	
    21	log = logging.getLogger(__name__)
    22	
    23	# Welche Keys aus dem LLM wirklich übernommen werden dürfen
    24	ALLOWED_KEYS = {
    25	    "falltyp",
    26	    "wellen_mm",
    27	    "gehause_mm",
    28	    "breite_mm",
    29	    "bauform",
    30	    "medium",
    31	    "temp_min_c",
    32	    "temp_max_c",
    33	    "druck_bar",
    34	    "drehzahl_u_min",
    35	    "geschwindigkeit_m_s",
    36	    "umgebung",
    37	    "prioritaet",
    38	    "besondere_anforderungen",
    39	    "bekannte_probleme",
    40	    # Ergänzungen für Konsistenz zum Prompt
    41	    "lang",
    42	    "domain",
    43	    "abmessung",
    44	}
    45	
    46	
    47	def _extract_json_any(s: str) -> str:
    48	    """
    49	    Zieht – falls nötig – ein JSON-Snippet aus einer evtl. vermischten Antwort.
    50	    """
    51	    s = (s or "").strip()
    52	    if not s:
    53	        return ""
    54	    if (s[:1] in "{[") and (s[-1:] in "}]"):
    55	        return s
    56	    s2 = strip_json_fence(s)
    57	    if (s2[:1] in "{[") and (s2[-1:] in "}]"):
    58	        return s2
    59	    # balanced JSON heuristics
    60	    m = re.search(r"\{(?:[^{}]|(?R))*\}", s, re.S)
    61	    if m:
    62	        return m.group(0)
    63	    m = re.search(r"\[(?:[^\[\]]|(?R))*\]", s, re.S)
    64	    return m.group(0) if m else ""
    65	
    66	
    67	def extract_params_with_llm(llm: ChatOpenAI, msgs: List[AnyMessage], existing: Parameters) -> Parameters:
    68	    """
    69	    Extrahiert Parameter streng als JSON.
    70	    - Stellt Supervisor-Systemprompt voran (einheitlicher Ton/Regeln).
    71	    - Nutzt JSON-Mode, wenn verfügbar; sonst Fallback + robuste JSON-Extraktion.
    72	    """
    73	    prompt = render_template(
    74	        "consult_extract_params.jinja2",
    75	        # Template erwartet "params_json"
    76	        params_json=json.dumps(existing or {}, ensure_ascii=False),
    77	        messages=messages_for_template(msgs),
    78	    )
    79	
    80	    # Wenn möglich: JSON-Response-Modus aktivieren (reduziert Parsing-Fehler)
    81	    json_llm = llm
    82	    try:
    83	        json_llm = llm.bind(response_format={"type": "json_object"})
    84	    except Exception:
    85	        pass
    86	
    87	    try:
    88	        resp = json_llm.invoke([
    89	            SystemMessage(content=get_agent_prompt("supervisor")),
    90	            HumanMessage(content=prompt),
    91	        ])
    92	        raw_content = getattr(resp, "content", "") or ""
    93	    except Exception as e:
    94	        log.warning("[extract_params_with_llm] llm_invoke_failed", exc_info=e)
    95	        raw_content = ""
    96	
    97	    # Robust gegen Fences / Prosa
    98	    content = strip_json_fence(raw_content)
    99	    if not content or not content.strip().startswith("{"):
   100	        content = _extract_json_any(raw_content) or _extract_json_any(content)
   101	
   102	    parsed: Dict[str, Any] = {}
   103	    try:
   104	        obj = json.loads(content) if content else {}
   105	        if isinstance(obj, dict):
   106	            parsed = {k: v for k, v in obj.items() if k in ALLOWED_KEYS}
   107	    except Exception:
   108	        log.debug("Param-JSON parse failed, raw: %r", (content or raw_content)[:2000])
   109	        parsed = {}
   110	
   111	    merged: Parameters = dict(existing or {})
   112	    for k, v in parsed.items():
   113	        if v not in (None, "", "unknown"):
   114	            merged[k] = v
   115	
   116	    # Heuristiken (z. B. Abmessungen "40/50x10" → mm-Felder)
   117	    merged = apply_heuristics_from_text(merged, msgs_text(msgs))
   118	    return merged


===== FILE: backend/app/services/langgraph/graph/consult/heuristic_extract.py =====
     1	from __future__ import annotations
     2	import re
     3	from typing import Dict, Optional
     4	
     5	_NUM = r"-?\d+(?:[.,]\d+)?"
     6	
     7	def _to_float(s: Optional[str]) -> Optional[float]:
     8	    if not s:
     9	        return None
    10	    try:
    11	        return float(s.replace(",", "."))
    12	    except Exception:
    13	        return None
    14	
    15	def pre_extract_params(text: str) -> Dict[str, object]:
    16	    t = text or ""
    17	    out: Dict[str, object] = {}
    18	
    19	    m = re.search(r"(?:\b(?:rwdr|ba|bauform)\b\s*)?(\d{1,3})\s*[x×]\s*(\d{1,3})\s*[x×]\s*(\d{1,3})", t, re.I)
    20	    if m:
    21	        out["wellen_mm"]  = int(m.group(1))
    22	        out["gehause_mm"] = int(m.group(2))
    23	        out["breite_mm"]  = int(m.group(3))
    24	
    25	    if re.search(r"\bhydraulik ?öl\b", t, re.I):
    26	        out["medium"] = "Hydrauliköl"
    27	    elif re.search(r"\böl\b", t, re.I):
    28	        out["medium"] = "Öl"
    29	    elif re.search(r"\bwasser\b", t, re.I):
    30	        out["medium"] = "Wasser"
    31	
    32	    m = re.search(r"(?:t\s*max|temp(?:eratur)?(?:\s*max)?|t)\s*[:=]?\s*(" + _NUM + r")\s*°?\s*c\b", t, re.I)
    33	    if not m:
    34	        m = re.search(r"\b(" + _NUM + r")\s*°?\s*c\b", t, re.I)
    35	    if m:
    36	        out["temp_max_c"] = _to_float(m.group(1))
    37	
    38	    m = re.search(r"(?:\bdruck\b|[^a-z]p)\s*[:=]?\s*(" + _NUM + r")\s*bar\b", t, re.I)
    39	    if not m:
    40	        m = re.search(r"\b(" + _NUM + r")\s*bar\b", t, re.I)
    41	    if m:
    42	        out["druck_bar"] = _to_float(m.group(1))
    43	
    44	    m = re.search(r"(?:\bn\b|drehzahl)\s*[:=]?\s*(\d{1,7})\s*(?:u/?min|rpm)\b", t, re.I)
    45	    if not m:
    46	        m = re.search(r"\b(\d{1,7})\s*(?:u/?min|rpm)\b", t, re.I)
    47	    if m:
    48	        out["drehzahl_u_min"] = int(m.group(1))
    49	
    50	    m = re.search(r"\bbauform\s*[:=]?\s*([A-Z0-9]{1,4})\b|\b(BA|B1|B2|TC|SC)\b", t, re.I)
    51	    if m:
    52	        out["bauform"] = (m.group(1) or m.group(2) or "").upper()
    53	
    54	    return out


===== FILE: backend/app/services/langgraph/graph/consult/__init__.py =====


===== FILE: backend/app/services/langgraph/graph/consult/intent_llm.py =====
     1	from __future__ import annotations
     2	
     3	"""
     4	LLM-basierter Intent-Router (verpflichtend).
     5	- Nutzt ChatOpenAI (Model per ENV, Default: gpt-5-nano).
     6	- Gibt eines der erlaubten Labels zurück, sonst 'chitchat'.
     7	- Fallback: robuste Heuristik, falls LLM fehlschlägt.
     8	"""
     9	
    10	import os
    11	import re
    12	import logging
    13	from typing import Any, Dict, List, Optional, TypedDict
    14	
    15	log = logging.getLogger("uvicorn.error")
    16	
    17	# Erlaubte Ziele (müssen mit build.py übereinstimmen)
    18	ALLOWED_ROUTES: List[str] = [
    19	    "rag_qa",
    20	    "material_agent",
    21	    "profile_agent",
    22	    "calc_agent",
    23	    "report_agent",
    24	    "memory_export",
    25	    "memory_delete",
    26	    "chitchat",
    27	]
    28	
    29	# Prompt für den reinen Label-Output
    30	_INTENT_PROMPT = """Du bist ein Intent-Router. Antworte NUR mit einem Label (genau wie angegeben):
    31	{allowed}
    32	
    33	Eingabe: {query}
    34	Label:"""
    35	
    36	# Heuristiken als Fallback
    37	_HEURISTICS: List[tuple[str, re.Pattern]] = [
    38	    ("memory_export", re.compile(r"\b(export|download|herunterladen|daten\s*export)\b", re.I)),
    39	    ("memory_delete", re.compile(r"\b(löschen|delete|entfernen)\b", re.I)),
    40	    ("calc_agent",    re.compile(r"\b(rechnen|berechne|calculate|calc|formel|formulas?)\b", re.I)),
    41	    ("report_agent",  re.compile(r"\b(report|bericht|pdf|zusammenfassung|protokoll)\b", re.I)),
    42	    ("material_agent",re.compile(r"\b(material|werkstoff|elastomer|ptfe|fkm|nbr|epdm)\b", re.I)),
    43	    ("profile_agent", re.compile(r"\b(profil|o-ring|x-ring|u-profil|lippe|dichtung\s*profil)\b", re.I)),
    44	    ("rag_qa",        re.compile(r"\b(warum|wie|quelle|dokument|datenblatt|docs?)\b", re.I)),
    45	]
    46	
    47	# State-Shape (nur für Typing; zur Laufzeit wird ein dict genutzt)
    48	class ConsultState(TypedDict, total=False):
    49	    user: str
    50	    chat_id: Optional[str]
    51	    input: str
    52	    route: str
    53	    response: str
    54	    citations: List[Dict[str, Any]]
    55	
    56	# LLM-Konfiguration
    57	try:
    58	    from langchain_openai import ChatOpenAI
    59	    _LLM_OK = bool(os.getenv("OPENAI_API_KEY"))
    60	except Exception:
    61	    ChatOpenAI = None  # type: ignore
    62	    _LLM_OK = False
    63	
    64	def _classify_heuristic(query: str) -> str:
    65	    q = (query or "").lower()
    66	    for label, pattern in _HEURISTICS:
    67	        if pattern.search(q):
    68	            return label
    69	    if re.search(r"[?]|(wie|warum|wieso|quelle|beleg)", q):
    70	        return "rag_qa"
    71	    return "chitchat"
    72	
    73	def _classify_llm(query: str) -> str:
    74	    if not (_LLM_OK and ChatOpenAI):
    75	        raise RuntimeError("LLM not available")
    76	    model_name = os.getenv("OPENAI_ROUTER_MODEL", "gpt-5-nano")
    77	    llm = ChatOpenAI(model=model_name, temperature=0, max_tokens=6)  # type: ignore
    78	    prompt = _INTENT_PROMPT.format(allowed=", ".join(ALLOWED_ROUTES), query=query.strip())
    79	    try:
    80	        resp = llm.invoke(prompt)  # type: ignore
    81	        label = str(getattr(resp, "content", "")).strip().lower()
    82	        if label in ALLOWED_ROUTES:
    83	            return label
    84	    except Exception as exc:
    85	        log.warning("LLM Intent error: %r", exc)
    86	    # Fallback falls Ausgabe nicht sauber ist
    87	    return _classify_heuristic(query)
    88	
    89	def intent_router_node(state: ConsultState) -> ConsultState:
    90	    """Graph-Node: setzt state['route'] über LLM (mit Heuristik-Fallback)."""
    91	    query = state.get("input", "") or ""
    92	    try:
    93	        route = _classify_llm(query)
    94	    except Exception:
    95	        route = _classify_heuristic(query)
    96	
    97	    if route not in ALLOWED_ROUTES:
    98	        route = "chitchat"
    99	
   100	    state["route"] = route
   101	    return state


===== FILE: backend/app/services/langgraph/graph/consult/io.py =====
     1	# backend/app/services/langgraph/graph/consult/io.py
     2	from __future__ import annotations
     3	
     4	import logging
     5	import os
     6	import re
     7	from typing import Any, Dict
     8	
     9	from langchain_core.messages import HumanMessage, AIMessage
    10	from langgraph.checkpoint.base import BaseCheckpointSaver  # Typkompatibel
    11	
    12	from .build import build_consult_graph
    13	
    14	log = logging.getLogger(__name__)
    15	
    16	OFFLINE_MODE = os.getenv("OFFLINE_MODE", "0") == "1"
    17	
    18	# Caches (prozessweit)
    19	_graph_no_cp = None
    20	_graph_by_cp_id: dict[int, Any] = {}  # id(checkpointer) -> compiled graph
    21	
    22	
    23	def _last_ai_text(msgs) -> str:
    24	    for m in reversed(msgs or []):
    25	        if isinstance(m, AIMessage) and isinstance(getattr(m, "content", None), str):
    26	            return (m.content or "").strip()
    27	    return ""
    28	
    29	
    30	# ––– Fallback ohne LLM –––
    31	_DIM_RX = re.compile(r"(\d{1,3})\s*[xX]\s*(\d{1,3})\s*[xX]\s*(\d{1,3})")
    32	_BAR_RX = re.compile(r"(-?\d+(?:[.,]\d+)?)\s*bar", re.I)
    33	
    34	def _local_fallback_reply(user_text: str) -> str:
    35	    t = (user_text or "").strip()
    36	    tl = t.lower()
    37	
    38	    dims = None
    39	    m = _DIM_RX.search(t)
    40	    if m:
    41	        dims = f"{int(m.group(1))}x{int(m.group(2))}x{int(m.group(3))}"
    42	
    43	    if "öl" in tl or "oel" in tl or "oil" in tl:
    44	        medium = "Öl"
    45	        material_hint = "FKM"
    46	        vorteile = "hohe Temperatur- und Ölbeständigkeit, gute Alterungsbeständigkeit"
    47	        einschraenkungen = "nicht ideal für Wasser/Heißwasser"
    48	    elif "wasser" in tl or "water" in tl:
    49	        medium = "Wasser"
    50	        material_hint = "EPDM (alternativ HNBR, je nach Temperatur)"
    51	        vorteile = "gute Wasser-/Dampfbeständigkeit (EPDM)"
    52	        einschraenkungen = "nicht ölbeständig (EPDM)"
    53	    else:
    54	        medium = "nicht angegeben"
    55	        material_hint = "NBR (Preis/Leistung) oder FKM (Temperatur/Öl)"
    56	        vorteile = "solide Beständigkeit je nach Materialwahl"
    57	        einschraenkungen = "Materialwahl abhängig von Medium/Temperatur"
    58	
    59	    pbar = None
    60	    mp = _BAR_RX.search(tl)
    61	    if mp:
    62	        try:
    63	            pbar = float(mp.group(1).replace(",", "."))
    64	        except Exception:
    65	            pbar = None
    66	
    67	    druck_hinweis = ""
    68	    if pbar is not None and pbar > 2:
    69	        druck_hinweis = (
    70	            "\n- **Hinweis:** Überdruck >2 bar ist für Standard-Radialdichtringe kritisch. "
    71	            "Bitte Druckstufen-/Entlastungslösungen prüfen."
    72	        )
    73	
    74	    typ = f"BA {dims}" if dims else "BA (Standard-Profil)"
    75	
    76	    return (
    77	        "🔎 **Empfehlung (Fallback – LLM temporär nicht erreichbar)**\n\n"
    78	        f"**Typ:** {typ}\n"
    79	        f"**Werkstoff (Hint):** {material_hint}\n"
    80	        f"**Medium:** {medium}\n\n"
    81	        f"**Vorteile:** {vorteile}\n"
    82	        f"**Einschränkungen:** {einschraenkungen}\n"
    83	        f"{druck_hinweis}\n\n"
    84	        "**Nächste Schritte:**\n"
    85	        "- Wenn du **Maße (Welle/Gehäuse/Breite)**, **Medium**, **Tmax**, **Druck** und **Drehzahl/Relativgeschwindigkeit** angibst,\n"
    86	        "  erstelle ich eine präzisere Empfehlung inkl. Alternativen.\n"
    87	        "_(Dies ist eine lokale Antwort ohne LLM; die Detailberatung folgt automatisch, sobald der Dienst wieder verfügbar ist.)_"
    88	    )
    89	
    90	
    91	def _get_graph(checkpointer: BaseCheckpointSaver | None):
    92	    """
    93	    Liefert einen kompilierten Consult-Graphen.
    94	    - Ohne Checkpointer: Singleton.
    95	    - Mit Checkpointer: pro-Saver gecachter Graph (id(checkpointer) als Key).
    96	    """
    97	    global _graph_no_cp, _graph_by_cp_id
    98	
    99	    if checkpointer is None:
   100	        if _graph_no_cp is None:
   101	            g = build_consult_graph()
   102	            _graph_no_cp = g.compile()
   103	            log.info("[consult.io] Graph compiled WITHOUT checkpointer.")
   104	        return _graph_no_cp
   105	
   106	    key = id(checkpointer)
   107	    if key not in _graph_by_cp_id:
   108	        try:
   109	            g = build_consult_graph()
   110	            _graph_by_cp_id[key] = g.compile(checkpointer=checkpointer)
   111	            log.info("[consult.io] Graph compiled WITH provided checkpointer.")
   112	        except Exception as e:
   113	            log.warning("[consult.io] Failed to compile with provided checkpointer: %s. Falling back.", e)
   114	            if _graph_no_cp is None:
   115	                g = build_consult_graph()
   116	                _graph_no_cp = g.compile()
   117	            return _graph_no_cp
   118	
   119	    return _graph_by_cp_id[key]
   120	
   121	
   122	def invoke_consult(
   123	    text: str,
   124	    *,
   125	    thread_id: str,
   126	    checkpointer: BaseCheckpointSaver | None = None,
   127	) -> str:
   128	    """
   129	    Führt eine Consult-Anfrage aus.
   130	    - checkpointer: Optionaler externer Saver (z. B. aus app.state).
   131	    """
   132	    user_text = (text or "").strip()
   133	    if not user_text:
   134	        return ""
   135	
   136	    if OFFLINE_MODE:
   137	        return _local_fallback_reply(user_text)
   138	
   139	    try:
   140	        graph = _get_graph(checkpointer)
   141	        cfg: Dict[str, Any] = {"configurable": {"thread_id": thread_id}}
   142	        initial = {"messages": [HumanMessage(content=user_text)]}
   143	        result = graph.invoke(initial, config=cfg)
   144	
   145	        out = _last_ai_text((result or {}).get("messages", []))
   146	        if out:
   147	            return out
   148	
   149	        log.warning("[consult.io] Graph returned no AI text; using local fallback.")
   150	        return _local_fallback_reply(user_text)
   151	
   152	    except Exception as e:
   153	        log.error("[consult.io] Graph invocation failed, using fallback. Error: %s", e)
   154	        return _local_fallback_reply(user_text)


===== FILE: backend/app/services/langgraph/graph/consult/memory_utils.py =====
     1	from __future__ import annotations
     2	
     3	import os
     4	import json
     5	from typing import List, Dict, Literal, TypedDict
     6	
     7	from redis import Redis
     8	from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, AnyMessage
     9	
    10	
    11	def _redis() -> Redis:
    12	    url = os.getenv("REDIS_URL", "redis://redis:6379/0")
    13	    return Redis.from_url(url, decode_responses=True)
    14	
    15	def _conv_key(thread_id: str) -> str:
    16	    # Gleicher Key wie im SSE: chat:stm:{thread_id}:messages
    17	    return f"chat:stm:{thread_id}:messages"
    18	
    19	
    20	def write_message(*, thread_id: str, role: Literal["user", "assistant", "system"], content: str) -> None:
    21	    if not content:
    22	        return
    23	    r = _redis()
    24	    key = _conv_key(thread_id)
    25	    item = json.dumps({"role": role, "content": content}, ensure_ascii=False)
    26	    pipe = r.pipeline()
    27	    pipe.lpush(key, item)
    28	    pipe.ltrim(key, 0, int(os.getenv("STM_MAX_ITEMS", "200")) - 1)
    29	    pipe.expire(key, int(os.getenv("STM_TTL_SEC", "604800")))  # 7 Tage
    30	    pipe.execute()
    31	
    32	
    33	def read_history_raw(thread_id: str, limit: int = 80) -> List[Dict[str, str]]:
    34	    """Rohdaten (älteste -> neueste)."""
    35	    r = _redis()
    36	    key = _conv_key(thread_id)
    37	    items = r.lrange(key, 0, limit - 1) or []
    38	    out: List[Dict[str, str]] = []
    39	    for s in reversed(items):  # Redis speichert neueste zuerst
    40	        try:
    41	            obj = json.loads(s)
    42	            role = (obj.get("role") or "").lower()
    43	            content = obj.get("content") or ""
    44	            if role and content:
    45	                out.append({"role": role, "content": content})
    46	        except Exception:
    47	            continue
    48	    return out
    49	
    50	
    51	def read_history(thread_id: str, limit: int = 80) -> List[AnyMessage]:
    52	    """LangChain-Messages (älteste -> neueste)."""
    53	    msgs: List[AnyMessage] = []
    54	    for item in read_history_raw(thread_id, limit=limit):
    55	        role = item["role"]
    56	        content = item["content"]
    57	        if role in ("user", "human"):
    58	            msgs.append(HumanMessage(content=content))
    59	        elif role in ("assistant", "ai"):
    60	            msgs.append(AIMessage(content=content))
    61	        elif role == "system":
    62	            msgs.append(SystemMessage(content=content))
    63	    return msgs


===== FILE: backend/app/services/langgraph/graph/consult/nodes/ask_missing.py =====
     1	# backend/app/services/langgraph/graph/consult/nodes/ask_missing.py
     2	from __future__ import annotations
     3	
     4	import logging
     5	from typing import Any, Dict, List
     6	
     7	from langchain_core.messages import AIMessage
     8	from app.services.langgraph.prompting import render_template
     9	from ..utils import missing_by_domain, anomaly_messages, normalize_messages
    10	
    11	log = logging.getLogger(__name__)
    12	
    13	# ---------- Feldlabels ----------
    14	FIELD_LABELS_RWDR = {
    15	    "falltyp": "Anwendungsfall (Ersatz/Neu/Optimierung)",
    16	    "wellen_mm": "Welle (mm)",
    17	    "gehause_mm": "Gehäuse (mm)",
    18	    "breite_mm": "Breite (mm)",
    19	    "bauform": "Bauform/Profil",
    20	    "medium": "Medium",
    21	    "temp_min_c": "Temperatur min (°C)",
    22	    "temp_max_c": "Temperatur max (°C)",
    23	    "druck_bar": "Druck (bar)",
    24	    "drehzahl_u_min": "Drehzahl (U/min)",
    25	    "geschwindigkeit_m_s": "Relativgeschwindigkeit (m/s)",
    26	    "umgebung": "Umgebung",
    27	    "prioritaet": "Priorität (z. B. Preis, Lebensdauer)",
    28	    "besondere_anforderungen": "Besondere Anforderungen",
    29	    "bekannte_probleme": "Bekannte Probleme",
    30	}
    31	DISPLAY_ORDER_RWDR = [
    32	    "falltyp","wellen_mm","gehause_mm","breite_mm","bauform","medium",
    33	    "temp_min_c","temp_max_c","druck_bar","drehzahl_u_min","geschwindigkeit_m_s",
    34	    "umgebung","prioritaet","besondere_anforderungen","bekannte_probleme",
    35	]
    36	
    37	FIELD_LABELS_HYD = {
    38	    "falltyp": "Anwendungsfall (Ersatz/Neu/Optimierung)",
    39	    "stange_mm": "Stange (mm)",
    40	    "nut_d_mm": "Nut-Ø D (mm)",
    41	    "nut_b_mm": "Nutbreite B (mm)",
    42	    "medium": "Medium",
    43	    "temp_max_c": "Temperatur max (°C)",
    44	    "druck_bar": "Druck (bar)",
    45	    "geschwindigkeit_m_s": "Relativgeschwindigkeit (m/s)",
    46	}
    47	DISPLAY_ORDER_HYD = [
    48	    "falltyp","stange_mm","nut_d_mm","nut_b_mm","medium","temp_max_c","druck_bar","geschwindigkeit_m_s",
    49	]
    50	
    51	def _friendly_list(keys: List[str], domain: str) -> str:
    52	    if domain == "hydraulics_rod":
    53	        labels, order = FIELD_LABELS_HYD, DISPLAY_ORDER_HYD
    54	    else:
    55	        labels, order = FIELD_LABELS_RWDR, DISPLAY_ORDER_RWDR
    56	    ordered = [k for k in order if k in keys]
    57	    return ", ".join(f"**{labels.get(k, k)}**" for k in ordered)
    58	
    59	# ---------- Node ----------
    60	def ask_missing_node(state: Dict[str, Any]) -> Dict[str, Any]:
    61	    """
    62	    Stellt Rückfragen nur bei Beratungsbedarf.
    63	    Liefert zusätzlich ein UI-Event zum Öffnen des Formular-Drawers.
    64	    """
    65	    consult_required = bool(state.get("consult_required", True))
    66	    if not consult_required:
    67	        return {**state, "messages": [], "phase": "ask_missing"}
    68	
    69	    _ = normalize_messages(state.get("messages", []))
    70	    params: Dict[str, Any] = state.get("params") or {}
    71	    domain: str = (state.get("domain") or "rwdr").strip().lower()
    72	    derived: Dict[str, Any] = state.get("derived") or {}
    73	
    74	    # Sprache (Fallback de)
    75	    lang = (params.get("lang") or state.get("lang") or "de").lower()
    76	
    77	    missing = missing_by_domain(domain, params)
    78	    log.info("[ask_missing_node] fehlend=%s domain=%s consult_required=%s", missing, domain, consult_required)
    79	
    80	    if missing:
    81	        friendly = _friendly_list(missing, domain)
    82	        # Einzeilenbeispiel je Domäne
    83	        example = (
    84	            "Welle 25, Gehäuse 47, Breite 7, Medium Öl, Tmax 80, Druck 2 bar, n 1500"
    85	            if domain != "hydraulics_rod"
    86	            else "Stange 25, Nut D 32, Nut B 6, Medium Öl, Tmax 80, Druck 160 bar, v 0,3 m/s"
    87	        )
    88	
    89	        content = render_template(
    90	            "ask_missing.jinja2",
    91	            domain=domain,
    92	            friendly=friendly,
    93	            example=example,
    94	            lang=lang,
    95	        )
    96	
    97	        ui_event = {
    98	            "ui_action": "open_form",
    99	            "form_id": f"{domain}_params_v1",
   100	            "schema_ref": f"domains/{domain}/params@1.0.0",
   101	            "missing": missing,
   102	            "prefill": {k: v for k, v in params.items() if v not in (None, "", [])},
   103	        }
   104	        return {
   105	            **state,
   106	            "messages": [AIMessage(content=content)],
   107	            "phase": "ask_missing",
   108	            "ui_event": ui_event,
   109	            "missing_fields": missing,
   110	        }
   111	
   112	    followups = anomaly_messages(domain, params, derived)
   113	    if followups:
   114	        content = render_template("ask_missing_followups.jinja2", followups=followups[:2], lang=lang)
   115	        ui_event = {
   116	            "ui_action": "open_form",
   117	            "form_id": f"{domain}_params_v1",
   118	            "schema_ref": f"domains/{domain}/params@1.0.0",
   119	            "missing": [],
   120	            "prefill": {k: v for k, v in params.items() if v not in (None, "", [])},
   121	        }
   122	        return {
   123	            **state,
   124	            "messages": [AIMessage(content=content)],
   125	            "phase": "ask_missing",
   126	            "ui_event": ui_event,
   127	            "missing_fields": [],
   128	        }
   129	
   130	    return {**state, "messages": [], "phase": "ask_missing"}


===== FILE: backend/app/services/langgraph/graph/consult/nodes/calc_agent.py =====
     1	# backend/app/services/langgraph/graph/consult/nodes/calc_agent.py
     2	from __future__ import annotations
     3	
     4	import logging
     5	from typing import Any, Dict
     6	
     7	log = logging.getLogger(__name__)
     8	
     9	
    10	def _num(x: Any) -> float | None:
    11	    try:
    12	        if x in (None, "", []):
    13	            return None
    14	        if isinstance(x, bool):
    15	            return None
    16	        return float(x)
    17	    except Exception:
    18	        return None
    19	
    20	
    21	def _deep_merge(a: Dict[str, Any], b: Dict[str, Any]) -> Dict[str, Any]:
    22	    """
    23	    flache & verschachtelte Dicts zusammenführen (b gewinnt),
    24	    nützlich um 'derived.calculated' nicht zu überschreiben.
    25	    """
    26	    out = dict(a or {})
    27	    for k, v in (b or {}).items():
    28	        if isinstance(v, dict) and isinstance(out.get(k), dict):
    29	            out[k] = _deep_merge(out[k], v)
    30	        else:
    31	            out[k] = v
    32	    return out
    33	
    34	
    35	def _calc_rwdr(params: Dict[str, Any]) -> Dict[str, Any]:
    36	    """Berechnungen für Radial-Wellendichtringe (RWDR)."""
    37	    d_mm = _num(params.get("wellen_mm"))
    38	    n_rpm = _num(params.get("drehzahl_u_min"))
    39	    p_bar = _num(params.get("druck_bar"))
    40	    tmax = _num(params.get("temp_max_c"))
    41	
    42	    calc: Dict[str, Any] = {}
    43	
    44	    # Umfangsgeschwindigkeit v = π * d[m] * n[1/s]
    45	    if d_mm is not None and n_rpm is not None and d_mm > 0 and n_rpm >= 0:
    46	        d_m = d_mm / 1000.0
    47	        v_ms = 3.141592653589793 * d_m * (n_rpm / 60.0)
    48	        calc["umfangsgeschwindigkeit_m_s"] = v_ms
    49	        # kompatibel zu älteren Keys
    50	        calc["surface_speed_m_s"] = round(v_ms, 3)
    51	
    52	    # PV-Indikator (einfaches Produkt) → Orientierung für thermische Last
    53	    if p_bar is not None and calc.get("umfangsgeschwindigkeit_m_s") is not None:
    54	        calc["pv_indicator_bar_ms"] = p_bar * calc["umfangsgeschwindigkeit_m_s"]
    55	
    56	    # Material-Hinweise (leichtgewichtig / erweiterbar)
    57	    mat_whitelist: list[str] = []
    58	    mat_blacklist: list[str] = []
    59	
    60	    medium = (params.get("medium") or "").strip().lower()
    61	    if "wasser" in medium:
    62	        # Wasser → NBR okay, FKM oft okay, PTFE nur bei spezieller Ausführung
    63	        mat_blacklist.append("PTFE")
    64	    if tmax is not None:
    65	        if tmax > 120:
    66	            mat_whitelist.append("FKM")
    67	        if tmax > 200:
    68	            # sehr hohe T → PTFE grundsätzlich denkbar (aber oben ggf. ausgeschlossen)
    69	            mat_whitelist.append("PTFE")
    70	
    71	    reqs: list[str] = []
    72	    if "PTFE" in mat_blacklist:
    73	        reqs.append("Vermeide Materialien: PTFE")
    74	
    75	    flags: Dict[str, Any] = {}
    76	    if p_bar is not None and p_bar > 1.0:
    77	        flags["druckbelastet"] = True
    78	
    79	    out = {
    80	        "calculated": calc,
    81	        "material_whitelist": mat_whitelist,
    82	        "material_blacklist": mat_blacklist,
    83	        "requirements": reqs,
    84	        "flags": flags,
    85	    }
    86	    return out
    87	
    88	
    89	def _calc_hydraulics_rod(params: Dict[str, Any]) -> Dict[str, Any]:
    90	    """Berechnungen für Hydraulik-Stangendichtungen."""
    91	    p_bar = _num(params.get("druck_bar"))
    92	    v_lin = _num(params.get("geschwindigkeit_m_s"))  # lineare Stangengeschwindigkeit
    93	    tmax = _num(params.get("temp_max_c"))
    94	
    95	    calc: Dict[str, Any] = {}
    96	    if p_bar is not None and v_lin is not None:
    97	        # einfacher PV-Indikator (lineare Geschwindigkeit)
    98	        calc["pv_indicator_bar_ms"] = p_bar * v_lin
    99	
   100	    # kleine Heuristik zur Extrusionsgefahr bei hohen Drücken
   101	    flags: Dict[str, Any] = {}
   102	    reqs: list[str] = []
   103	    if p_bar is not None and p_bar >= 160:
   104	        flags["extrusion_risk"] = True
   105	        reqs.append("Stütz-/Back-up-Ring prüfen (≥160 bar).")
   106	
   107	    # Materialpräferenz bei hohen Temperaturen
   108	    mat_whitelist: list[str] = []
   109	    if tmax is not None and tmax > 100:
   110	        mat_whitelist.append("FKM")
   111	
   112	    out = {
   113	        "calculated": calc,
   114	        "flags": flags,
   115	        "requirements": reqs,
   116	        "material_whitelist": mat_whitelist,
   117	        "material_blacklist": [],
   118	    }
   119	    return out
   120	
   121	
   122	def calc_agent_node(state: Dict[str, Any]) -> Dict[str, Any]:
   123	    """
   124	    Dedizierter Kalkulations-Node:
   125	    - führt domänenspezifische Rechen- & Heuristikschritte aus,
   126	    - schreibt Ergebnisse nach state['derived'] (nicht destruktiv),
   127	    - hinterlässt 'phase': 'calc_agent'.
   128	    """
   129	    domain = (state.get("domain") or "rwdr").strip().lower()
   130	    params = dict(state.get("params") or {})
   131	    derived_existing = dict(state.get("derived") or {})
   132	
   133	    try:
   134	        if domain == "hydraulics_rod":
   135	            derived_new = _calc_hydraulics_rod(params)
   136	        else:
   137	            # Default: RWDR
   138	            derived_new = _calc_rwdr(params)
   139	    except Exception as e:
   140	        log.warning("[calc_agent] calc_failed", exc=str(e))
   141	        # Fehler nicht eskalieren – einfach Phase setzen
   142	        return {**state, "phase": "calc_agent"}
   143	
   144	    # nicht-destruktiv zusammenführen
   145	    derived_merged = _deep_merge(derived_existing, derived_new)
   146	
   147	    # Kompatibilität: einzelner Key für v [m/s], falls benötigt
   148	    v = (
   149	        derived_merged.get("calculated", {}).get("umfangsgeschwindigkeit_m_s")
   150	        or params.get("relativgeschwindigkeit_ms")
   151	    )
   152	    if v is not None:
   153	        derived_merged["relativgeschwindigkeit_ms"] = v
   154	
   155	    new_state = {**state, "derived": derived_merged, "phase": "calc_agent"}
   156	    return new_state


===== FILE: backend/app/services/langgraph/graph/consult/nodes/calc_node.py =====
     1	# backend/app/services/langgraph/graph/consult/nodes/calc_node.py
     2	from __future__ import annotations
     3	
     4	import math
     5	from typing import Any, Dict, Tuple
     6	
     7	def _norm_float(x: Any) -> float | None:
     8	    try:
     9	        if x is None or x == "" or x == []:
    10	            return None
    11	        return float(x)
    12	    except Exception:
    13	        return None
    14	
    15	def _merge_derived(base: Dict[str, Any], add: Dict[str, Any]) -> Dict[str, Any]:
    16	    """Non-destruktiv in state['derived'] zusammenführen."""
    17	    out = dict(base or {})
    18	    for k, v in (add or {}).items():
    19	        if isinstance(v, dict) and isinstance(out.get(k), dict):
    20	            out[k] = {**out[k], **v}
    21	        else:
    22	            out[k] = v
    23	    return out
    24	
    25	# -------------------------
    26	#   RWDR – Berechnungen
    27	# -------------------------
    28	def _calc_rwdr(params: Dict[str, Any]) -> Dict[str, Any]:
    29	    d_mm = _norm_float(params.get("wellen_mm"))
    30	    n_rpm = _norm_float(params.get("drehzahl_u_min"))
    31	    druck_bar = _norm_float(params.get("druck_bar"))
    32	    tmax = _norm_float(params.get("temp_max_c"))
    33	
    34	    v_ms = None
    35	    if d_mm and n_rpm:
    36	        d_m = d_mm / 1000.0
    37	        n_rps = n_rpm / 60.0
    38	        v_ms = math.pi * d_m * n_rps  # Umfangsgeschwindigkeit
    39	
    40	    pv = None
    41	    if v_ms is not None and druck_bar is not None:
    42	        pv = druck_bar * v_ms  # einfacher PV-Indikator
    43	
    44	    # leichte Material-Whitelist/Blacklist-Heuristiken
    45	    medium_raw = (params.get("medium") or "").strip().lower()
    46	    mat_whitelist: list[str] = []
    47	    mat_blacklist: list[str] = []
    48	
    49	    if "wasser" in medium_raw:
    50	        mat_whitelist += ["EPDM"]
    51	        mat_blacklist += ["NBR"]
    52	    if "öl" in medium_raw or "oel" in medium_raw:
    53	        mat_whitelist += ["NBR", "FKM"]
    54	
    55	    calculated = {
    56	        "umfangsgeschwindigkeit_m_s": v_ms,
    57	        "surface_speed_m_s": round(v_ms, 3) if isinstance(v_ms, (int, float)) else None,
    58	        "pv_indicator_bar_ms": pv,
    59	        "druck_bar": druck_bar,
    60	        "temp_max_c": tmax,
    61	    }
    62	    # Filtere Nones raus
    63	    calculated = {k: v for k, v in calculated.items() if v is not None}
    64	
    65	    return {
    66	        "calculated": calculated,
    67	        "flags": {},
    68	        "warnings": [],
    69	        "requirements": [],
    70	        "material_whitelist": mat_whitelist,
    71	        "material_blacklist": mat_blacklist,
    72	    }
    73	
    74	# -------------------------
    75	#   Hydraulik-Stange
    76	# -------------------------
    77	def _calc_hydraulics_rod(params: Dict[str, Any]) -> Dict[str, Any]:
    78	    v_ms = _norm_float(params.get("geschwindigkeit_m_s"))
    79	    druck_bar = _norm_float(params.get("druck_bar"))
    80	    tmax = _norm_float(params.get("temp_max_c"))
    81	    stange_mm = _norm_float(params.get("stange_mm"))
    82	
    83	    # Umfangsgeschwindigkeit nur, wenn Drehzahl bekannt; sonst weglassen.
    84	    u_ms = None
    85	    n_rpm = _norm_float(params.get("drehzahl_u_min"))
    86	    if stange_mm and n_rpm:
    87	        d_m = stange_mm / 1000.0
    88	        n_rps = n_rpm / 60.0
    89	        u_ms = math.pi * d_m * n_rps
    90	
    91	    pv = None
    92	    if v_ms is not None and druck_bar is not None:
    93	        pv = druck_bar * v_ms
    94	
    95	    flags: Dict[str, bool] = {}
    96	    requirements: list[str] = []
    97	
    98	    if (druck_bar or 0) >= 160:
    99	        flags["extrusion_risk"] = True
   100	        requirements.append("Stütz-/Back-up-Ring prüfen (≥160 bar).")
   101	
   102	    calculated = {
   103	        "geschwindigkeit_m_s": v_ms,
   104	        "umfangsgeschwindigkeit_m_s": u_ms,
   105	        "pv_indicator_bar_ms": pv,
   106	        "druck_bar": druck_bar,
   107	        "temp_max_c": tmax,
   108	        "stange_mm": stange_mm,
   109	        "bohrung_mm": _norm_float(params.get("nut_d_mm")),
   110	    }
   111	    calculated = {k: v for k, v in calculated.items() if v is not None}
   112	
   113	    return {
   114	        "calculated": calculated,
   115	        "flags": flags,
   116	        "warnings": [],
   117	        "requirements": requirements,
   118	        "material_whitelist": [],
   119	        "material_blacklist": [],
   120	    }
   121	
   122	# --------------------------------
   123	#   Öffentlicher LangGraph-Node
   124	# --------------------------------
   125	def calc_node(state: Dict[str, Any]) -> Dict[str, Any]:
   126	    """
   127	    Aggregiert domänenspezifische Vorberechnungen und schreibt sie nach
   128	    state['derived'] zurück. Keine I/O, keine LLM-Aufrufe.
   129	    """
   130	    params: Dict[str, Any] = dict(state.get("params") or {})
   131	    domain = (state.get("domain") or "rwdr").strip().lower()
   132	
   133	    # Bestehende derived (falls vorher schon vorhanden) übernehmen
   134	    derived_in = dict(state.get("derived") or {})
   135	
   136	    try:
   137	        if domain in ("rwdr", "radial_wellendichtring", "wellendichtring"):
   138	            out = _calc_rwdr(params)
   139	        elif domain in ("hydraulics_rod", "rod", "hydraulik_stange"):
   140	            out = _calc_hydraulics_rod(params)
   141	        else:
   142	            out = {"calculated": {}, "flags": {}, "warnings": [], "requirements": []}
   143	
   144	        # zusammenführen
   145	        derived_out = _merge_derived(
   146	            derived_in,
   147	            {
   148	                "calculated": out.get("calculated", {}),
   149	                "flags": {**derived_in.get("flags", {}), **out.get("flags", {})},
   150	                "warnings": list({*(derived_in.get("warnings") or []), *(out.get("warnings") or [])}),
   151	                "requirements": list({*(derived_in.get("requirements") or []), *(out.get("requirements") or [])}),
   152	                "material_whitelist": list({*derived_in.get("material_whitelist", []), *out.get("material_whitelist", [])}),
   153	                "material_blacklist": list({*derived_in.get("material_blacklist", []), *out.get("material_blacklist", [])}),
   154	            },
   155	        )
   156	
   157	        return {**state, "derived": derived_out, "phase": "calc"}
   158	    except Exception:
   159	        # Fallback: state unverändert weiterreichen
   160	        return {**state, "derived": derived_in, "phase": "calc"}


===== FILE: backend/app/services/langgraph/graph/consult/nodes/explain.py =====
     1	# backend/app/services/langgraph/graph/consult/nodes/explain.py
     2	from __future__ import annotations
     3	
     4	from typing import Any, Dict, List, Optional, Callable
     5	import json
     6	import structlog
     7	from langchain_core.messages import AIMessage
     8	from app.services.langgraph.prompting import render_template
     9	
    10	log = structlog.get_logger(__name__)
    11	
    12	def _top_sources(docs: List[Dict[str, Any]], k: int = 3) -> List[str]:
    13	    if not docs:
    14	        return []
    15	    def _score(d: Dict[str, Any]) -> float:
    16	        try:
    17	            if d.get("fused_score") is not None:
    18	                return float(d["fused_score"])
    19	            return max(float(d.get("vector_score") or 0.0),
    20	                       float(d.get("keyword_score") or 0.0) / 100.0)
    21	        except Exception:
    22	            return 0.0
    23	    tops = sorted(docs, key=_score, reverse=True)[:k]
    24	    out: List[str] = []
    25	    for d in tops:
    26	        src = d.get("source") or (d.get("metadata") or {}).get("source") or ""
    27	        if src:
    28	            out.append(str(src))
    29	    seen, uniq = set(), []
    30	    for s in out:
    31	        if s not in seen:
    32	            seen.add(s)
    33	            uniq.append(s)
    34	    return uniq
    35	
    36	def _emit_text(events: Optional[Callable[[Dict[str, Any]], None]],
    37	               node: str, text: str, chunk_size: int = 180) -> None:
    38	    if not events or not text:
    39	        return
    40	    for i in range(0, len(text), chunk_size):
    41	        events({"type": "stream_text", "node": node, "text": text[i:i+chunk_size]})
    42	
    43	def _last_ai_text(state: Dict[str, Any]) -> str:
    44	    """Zieht den Text der letzten AIMessage (string oder tool-structured)."""
    45	    msgs = state.get("messages") or []
    46	    last_ai = None
    47	    for m in reversed(msgs):
    48	        t = (getattr(m, "type", "") or getattr(m, "role", "") or "").lower()
    49	        if t in ("ai", "assistant"):
    50	            last_ai = m
    51	            break
    52	    if not last_ai:
    53	        return ""
    54	    content = getattr(last_ai, "content", None)
    55	    if isinstance(content, str):
    56	        return content.strip()
    57	    # LangChain kann Liste aus {"type":"text","text":"..."} liefern
    58	    out_parts: List[str] = []
    59	    if isinstance(content, list):
    60	        for p in content:
    61	            if isinstance(p, str):
    62	                out_parts.append(p)
    63	            elif isinstance(p, dict) and isinstance(p.get("text"), str):
    64	                out_parts.append(p["text"])
    65	    return "\n".join(out_parts).strip()
    66	
    67	def _parse_recommendation(text: str) -> Dict[str, Any]:
    68	    """
    69	    Akzeptiert:
    70	      1) {"empfehlungen":[{typ, werkstoff, begruendung, vorteile, einschraenkungen, ...}, ...]}
    71	      2) {"main": {...}, "alternativen": [...], "hinweise":[...]}
    72	      3) {"text": "<JSON string>"}  -> wird rekursiv geparst
    73	    """
    74	    if not text:
    75	        return {}
    76	
    77	    def _loads_maybe(s: str):
    78	        try:
    79	            return json.loads(s)
    80	        except Exception:
    81	            return None
    82	
    83	    obj = _loads_maybe(text)
    84	    if isinstance(obj, dict) and "text" in obj and isinstance(obj["text"], str):
    85	        obj2 = _loads_maybe(obj["text"])
    86	        if isinstance(obj2, dict):
    87	            obj = obj2
    88	
    89	    if not isinstance(obj, dict):
    90	        return {}
    91	
    92	    # Form 2
    93	    if "main" in obj or "alternativen" in obj:
    94	        main = obj.get("main") or {}
    95	        alternativen = obj.get("alternativen") or []
    96	        hinweise = obj.get("hinweise") or []
    97	        return {"main": main, "alternativen": alternativen, "hinweise": hinweise}
    98	
    99	    # Form 1
   100	    if isinstance(obj.get("empfehlungen"), list) and obj["empfehlungen"]:
   101	        recs = obj["empfehlungen"]
   102	        main = recs[0] if isinstance(recs[0], dict) else {}
   103	        alternativen = [r for r in recs[1:] if isinstance(r, dict)]
   104	        return {"main": main, "alternativen": alternativen, "hinweise": obj.get("hinweise") or []}
   105	
   106	    return {}
   107	
   108	def explain_node(state: Dict[str, Any], *, events: Optional[Callable[[Dict[str, Any]], None]] = None) -> Dict[str, Any]:
   109	    """
   110	    Rendert die Empfehlung als freundliches Markdown (explain.jinja2),
   111	    streamt Chunks (falls WS-Events übergeben werden) und hängt eine AIMessage an.
   112	    Holt sich – falls nötig – main/alternativen automatisch aus der letzten AI-JSON.
   113	    """
   114	    params: Dict[str, Any] = state.get("params") or {}
   115	    docs: List[Dict[str, Any]] = state.get("retrieved_docs") or state.get("docs") or []
   116	    sources = _top_sources(docs, k=3)
   117	
   118	    # Falls main/alternativen/hinweise fehlen, aus der letzten AI-Message extrahieren
   119	    main = state.get("main") or {}
   120	    alternativen = state.get("alternativen") or []
   121	    hinweise = state.get("hinweise") or []
   122	    if not main and not alternativen:
   123	        parsed = _parse_recommendation(_last_ai_text(state))
   124	        if parsed:
   125	            main = parsed.get("main") or main
   126	            alternativen = parsed.get("alternativen") or alternativen
   127	            if not hinweise:
   128	                hinweise = parsed.get("hinweise") or []
   129	
   130	    md = render_template(
   131	        "explain.jinja2",
   132	        main=main or {},
   133	        alternativen=alternativen or [],
   134	        derived=state.get("derived") or {},
   135	        hinweise=hinweise or [],
   136	        params=params,
   137	        sources=sources,
   138	    ).strip()
   139	
   140	    _emit_text(events, node="explain", text=md)
   141	
   142	    msgs = (state.get("messages") or []) + [AIMessage(content=md)]
   143	    return {
   144	        **state,
   145	        "main": main,
   146	        "alternativen": alternativen,
   147	        "hinweise": hinweise,
   148	        "phase": "explain",
   149	        "messages": msgs,
   150	        "explanation": md,
   151	        "retrieved_docs": docs,
   152	    }


===== FILE: backend/app/services/langgraph/graph/consult/nodes/__init__.py =====
     1	# backend/app/services/langgraph/graph/consult/nodes/__init__.py
     2	# (nur für Paketinitialisierung)


===== FILE: backend/app/services/langgraph/graph/consult/nodes/intake.py =====
     1	# backend/app/services/langgraph/graph/consult/nodes/intake.py
     2	from __future__ import annotations
     3	
     4	import json
     5	import logging
     6	from typing import Any, Dict
     7	
     8	from langchain_core.messages import HumanMessage
     9	from app.services.langgraph.llm_factory import get_llm as create_llm
    10	from app.services.langgraph.prompting import (
    11	    render_template,
    12	    messages_for_template,
    13	    strip_json_fence,
    14	)
    15	from ..utils import normalize_messages
    16	
    17	log = logging.getLogger(__name__)
    18	
    19	
    20	def intake_node(state: Dict[str, Any]) -> Dict[str, Any]:
    21	    """
    22	    Analysiert die Eingabe, klassifiziert den Intent und extrahiert Parameter.
    23	    Schreibt Ergebnis deterministisch in state['triage'] und state['params'].
    24	    """
    25	    msgs = normalize_messages(state.get("messages", []))
    26	    params = dict(state.get("params") or {})
    27	
    28	    # Prompt rendern
    29	    prompt = render_template(
    30	        "intake_triage.jinja2",
    31	        messages=messages_for_template(msgs),
    32	        params=params,
    33	        params_json=json.dumps(params, ensure_ascii=False),
    34	    )
    35	
    36	    llm = create_llm()
    37	    try:
    38	        resp = llm.invoke([HumanMessage(content=prompt)])
    39	        raw = strip_json_fence(getattr(resp, "content", "") or "")
    40	        data = json.loads(raw)
    41	    except Exception as e:
    42	        log.warning("intake_node: Parse- oder LLM-Fehler: %s", e, exc_info=True)
    43	        data = {}
    44	
    45	    # Intent & Params sauber übernehmen
    46	    intent = str((data.get("intent") or "unknown")).strip().lower()
    47	    new_params = dict(params)
    48	    if isinstance(data.get("params"), dict):
    49	        for k, v in data["params"].items():
    50	            # nur nicht-leere Werte übernehmen
    51	            if v not in (None, "", "unknown"):
    52	                new_params[k] = v
    53	
    54	    triage = {
    55	        "intent": intent if intent in ("greeting", "smalltalk", "consult", "unknown") else "unknown",
    56	        "confidence": 1.0 if intent in ("greeting", "smalltalk", "consult") else 0.0,
    57	        "reply": "",
    58	        "flags": {"source": "intake_triage"},
    59	    }
    60	
    61	    # Keine AIMessage nötig – Routing übernimmt build._route_from_intake()
    62	    return {
    63	        "messages": [],
    64	        "params": new_params,
    65	        "triage": triage,
    66	        "phase": "intake",
    67	    }


===== FILE: backend/app/services/langgraph/graph/consult/nodes/rag.py =====
     1	# backend/app/services/langgraph/graph/consult/nodes/rag.py
     2	"""
     3	RAG-Node: holt Hybrid-Treffer (Qdrant + Redis BM25), baut kompakten
     4	Kontext-String und legt beides in den State (retrieved_docs/docs, context) ab.
     5	"""
     6	from __future__ import annotations
     7	from typing import Any, Dict, List, Optional
     8	import structlog
     9	
    10	from .....rag import rag_orchestrator as ro  # relativer Import
    11	
    12	log = structlog.get_logger(__name__)
    13	
    14	
    15	def _extract_query(state: Dict[str, Any]) -> str:
    16	    return (
    17	        state.get("query")
    18	        or state.get("question")
    19	        or state.get("user_input")
    20	        or state.get("input")
    21	        or ""
    22	    )
    23	
    24	
    25	def _extract_tenant(state: Dict[str, Any]) -> Optional[str]:
    26	    ctx = state.get("context") or {}
    27	    return state.get("tenant") or (ctx.get("tenant") if isinstance(ctx, dict) else None)
    28	
    29	
    30	def _context_from_docs(docs: List[Dict[str, Any]], max_chars: int = 1200) -> str:
    31	    """Kompakter Textkontext für Prompting (inkl. Quelle)."""
    32	    if not docs:
    33	        return ""
    34	    parts: List[str] = []
    35	    for d in docs[:6]:
    36	        t = (d.get("text") or "").strip()
    37	        if not t:
    38	            continue
    39	        src = d.get("source") or (d.get("metadata") or {}).get("source")
    40	        if src:
    41	            t = f"{t}\n[source: {src}]"
    42	        parts.append(t)
    43	    ctx = "\n\n".join(parts)
    44	    return ctx[:max_chars]
    45	
    46	
    47	def run_rag_node(state: Dict[str, Any]) -> Dict[str, Any]:
    48	    """
    49	    Eingänge (optional):
    50	      - query/question/user_input/input
    51	      - tenant bzw. context.tenant
    52	      - rag_filters, rag_k, rag_rerank
    53	    Ausgänge:
    54	      - retrieved_docs/docs: List[Dict[str, Any]]
    55	      - context: str
    56	    """
    57	    query = _extract_query(state)
    58	    tenant = _extract_tenant(state)
    59	    filters = state.get("rag_filters") or None
    60	    k = int(state.get("rag_k") or ro.FINAL_K)
    61	    use_rerank = bool(state.get("rag_rerank", True))
    62	
    63	    if not query.strip():
    64	        return {**state, "retrieved_docs": [], "docs": [], "context": "", "phase": "rag"}
    65	
    66	    docs = ro.hybrid_retrieve(
    67	        query=query,
    68	        tenant=tenant,
    69	        k=k,
    70	        metadata_filters=filters,
    71	        use_rerank=use_rerank,
    72	    )
    73	
    74	    context = state.get("context")
    75	    if not isinstance(context, str) or not context.strip():
    76	        context = _context_from_docs(docs)
    77	
    78	    out = {
    79	        **state,
    80	        "retrieved_docs": docs,
    81	        "docs": docs,              # Alias für nachfolgende Nodes
    82	        "context": context,
    83	        "phase": "rag",
    84	    }
    85	    try:
    86	        log.info("[rag_node] retrieved", n=len(docs), tenant=tenant or "-", ctx_len=len(context or ""))
    87	    except Exception:
    88	        pass
    89	    return out
    90	
    91	
    92	__all__ = ["run_rag_node"]


===== FILE: backend/app/services/langgraph/graph/consult/nodes/recommend.py =====
     1	# backend/app/services/langgraph/graph/consult/nodes/recommend.py
     2	from __future__ import annotations
     3	
     4	import json
     5	import re
     6	from typing import Any, Dict, List, Optional
     7	
     8	import structlog
     9	from langchain_core.messages import AIMessage, SystemMessage
    10	from langchain_core.runnables.config import RunnableConfig
    11	
    12	from app.services.langgraph.llm_factory import get_llm as create_llm
    13	from app.services.langgraph.prompting import (
    14	    render_template,
    15	    messages_for_template,
    16	    strip_json_fence,
    17	)
    18	from app.services.langgraph.prompt_registry import get_agent_prompt
    19	from ..utils import normalize_messages, last_user_text
    20	
    21	log = structlog.get_logger(__name__)
    22	
    23	
    24	def _extract_text_from_chunk(chunk) -> List[str]:
    25	    out: List[str] = []
    26	    if not chunk:
    27	        return out
    28	    c = getattr(chunk, "content", None)
    29	    if isinstance(c, str) and c:
    30	        out.append(c)
    31	    elif isinstance(c, list):
    32	        for part in c:
    33	            if isinstance(part, str):
    34	                out.append(part)
    35	            elif isinstance(part, dict) and isinstance(part.get("text"), str):
    36	                out.append(part["text"])
    37	    ak = getattr(chunk, "additional_kwargs", None)
    38	    if isinstance(ak, dict):
    39	        for k in ("delta", "content", "text", "token"):
    40	            v = ak.get(k)
    41	            if isinstance(v, str) and v:
    42	                out.append(v)
    43	    if isinstance(chunk, dict):
    44	        for k in ("delta", "content", "text", "token"):
    45	            v = chunk.get(k)
    46	            if isinstance(v, str) and v:
    47	                out.append(v)
    48	    return out
    49	
    50	
    51	def _extract_json_any(s: str) -> str:
    52	    s = (s or "").strip()
    53	    if not s:
    54	        return ""
    55	    if (s[:1] in "{[") and (s[-1:] in "}]"):
    56	        return s
    57	    s2 = strip_json_fence(s)
    58	    if (s2[:1] in "{[") and (s2[-1:] in "}]"):
    59	        return s2
    60	    # balanced JSON heuristics
    61	    m = re.search(r"\{(?:[^{}]|(?R))*\}", s, re.S)
    62	    if m:
    63	        return m.group(0)
    64	    m = re.search(r"\[(?:[^\[\]]|(?R))*\]", s, re.S)
    65	    return m.group(0) if m else ""
    66	
    67	
    68	def _parse_empfehlungen(raw: str) -> Optional[List[Dict[str, Any]]]:
    69	    if not raw:
    70	        return None
    71	    try:
    72	        data = json.loads(strip_json_fence(raw))
    73	        if isinstance(data, dict) and isinstance(data.get("empfehlungen"), list):
    74	            return data["empfehlungen"]
    75	    except Exception as e:
    76	        log.warning("[recommend_node] json_parse_error", err=str(e))
    77	    return None
    78	
    79	
    80	# -------- Markdown → strukturierter Fallback --------
    81	_RX = {
    82	    "typ": re.compile(r"(?im)^\s*Typ:\s*(.+?)\s*$"),
    83	    "werkstoff": re.compile(r"(?im)^\s*Werkstoff:\s*(.+?)\s*$"),
    84	    "vorteile": re.compile(
    85	        r"(?is)\bVorteile:\s*(.+?)(?:\n\s*(?:Einschr[aä]nkungen|Begr[üu]ndung|Abgeleiteter|Alternativen)\b|$)"
    86	    ),
    87	    "einschraenkungen": re.compile(
    88	        r"(?is)\bEinschr[aä]nkungen:\s*(.+?)(?:\n\s*(?:Begr[üu]ndung|Abgeleiteter|Alternativen)\b|$)"
    89	    ),
    90	    "begruendung": re.compile(
    91	        r"(?is)\bBegr[üu]ndung:\s*(.+?)(?:\n\s*(?:Abgeleiteter|Alternativen)\b|$)"
    92	    ),
    93	}
    94	
    95	
    96	def _split_items(s: str) -> List[str]:
    97	    if not s:
    98	        return []
    99	    s = re.sub(r"[•\-\u2013\u2014]\s*", ", ", s)
   100	    parts = re.split(r"[;,]\s*|\s{2,}", s.strip())
   101	    return [p.strip(" .") for p in parts if p and not p.isspace()]
   102	
   103	
   104	def _coerce_from_markdown(text: str) -> Optional[List[Dict[str, Any]]]:
   105	    if not text:
   106	        return None
   107	
   108	    def _m(rx):
   109	        m = rx.search(text)
   110	        return (m.group(1).strip() if m else "")
   111	
   112	    typ = _m(_RX["typ"])
   113	    werkstoff = _m(_RX["werkstoff"])
   114	    vorteile = _split_items(_m(_RX["vorteile"]))
   115	    einschr = _split_items(_m(_RX["einschraenkungen"]))
   116	    begr = _m(_RX["begruendung"])
   117	    if not (typ or werkstoff or begr or vorteile or einschr):
   118	        return None
   119	    return [{
   120	        "typ": typ or "",
   121	        "werkstoff": werkstoff or "",
   122	        "begruendung": begr or "",
   123	        "vorteile": vorteile or [],
   124	        "einschraenkungen": einschr or [],
   125	        "geeignet_fuer": [],
   126	    }]
   127	
   128	
   129	def _context_from_docs(docs: List[Dict[str, Any]], max_chars: int = 1200) -> str:
   130	    if not docs:
   131	        return ""
   132	    parts: List[str] = []
   133	    for d in docs[:6]:
   134	        t = (d.get("text") or "").strip()
   135	        if not t:
   136	            continue
   137	        src = d.get("source") or (d.get("metadata") or {}).get("source")
   138	        if src:
   139	            t = f"{t}\n[source: {src}]"
   140	        parts.append(t)
   141	    ctx = "\n\n".join(parts)
   142	    return ctx[:max_chars]
   143	
   144	
   145	def recommend_node(state: Dict[str, Any], config: Optional[RunnableConfig] = None) -> Dict[str, Any]:
   146	    msgs = normalize_messages(state.get("messages", []))
   147	    params: Dict[str, Any] = state.get("params") or {}
   148	    domain = (state.get("domain") or "").strip().lower()
   149	    derived = state.get("derived") or {}
   150	    retrieved_docs: List[Dict[str, Any]] = state.get("retrieved_docs") or []
   151	
   152	    context = state.get("context") or _context_from_docs(retrieved_docs)
   153	    if context:
   154	        log.info("[recommend_node] using_context", n_docs=len(retrieved_docs), ctx_len=len(context))
   155	
   156	    base_llm = create_llm()
   157	    try:
   158	        llm = base_llm.bind(response_format={"type": "json_object"})
   159	    except Exception:
   160	        llm = base_llm
   161	
   162	    recent_user = (last_user_text(msgs) or "").strip()
   163	
   164	    prompt = render_template(
   165	        "recommend.jinja2",
   166	        messages=messages_for_template(msgs),
   167	        params=params,
   168	        domain=domain,
   169	        derived=derived,
   170	        recent_user=recent_user,
   171	        context=context,
   172	    )
   173	
   174	    effective_cfg: RunnableConfig = (config or {}).copy()  # type: ignore[assignment]
   175	    if "run_name" not in (effective_cfg or {}):
   176	        effective_cfg = {**effective_cfg, "run_name": "recommend"}  # type: ignore[dict-item]
   177	
   178	    # Domänen-spezifischen Systemprompt + JSON-Instruktion kombinieren
   179	    sys_msgs = [
   180	        SystemMessage(content=get_agent_prompt(domain or "rwdr")),
   181	        SystemMessage(content=prompt),
   182	    ]
   183	
   184	    content_parts: List[str] = []
   185	    try:
   186	        for chunk in llm.with_config(effective_cfg).stream(sys_msgs):
   187	            content_parts.extend(_extract_text_from_chunk(chunk))
   188	    except Exception as e:
   189	        log.warning("[recommend_node] stream_failed", err=str(e))
   190	        try:
   191	            resp = llm.invoke(sys_msgs, config=effective_cfg)
   192	            content_parts = [getattr(resp, "content", "") or ""]
   193	        except Exception as e2:
   194	            log.error("[recommend_node] invoke_failed", err=str(e2))
   195	            payload = json.dumps({"empfehlungen": []}, ensure_ascii=False, separators=(",", ":"))
   196	            ai_msg = AIMessage(content=payload)
   197	            return {
   198	                **state,
   199	                "messages": msgs + [ai_msg],
   200	                "answer": payload,
   201	                "phase": "recommend",
   202	                "empfehlungen": [],
   203	                "retrieved_docs": retrieved_docs,
   204	                "docs": retrieved_docs,
   205	                "context": context,
   206	            }
   207	
   208	    raw = ("".join(content_parts) or "").strip()
   209	    log.info("[recommend_node] stream_len", chars=len(raw))
   210	
   211	    # 1) echtes JSON?
   212	    json_snippet = _extract_json_any(raw)
   213	    recs = _parse_empfehlungen(json_snippet) or _parse_empfehlungen(raw)
   214	
   215	    # 2) Fallback: Markdown → strukturieren
   216	    if not recs:
   217	        recs = _coerce_from_markdown(raw)
   218	
   219	    # 3) Letzter Fallback
   220	    if not recs:
   221	        recs = [{
   222	            "typ": "",
   223	            "werkstoff": "",
   224	            "begruendung": (raw[:600] if raw else "Keine strukturierte Empfehlung erhalten."),
   225	            "vorteile": [],
   226	            "einschraenkungen": [],
   227	            "geeignet_fuer": [],
   228	        }]
   229	
   230	    content_out = json.dumps({"empfehlungen": recs}, ensure_ascii=False, separators=(",", ":"))
   231	    content_out = content_out.replace("\n", " ").strip()  # sicher einzeilig
   232	
   233	    log.info("[recommend_node] emitting_json", length=len(content_out))
   234	
   235	    ai_msg = AIMessage(content=content_out)
   236	    return {
   237	        **state,
   238	        "messages": msgs + [ai_msg],
   239	        "answer": content_out,
   240	        "phase": "recommend",
   241	        "empfehlungen": recs,
   242	        "retrieved_docs": retrieved_docs,
   243	        "docs": retrieved_docs,
   244	        "context": context,
   245	    }


===== FILE: backend/app/services/langgraph/graph/consult/nodes/validate_answer.py =====
     1	# backend/app/services/langgraph/graph/consult/nodes/validate_answer.py
     2	from __future__ import annotations
     3	
     4	import math
     5	from typing import Any, Dict, List
     6	import structlog
     7	
     8	from ..state import ConsultState
     9	
    10	log = structlog.get_logger(__name__)
    11	
    12	
    13	def _sigmoid(x: float) -> float:
    14	    try:
    15	        return 1.0 / (1.0 + math.exp(-x))
    16	    except Exception:
    17	        return 0.5
    18	
    19	
    20	def _confidence_from_docs(docs: List[Dict[str, Any]]) -> float:
    21	    """
    22	    Grobe Konfidenzabschätzung aus RAG-Scores.
    23	    Nutzt fused_score, sonst max(vector_score, keyword_score/100).
    24	    """
    25	    if not docs:
    26	        return 0.15
    27	
    28	    vals: List[float] = []
    29	    for d in docs[:6]:
    30	        vs = d.get("vector_score")
    31	        ks = d.get("keyword_score")
    32	        fs = d.get("fused_score")
    33	        try:
    34	            base = float(fs if fs is not None else max(float(vs or 0.0), float(ks or 0.0) / 100.0))
    35	        except Exception:
    36	            base = 0.0
    37	        vals.append(_sigmoid(base))
    38	
    39	    conf = sum(vals) / max(1, len(vals))
    40	    return max(0.05, min(0.98, conf))
    41	
    42	
    43	def _top_source(d: Dict[str, Any]) -> str:
    44	    return (d.get("source")
    45	            or (d.get("metadata") or {}).get("source")
    46	            or "")
    47	
    48	
    49	def validate_answer(state: ConsultState) -> ConsultState:
    50	    """
    51	    Bewertet die Antwortqualität (Konfidenz/Quellen) und MERGT den State,
    52	    ohne RAG-Felder zu verlieren.
    53	    """
    54	    retrieved_docs: List[Dict[str, Any]] = state.get("retrieved_docs") or state.get("docs") or []
    55	    context: str = state.get("context") or ""
    56	
    57	    conf = _confidence_from_docs(retrieved_docs)
    58	    needs_more = bool(state.get("needs_more_params")) or conf < 0.35
    59	
    60	    validation: Dict[str, Any] = {
    61	        "n_docs": len(retrieved_docs),
    62	        "confidence": round(conf, 3),
    63	        "top_source": _top_source(retrieved_docs[0]) if retrieved_docs else "",
    64	    }
    65	
    66	    log.info(
    67	        "validate_answer",
    68	        confidence=validation["confidence"],
    69	        needs_more_params=needs_more,
    70	        n_docs=validation["n_docs"],
    71	        top_source=validation["top_source"],
    72	    )
    73	
    74	    return {
    75	        **state,
    76	        "phase": "validate_answer",
    77	        "validation": validation,
    78	        "confidence": conf,
    79	        "needs_more_params": needs_more,
    80	        # explizit erhalten
    81	        "retrieved_docs": retrieved_docs,
    82	        "docs": retrieved_docs,
    83	        "context": context,
    84	    }


===== FILE: backend/app/services/langgraph/graph/consult/nodes/validate.py =====
     1	# backend/app/services/langgraph/graph/consult/nodes/validate.py
     2	from __future__ import annotations
     3	from typing import Any, Dict
     4	
     5	
     6	def _to_float(x: Any) -> Any:
     7	    try:
     8	        if isinstance(x, bool):
     9	            return x
    10	        return float(x)
    11	    except Exception:
    12	        return x
    13	
    14	
    15	def validate_node(state: Dict[str, Any]) -> Dict[str, Any]:
    16	    """
    17	    Leichter Parameter-Check/Normalisierung vor RAG.
    18	    WICHTIG: Keine Berechnungen, kein Calculator-Aufruf – das macht calc_agent.
    19	    """
    20	    params = dict(state.get("params") or {})
    21	
    22	    # numerische Felder best-effort in float wandeln
    23	    for k in (
    24	        "temp_max_c", "temp_min_c", "druck_bar", "drehzahl_u_min",
    25	        "wellen_mm", "gehause_mm", "breite_mm",
    26	        "relativgeschwindigkeit_ms",
    27	        "tmax_c", "pressure_bar", "n_u_min", "rpm", "v_ms",
    28	    ):
    29	        if k in params and params[k] not in (None, "", []):
    30	            params[k] = _to_float(params[k])
    31	
    32	    # einfache Alias-Harmonisierung (falls Ziel noch leer)
    33	    alias = {
    34	        "tmax_c": "temp_max_c",
    35	        "pressure_bar": "druck_bar",
    36	        "n_u_min": "drehzahl_u_min",
    37	        "rpm": "drehzahl_u_min",
    38	        "v_ms": "relativgeschwindigkeit_ms",
    39	    }
    40	    for src, dst in alias.items():
    41	        if (params.get(dst) in (None, "", [])) and (params.get(src) not in (None, "", [])):
    42	            params[dst] = params[src]
    43	
    44	    return {**state, "params": params, "phase": "validate"}


===== FILE: backend/app/services/langgraph/graph/consult/state.py =====
     1	# backend/app/services/langgraph/graph/consult/state.py
     2	from __future__ import annotations
     3	
     4	from typing import Any, Dict, List, Optional, TypedDict
     5	from typing_extensions import Annotated
     6	from langchain_core.messages import AnyMessage
     7	from langgraph.graph import add_messages
     8	
     9	
    10	# ---- Parameter- & Derived-Typen -------------------------------------------------
    11	class Parameters(TypedDict, total=False):
    12	    # Kernparameter
    13	    temp_max_c: float
    14	    druck_bar: float
    15	    drehzahl_u_min: float
    16	    wellen_mm: float
    17	    relativgeschwindigkeit_ms: float
    18	    # Aliasse / Harmonisierung
    19	    tmax_c: float
    20	    pressure_bar: float
    21	    n_u_min: float
    22	    rpm: float
    23	    v_ms: float
    24	    # optionale Filter/Routing
    25	    material: str
    26	    profile: str
    27	    domain: str
    28	    norm: str
    29	    lang: str
    30	
    31	
    32	class Derived(TypedDict, total=False):
    33	    relativgeschwindigkeit_ms: float
    34	    # Platzhalter für weitere abgeleitete Größen
    35	    # z. B. reibleistung_w: float
    36	
    37	
    38	# ---- Graph-State ----------------------------------------------------------------
    39	class ConsultState(TypedDict, total=False):
    40	    # Dialog
    41	    messages: Annotated[List[AnyMessage], add_messages]
    42	    query: str
    43	
    44	    # Parameter
    45	    params: Parameters
    46	    derived: Derived
    47	
    48	    # Routing / Kontext
    49	    user_id: Optional[str]
    50	    tenant: Optional[str]
    51	    domain: Optional[str]
    52	    phase: Optional[str]
    53	    consult_required: Optional[bool]   # <-- aufgenommen, da im Flow genutzt
    54	
    55	    # ---- UI/Frontend-Integration (muss explizit im State sein!) -------------------
    56	    ui_event: Dict[str, Any]           # <-- NEW: Formular-/Sidebar-Trigger
    57	    missing_fields: List[str]          # <-- NEW: für Frontend-Anzeige
    58	
    59	    # --- RAG-Ergebnis (muss hier definiert sein, sonst wird es vom Graph gedroppt!) ---
    60	    retrieved_docs: List[Dict[str, Any]]   # Hybrid-Treffer inkl. Scores/Quellen
    61	    context: str                            # kompakter Prompt-Kontext aus Treffern
    62	
    63	    # Empfehlungen / Ergebnis
    64	    empfehlungen: List[Dict[str, Any]]
    65	
    66	    # Qualitäts-/Validierungsinfos
    67	    validation: Dict[str, Any]
    68	    confidence: float
    69	    needs_more_params: bool
    70	
    71	    # --- Legacy-Felder (Kompatibilität; werden vom aktuellen Flow nicht aktiv gesetzt) ---
    72	    docs: List[Dict[str, Any]]
    73	    citations: List[str]
    74	    answer: Optional[str]


===== FILE: backend/app/services/langgraph/graph/consult/utils.py =====
     1	# backend/app/services/langgraph/graph/consult/utils.py
     2	from __future__ import annotations
     3	
     4	import logging
     5	import re
     6	from typing import Any, Dict, Iterable, List, Optional
     7	
     8	from langgraph.graph.message import add_messages
     9	from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, SystemMessage
    10	
    11	log = logging.getLogger(__name__)
    12	
    13	# -------------------------------------------------------------------
    14	# Message utilities
    15	# -------------------------------------------------------------------
    16	
    17	def deserialize_message(x: Any) -> AnyMessage:
    18	    """Robuste Konvertierung nach LangChain-Message-Objekten."""
    19	    if isinstance(x, (HumanMessage, AIMessage, SystemMessage)):
    20	        return x
    21	    if isinstance(x, dict) and "role" in x:
    22	        role = (x.get("role") or "").lower()
    23	        content = x.get("content") or ""
    24	        if role in ("user", "human"):
    25	            return HumanMessage(content=content)
    26	        if role in ("assistant", "ai"):
    27	            return AIMessage(content=content)
    28	        if role == "system":
    29	            return SystemMessage(content=content)
    30	    if isinstance(x, str):
    31	        return HumanMessage(content=x)
    32	    return HumanMessage(content=str(x))
    33	
    34	
    35	def normalize_messages(seq: Iterable[Any]) -> List[AnyMessage]:
    36	    return [deserialize_message(m) for m in (seq or [])]
    37	
    38	
    39	def merge_messages(left: Iterable[Any], right: Iterable[Any]) -> List[AnyMessage]:
    40	    return add_messages(normalize_messages(left), normalize_messages(right))
    41	
    42	
    43	def last_user_text(msgs: List[AnyMessage]) -> str:
    44	    for m in reversed(msgs or []):
    45	        if isinstance(m, HumanMessage):
    46	            return (m.content or "").strip()
    47	    return ""
    48	
    49	
    50	def messages_text(msgs: List[AnyMessage], *, only_user: bool = False) -> str:
    51	    """
    52	    Verkettet Text aller Messages.
    53	    - only_user=True -> nur HumanMessage.
    54	    """
    55	    parts: List[str] = []
    56	    for m in msgs or []:
    57	        if only_user and not isinstance(m, HumanMessage):
    58	            continue
    59	        c = getattr(m, "content", None)
    60	        if isinstance(c, str) and c:
    61	            parts.append(c)
    62	    return "\n".join(parts)
    63	
    64	
    65	# Kompatibilitäts-Alias (einige Module importieren 'msgs_text')
    66	msgs_text = messages_text
    67	
    68	
    69	def only_user_text(msgs: List[AnyMessage]) -> str:
    70	    """Nur die User-Texte zusammengefasst (ohne Lowercasing)."""
    71	    return messages_text(msgs, only_user=True)
    72	
    73	
    74	def only_user_text_lower(msgs: List[AnyMessage]) -> str:
    75	    """Nur die User-Texte, zu Kleinbuchstaben normalisiert."""
    76	    return only_user_text(msgs).lower()
    77	
    78	
    79	# -------------------------------------------------------------------
    80	# Numeric parsing & heuristics
    81	# -------------------------------------------------------------------
    82	
    83	def _num_from_str(raw: str) -> Optional[float]:
    84	    """Float aus Strings wie '1 200,5' oder '1.200,5' oder '1200.5' extrahieren."""
    85	    try:
    86	        s = (raw or "").replace(" ", "").replace(".", "").replace(",", ".")
    87	        return float(s)
    88	    except Exception:
    89	        return None
    90	
    91	
    92	def apply_heuristics_from_text(params: Dict[str, Any], text: str) -> Dict[str, Any]:
    93	    """
    94	    Deterministische Fallbacks, falls das LLM Werte nicht gesetzt hat:
    95	      - 'kein/ohne Überdruck/Druck' -> druck_bar = 0
    96	      - '... Druck: 5 bar'          -> druck_bar = 5
    97	      - 'Drehzahl 1.200 U/min'      -> drehzahl_u_min = 1200
    98	      - 'dauerhaft X U/min'         -> drehzahl_u_min = X
    99	      - 'Geschwindigkeit 0.5 m/s'   -> geschwindigkeit_m_s = 0.5
   100	    """
   101	    t = (text or "").lower()
   102	    merged: Dict[str, Any] = dict(params or {})
   103	
   104	    # Druck
   105	    if merged.get("druck_bar") in (None, "", "unknown"):
   106	        if re.search(r"\b(kein|ohne)\s+(überdruck|ueberdruck|druck)\b", t, re.I):
   107	            merged["druck_bar"] = 0.0
   108	        else:
   109	            m = re.search(r"(?:überdruck|ueberdruck|druck)\s*[:=]?\s*([0-9][\d\.\s,]*)\s*bar\b", t, re.I)
   110	            if m:
   111	                val = _num_from_str(m.group(1))
   112	                if val is not None:
   113	                    merged["druck_bar"] = val
   114	
   115	    # Drehzahl (generisch)
   116	    if merged.get("drehzahl_u_min") in (None, "", "unknown"):
   117	        m = re.search(r"drehzahl[^0-9]{0,12}([0-9][\d\.\s,]*)\s*(?:u\s*/?\s*min|rpm)\b", t, re.I)
   118	        if m:
   119	            val = _num_from_str(m.group(1))
   120	            if val is not None:
   121	                merged["drehzahl_u_min"] = int(round(val))
   122	
   123	    # Spezifisch „dauerhaft“
   124	    m_dauer = re.search(
   125	        r"(dauerhaft|kontinuierlich)[^0-9]{0,12}([0-9][\d\.\s,]*)\s*(?:u\s*/?\s*min|rpm)\b",
   126	        t,
   127	        re.I,
   128	    )
   129	    if m_dauer:
   130	        val = _num_from_str(m_dauer.group(2))
   131	        if val is not None:
   132	            merged["drehzahl_u_min"] = int(round(val))
   133	
   134	    # Relativgeschwindigkeit in m/s
   135	    if merged.get("geschwindigkeit_m_s") in (None, "", "unknown"):
   136	        m_speed = re.search(r"(geschwindigkeit|v)[^0-9]{0,12}([0-9][\d\.\s,]*)\s*m\s*/\s*s", t, re.I)
   137	        if m_speed:
   138	            val = _num_from_str(m_speed.group(2))
   139	            if val is not None:
   140	                merged["geschwindigkeit_m_s"] = float(val)
   141	
   142	    return merged
   143	
   144	
   145	# -------------------------------------------------------------------
   146	# Validation & anomaly messages
   147	# -------------------------------------------------------------------
   148	
   149	def _is_missing_value(key: str, val: Any) -> bool:
   150	    if val is None or val == "" or val == "unknown":
   151	        return True
   152	    # 0 bar ist gültig
   153	    if key == "druck_bar":
   154	        try:
   155	            float(val)
   156	            return False
   157	        except Exception:
   158	            return True
   159	    # Positive Größen brauchen > 0
   160	    if key in (
   161	        "wellen_mm", "gehause_mm", "breite_mm", "drehzahl_u_min", "geschwindigkeit_m_s",
   162	        "stange_mm", "nut_d_mm", "nut_b_mm"
   163	    ):
   164	        try:
   165	            return float(val) <= 0
   166	        except Exception:
   167	            return True
   168	    # temp_max_c: nur presence check
   169	    if key == "temp_max_c":
   170	        try:
   171	            float(val)
   172	            return False
   173	        except Exception:
   174	            return True
   175	    return False
   176	
   177	
   178	def _required_fields_by_domain(domain: str) -> List[str]:
   179	    # Hydraulik-Stange nutzt stange_mm / nut_d_mm / nut_b_mm
   180	    if (domain or "rwdr") == "hydraulics_rod":
   181	        return [
   182	            "falltyp",
   183	            "stange_mm",
   184	            "nut_d_mm",
   185	            "nut_b_mm",
   186	            "medium",
   187	            "temp_max_c",
   188	            "druck_bar",
   189	            "geschwindigkeit_m_s",
   190	        ]
   191	    # default: rwdr
   192	    return [
   193	        "falltyp",
   194	        "wellen_mm",
   195	        "gehause_mm",
   196	        "breite_mm",
   197	        "medium",
   198	        "temp_max_c",
   199	        "druck_bar",
   200	        "drehzahl_u_min",
   201	    ]
   202	
   203	
   204	def _missing_by_domain(domain: str, params: Dict[str, Any]) -> List[str]:
   205	    req = _required_fields_by_domain(domain or "rwdr")
   206	    return [k for k in req if _is_missing_value(k, (params or {}).get(k))]
   207	
   208	
   209	# Öffentlicher Alias (manche Module importieren ohne Unterstrich)
   210	missing_by_domain = _missing_by_domain
   211	
   212	
   213	def _anomaly_messages(domain: str, params: Dict[str, Any], derived: Dict[str, Any]) -> List[str]:
   214	    """
   215	    Erzeugt Rückfragen basierend auf abgeleiteten Flags (domainabhängig).
   216	    Erwartet 'derived' z. B.: {"flags": {...}, "warnings": [...], "requirements": [...]}
   217	    """
   218	    msgs: List[str] = []
   219	    flags = (derived.get("flags") or {})
   220	
   221	    # RWDR – Druckstufenfreigabe
   222	    if flags.get("requires_pressure_stage") and not flags.get("pressure_stage_ack"):
   223	        msgs.append(
   224	            "Ein Überdruck >2 bar ist für Standard-Radialdichtringe kritisch. "
   225	            "Dürfen Druckstufenlösungen geprüft werden?"
   226	        )
   227	
   228	    # Hohe Drehzahl/Geschwindigkeit
   229	    if flags.get("speed_high"):
   230	        msgs.append("Die Drehzahl/Umfangsgeschwindigkeit ist hoch – ist sie dauerhaft oder nur kurzzeitig (Spitzen)?")
   231	
   232	    # Sehr hohe Temperatur
   233	    if flags.get("temp_very_high"):
   234	        msgs.append("Die Temperatur ist sehr hoch. Handelt es sich um Dauer- oder Spitzentemperaturen?")
   235	
   236	    # Hydraulik Stange – Extrusions-/Back-up-Ring-Freigabe
   237	    if (domain or "") == "hydraulics_rod" and flags.get("extrusion_risk") and not flags.get("extrusion_risk_ack"):
   238	        msgs.append("Bei dem Druck besteht Extrusionsrisiko. Darf eine Stütz-/Back-up-Ring-Lösung geprüft werden?")
   239	
   240	    return msgs
   241	
   242	
   243	# Öffentlicher Alias
   244	anomaly_messages = _anomaly_messages
   245	
   246	# --- Output-Cleaner: Leading-Meta entfernen, Suffix-Echos entfernen, De-Dupe --
   247	
   248	def _strip(s: str) -> str:
   249	    return (s or "").strip()
   250	
   251	def _normalize_newlines(text: str) -> str:
   252	    """Normalisiert Zeilenenden und trimmt überflüssige Leerzeichen am Zeilenende."""
   253	    if not isinstance(text, str):
   254	        return text
   255	    t = re.sub(r"\r\n?|\r", "\n", text)
   256	    t = "\n".join(line.rstrip() for line in t.split("\n"))
   257	    return t
   258	
   259	def strip_leading_meta_blocks(text: str) -> str:
   260	    """
   261	    Entfernt am *Anfang* der Antwort Meta-Blöcke wie:
   262	      - führende JSON-/YAML-Objekte
   263	      - ```…``` fenced code blocks
   264	      - '# QA-Notiz …' bis zur nächsten Leerzeile
   265	    Wir iterieren, bis kein solcher Block mehr vorne steht.
   266	    """
   267	    if not isinstance(text, str) or not text.strip():
   268	        return text
   269	    t = text.lstrip()
   270	
   271	    changed = True
   272	    # max. 5 Durchläufe als Sicherung
   273	    for _ in range(5):
   274	        if not changed:
   275	            break
   276	        changed = False
   277	
   278	        # Fenced code block (beliebiges fence, inkl. json/yaml)
   279	        m = re.match(r"^\s*```[\s\S]*?```\s*", t)
   280	        if m:
   281	            t = t[m.end():].lstrip()
   282	            changed = True
   283	            continue
   284	
   285	        # Führendes JSON-/YAML-Objekt (heuristisch, nicht perfekt balanciert)
   286	        m = re.match(r"^\s*\{[\s\S]*?\}\s*(?=\n|$)", t)
   287	        if m:
   288	            t = t[m.end():].lstrip()
   289	            changed = True
   290	            continue
   291	        m = re.match(r"^\s*---[\s\S]*?---\s*(?=\n|$)", t)  # YAML frontmatter
   292	        if m:
   293	            t = t[m.end():].lstrip()
   294	            changed = True
   295	            continue
   296	
   297	        # QA-Notiz-Block bis zur nächsten Leerzeile
   298	        m = re.match(r"^\s*#\s*QA-Notiz[^\n]*\n[\s\S]*?(?:\n\s*\n|$)", t, flags=re.IGNORECASE)
   299	        if m:
   300	            t = t[m.end():].lstrip()
   301	            changed = True
   302	            continue
   303	
   304	    return t
   305	
   306	def clean_ai_output(ai_text: str, recent_user_texts: List[str]) -> str:
   307	    """
   308	    Entfernt angehängte Echos zuletzt gesagter User-Texte am Ende der AI-Ausgabe.
   309	    - vergleicht trim-normalisiert (Suffix)
   310	    - entfernt ganze trailing Blöcke, falls sie exakt einem der recent_user_texts entsprechen
   311	    """
   312	    if not isinstance(ai_text, str) or not ai_text:
   313	        return ai_text
   314	
   315	    out = ai_text.rstrip()
   316	
   317	    # Prüfe Kandidaten in abnehmender Länge (stabil gegen Teilmengen)
   318	    for u in sorted(set(recent_user_texts or []), key=len, reverse=True):
   319	        u_s = _strip(u)
   320	        if not u_s:
   321	            continue
   322	
   323	        # Work on a normalized working copy for suffix check
   324	        norm_out = _strip(out)
   325	        if norm_out.endswith(u_s):
   326	            # schneide die letzte (nicht-normalisierte) Vorkommen-Stelle am Ende ab
   327	            raw_idx = out.rstrip().rfind(u_s)
   328	            if raw_idx != -1:
   329	                out = out[:raw_idx].rstrip()
   330	
   331	    return out
   332	
   333	def _norm_key(block: str) -> str:
   334	    """Normierungs-Schlüssel für Block-Vergleich (whitespace-/case-insensitiv)."""
   335	    return re.sub(r"\s+", " ", (block or "").strip()).lower()
   336	
   337	def dedupe_text_blocks(text: str) -> str:
   338	    """
   339	    Entfernt doppelte inhaltlich identische Absätze/Blöcke, robust gegen CRLF
   340	    und gemischte Leerzeilen. Als Absatztrenner gilt: ≥1 (auch nur whitespace-) Leerzeile.
   341	    Zusätzlich werden identische, aufeinanderfolgende Einzelzeilen entfernt.
   342	    """
   343	    if not isinstance(text, str) or not text.strip():
   344	        return text
   345	
   346	    t = _normalize_newlines(text)
   347	
   348	    # Absätze anhand *mindestens* einer Leerzeile trennen (auch wenn nur Whitespace in der Leerzeile steht)
   349	    parts = [p.strip() for p in re.split(r"\n\s*\n+", t.strip()) if p.strip()]
   350	
   351	    seen = set()
   352	    out_blocks = []
   353	    for p in parts:
   354	        k = _norm_key(p)
   355	        if k in seen:
   356	            continue
   357	        seen.add(k)
   358	        out_blocks.append(p)
   359	
   360	    # Zusammensetzen mit Leerzeile zwischen Absätzen
   361	    merged = "\n\n".join(out_blocks)
   362	
   363	    # Zusätzlicher Schutz: identische direkt aufeinanderfolgende Zeilen entfernen
   364	    final_lines = []
   365	    prev_key = None
   366	    for line in merged.split("\n"):
   367	        key = _norm_key(line)
   368	        if key and key == prev_key:
   369	            continue
   370	        final_lines.append(line)
   371	        prev_key = key
   372	
   373	    return "\n".join(final_lines)
   374	
   375	def clean_and_dedupe(ai_text: str, recent_user_texts: List[str]) -> str:
   376	    """
   377	    Reihenfolge:
   378	      1) Führende Meta-Blöcke entfernen
   379	      2) Trailing User-Echos abschneiden
   380	      3) Identische Absätze/Zeilen de-dupen
   381	    """
   382	    head_clean = strip_leading_meta_blocks(ai_text)
   383	    tail_clean = clean_ai_output(head_clean, recent_user_texts)
   384	    return dedupe_text_blocks(tail_clean)


===== FILE: backend/app/services/langgraph/graph/__init__.py =====


===== FILE: backend/app/services/langgraph/graph/intent_router.py =====
     1	# backend/app/services/langgraph/graph/intent_router.py
     2	from __future__ import annotations
     3	
     4	import json
     5	import os
     6	import re
     7	from typing import Any, Literal, Sequence
     8	
     9	from ..prompting import render_template
    10	from app.services.langgraph.llm_router import get_router_llm, get_router_fallback_llm
    11	from langchain_core.messages import HumanMessage
    12	
    13	_TEMPLATE_FILE = "intent_router.jinja2"
    14	
    15	# simple fast-path regex (keine Änderung)
    16	_FAST_CONSULT_RX = re.compile(
    17	    r"\b(optimal\w*\s+(dichtung|empfehlung|lösung|auswahl)|"
    18	    r"optimale\s+dichtungsempfehlung|rwdr|wellendichtring|25x\d+x\d+)\b",
    19	    re.I,
    20	)
    21	
    22	def _last_user_text(messages: Sequence[Any]) -> str:
    23	    if not messages:
    24	        return ""
    25	    for m in reversed(messages):
    26	        content = getattr(m, "content", None)
    27	        if content:
    28	            return str(content)
    29	        if isinstance(m, dict):
    30	            c = m.get("content") or m.get("text") or m.get("message")
    31	            if c:
    32	                return str(c)
    33	    return ""
    34	
    35	def _strip_json_fence(text: str) -> str:
    36	    if not text:
    37	        return ""
    38	    m = re.search(r"```(?:json)?\s*(.*?)\s*```", text, flags=re.DOTALL | re.IGNORECASE)
    39	    if m:
    40	        return m.group(1).strip()
    41	    return text.strip().strip("`").strip()
    42	
    43	def _conf_min() -> float:
    44	    try:
    45	        return float(os.getenv("INTENT_CONF_MIN", "0.60"))
    46	    except Exception:
    47	        return 0.60
    48	
    49	def _safe_parse_intent(s: str) -> tuple[str, float]:
    50	    try:
    51	        data = json.loads(_strip_json_fence(s))
    52	        intent = str(data.get("intent") or "").strip().lower()
    53	        conf = float(data.get("confidence") or 0.0)
    54	        return intent, conf
    55	    except Exception:
    56	        return "", 0.0
    57	
    58	def classify_intent(base_llm: Any, messages: Sequence[Any]) -> Literal["consult", "chitchat"]:
    59	    """
    60	    Intent-Routing:
    61	      - Fast-Path Regex
    62	      - Router-LLM (mini) + Fallback (small)
    63	      - Confidence-Threshold
    64	      - Fail-open -> 'consult'
    65	    """
    66	    user_text = _last_user_text(messages).strip()
    67	    if _FAST_CONSULT_RX.search(user_text):
    68	        return "consult"
    69	
    70	    router = get_router_llm()
    71	    fallback = get_router_fallback_llm()
    72	    prompt = render_template(_TEMPLATE_FILE, input_text=user_text)
    73	
    74	    def _ask(llm) -> tuple[str, float]:
    75	        resp = llm.invoke([HumanMessage(content=prompt)])
    76	        content = getattr(resp, "content", None) or str(resp)
    77	        return _safe_parse_intent(content)
    78	
    79	    try:
    80	        intent, conf = _ask(router)
    81	    except Exception:
    82	        try:
    83	            intent, conf = _ask(fallback)
    84	        except Exception:
    85	            return "consult"
    86	
    87	    if conf < _conf_min():
    88	        try:
    89	            i2, c2 = _ask(fallback)
    90	            if c2 >= conf:
    91	                intent, conf = i2, c2
    92	        except Exception:
    93	            pass
    94	
    95	    if intent == "smalltalk" and conf >= _conf_min():
    96	        return "chitchat"
    97	    return "consult"


===== FILE: backend/app/services/langgraph/graph/nodes/deterministic.py =====
     1	from __future__ import annotations
     2	from typing import Dict, Any
     3	from math import pi
     4	
     5	def intake_validate(state: Dict[str, Any]) -> Dict[str, Any]:
     6	    p = state.get("params", {}) or {}
     7	    missing = []
     8	    for k in ("medium", "pressure_bar", "temp_max_c"):
     9	        if p.get(k) is None:
    10	            missing.append(k)
    11	    if state.get("mode") == "consult" and p.get("speed_rpm") is None:
    12	        missing.append("speed_rpm")
    13	    state.setdefault("derived", {}).setdefault("notes", [])
    14	    if missing:
    15	        state.setdefault("ui_events", []).append({
    16	            "ui_action": "open_form",
    17	            "payload": {"form_id": "rwdr_params_v1", "missing": missing, "prefill": p}
    18	        })
    19	    return state
    20	
    21	def _v_m_s(d_mm: float, rpm: float) -> float:
    22	    return pi * (d_mm/1000.0) * (rpm/60.0)
    23	
    24	def _dn(d_mm: float, rpm: float) -> float:
    25	    return d_mm * rpm
    26	
    27	def calc_core(state: Dict[str, Any]) -> Dict[str, Any]:
    28	    p = state.get("params", {}) or {}
    29	    d = state.get("derived", {}) or {}
    30	    shaft = p.get("shaft_d")
    31	    rpm = p.get("speed_rpm")
    32	    pressure = p.get("pressure_bar")
    33	    if shaft and rpm:
    34	        v = _v_m_s(shaft, rpm)
    35	        d["v_m_s"] = round(v, 4)
    36	        d["dn_value"] = round(_dn(shaft, rpm), 2)
    37	    if pressure and "v_m_s" in d:
    38	        d["pv_indicator_bar_ms"] = round(float(pressure) * float(d["v_m_s"]), 4)
    39	    state["derived"] = d
    40	    return state
    41	
    42	def calc_advanced(state: Dict[str, Any]) -> Dict[str, Any]:
    43	    p = state.get("params", {}) or {}
    44	    notes = state.setdefault("derived", {}).setdefault("notes", [])
    45	    if p.get("temp_max_c", 0) > 200:
    46	        notes.append("Hohe Temperatur: Fluorpolymere prüfen.")
    47	    if (p.get("pressure_bar") or 0) > 5:
    48	        notes.append("Druck > 5 bar: Stützelement/Extrusionsschutz prüfen.")
    49	    return state


===== FILE: backend/app/services/langgraph/graph/nodes/explain_nodes.py =====
     1	from __future__ import annotations
     2	from typing import Dict, Any
     3	from app.services.langgraph.policies.model_routing import RoutingContext, llm_params_for, should_use_llm
     4	from app.services.langgraph.llm_factory import get_llm
     5	
     6	def explain(state: Dict[str, Any]) -> Dict[str, Any]:
     7	    if not should_use_llm("explain"):
     8	        return state
     9	    ctx = RoutingContext(
    10	        node="explain",
    11	        confidence=state.get("confidence"),
    12	        red_flags=bool(state.get("red_flags")),
    13	        regulatory=bool(state.get("regulatory")),
    14	    )
    15	    llm_cfg = llm_params_for("explain", ctx)
    16	    # sanitize unsupported kwargs
    17	    llm_cfg.pop("top_p", None)
    18	    llm = get_llm(**llm_cfg)
    19	
    20	    params = state.get("params", {})
    21	    derived = state.get("derived", {})
    22	    sources = state.get("sources", [])
    23	    prompt = (
    24	        "Erkläre die Auswahlkriterien kurz und sachlich. Nutze nur PARAMS, abgeleitete Werte und Quellen. "
    25	        "Keine Produkte, keine Entscheidungen. Quellen benennen.\n"
    26	        f"PARAMS: {params}\nDERIVED: {derived}\nSOURCES: {sources}\n"
    27	        "Gib 3–6 Sätze."
    28	    )
    29	    msg = llm.invoke([{"role": "user", "content": prompt}])
    30	    state.setdefault("messages", []).append({"role": "assistant", "content": msg.content})
    31	    return state


===== FILE: backend/app/services/langgraph/graph/nodes/__init__.py =====


===== FILE: backend/app/services/langgraph/graph/nodes/rag_nodes.py =====
     1	from __future__ import annotations
     2	from typing import Dict, Any, List
     3	from datetime import date
     4	from app.services.langgraph.tools.telemetry import telemetry, PARTNER_COVERAGE, NO_MATCH_RATE
     5	
     6	def rag_retrieve(state: Dict[str, Any]) -> Dict[str, Any]:
     7	    cands = state.get("candidates") or []
     8	    state["sources"] = state.get("sources") or []
     9	    state.setdefault("telemetry", {})["candidates_total"] = len(cands)
    10	    return state
    11	
    12	def _is_partner(c: Dict[str, Any]) -> bool:
    13	    tier = (c.get("paid_tier") or "none").lower()
    14	    active = bool(c.get("active", False))
    15	    valid_until = (c.get("contract_valid_until") or "")
    16	    try:
    17	        y, m, d = map(int, valid_until.split("-"))
    18	        ok_date = date(y, m, d) >= date.today()
    19	    except Exception:
    20	        ok_date = False
    21	    return tier != "none" and active and ok_date
    22	
    23	def partner_only_filter(state: Dict[str, Any]) -> Dict[str, Any]:
    24	    cands: List[Dict[str, Any]] = state.get("candidates") or []
    25	    partners = [c for c in cands if _is_partner(c)]
    26	    state["candidates"] = partners
    27	    total = state.get("telemetry", {}).get("candidates_total", 0)
    28	    coverage = (len(partners) / total) if total else 0.0
    29	    telemetry.set_gauge(PARTNER_COVERAGE, coverage)
    30	    if not partners:
    31	        state.setdefault("ui_events", []).append({"ui_action": "no_partner_available", "payload": {}})
    32	        telemetry.incr(NO_MATCH_RATE, 1)
    33	    return state
    34	
    35	def rules_filter(state: Dict[str, Any]) -> Dict[str, Any]:
    36	    return state


===== FILE: backend/app/services/langgraph/graph/nodes/rfq_nodes.py =====
     1	from __future__ import annotations
     2	from typing import Dict, Any
     3	from datetime import datetime
     4	import os, uuid
     5	from app.services.langgraph.pdf.rfq_renderer import generate_rfq_pdf
     6	from app.services.langgraph.tools.telemetry import telemetry, RFQ_GENERATED
     7	from app.services.langgraph.tools.ui_events import UI, make_event
     8	
     9	def decision_ready(state: Dict[str, Any]) -> Dict[str, Any]:
    10	    state.setdefault("ui_events", []).append(make_event(UI["DECISION_READY"], summary={
    11	        "params": state.get("params"),
    12	        "derived": state.get("derived"),
    13	        "candidate_count": len(state.get("candidates") or []),
    14	    }))
    15	    return state
    16	
    17	def await_user_action(state: Dict[str, Any]) -> Dict[str, Any]:
    18	    return state
    19	
    20	def generate_rfq_pdf_node(state: Dict[str, Any]) -> Dict[str, Any]:
    21	    if state.get("user_action") != "export_pdf":
    22	        return state
    23	    out_dir = os.getenv("RFQ_PDF_DIR", "/app/data/rfq")
    24	    os.makedirs(out_dir, exist_ok=True)
    25	    fname = f"rfq_{datetime.utcnow().strftime('%Y%m%dT%H%M%S')}_{uuid.uuid4().hex[:6]}.pdf"
    26	    path = os.path.join(out_dir, fname)
    27	    payload = {
    28	        "params": state.get("params"),
    29	        "derived": state.get("derived"),
    30	        "candidates": state.get("candidates"),
    31	        "sources": state.get("sources"),
    32	        "legal_notice": "Verbindliche Eignungszusage obliegt dem Hersteller.",
    33	    }
    34	    generate_rfq_pdf(payload, path)
    35	    state["rfq_pdf"] = {"path": path, "created_at": datetime.utcnow().isoformat() + "Z", "download_token": uuid.uuid4().hex}
    36	    telemetry.incr(RFQ_GENERATED, 1)
    37	    return state
    38	
    39	def deliver_pdf(state: Dict[str, Any]) -> Dict[str, Any]:
    40	    if state.get("rfq_pdf"):
    41	        state.setdefault("ui_events", []).append(make_event(UI["RFQ_READY"], pdf=state["rfq_pdf"]))
    42	    return state


===== FILE: backend/app/services/langgraph/graph/orchestrator.py =====
     1	from __future__ import annotations
     2	from typing import Dict, Any
     3	from langgraph.graph import StateGraph, END
     4	from app.services.langgraph.graph.state import SealAIState
     5	from app.services.langgraph.graph.nodes.deterministic import intake_validate, calc_core, calc_advanced
     6	from app.services.langgraph.graph.nodes.rag_nodes import rag_retrieve, partner_only_filter, rules_filter
     7	from app.services.langgraph.graph.nodes.explain_nodes import explain
     8	from app.services.langgraph.graph.nodes.rfq_nodes import decision_ready, await_user_action, generate_rfq_pdf_node, deliver_pdf
     9	
    10	def build_sealai_graph() -> StateGraph:
    11	    g = StateGraph(SealAIState)
    12	    g.add_node("intake_validate", intake_validate)
    13	    g.add_node("calc_core", calc_core)
    14	    g.add_node("calc_advanced", calc_advanced)
    15	    g.add_node("rag_retrieve", rag_retrieve)
    16	    g.add_node("partner_only_filter", partner_only_filter)
    17	    g.add_node("rules_filter", rules_filter)
    18	    g.add_node("explain", explain)
    19	    g.add_node("decision_ready", decision_ready)
    20	    g.add_node("await_user_action", await_user_action)
    21	    g.add_node("generate_rfq_pdf", generate_rfq_pdf_node)
    22	    g.add_node("deliver_pdf", deliver_pdf)
    23	
    24	    g.set_entry_point("intake_validate")
    25	    g.add_edge("intake_validate", "calc_core")
    26	    g.add_edge("calc_core", "calc_advanced")
    27	    g.add_edge("calc_advanced", "rag_retrieve")
    28	    g.add_edge("rag_retrieve", "partner_only_filter")
    29	    g.add_edge("partner_only_filter", "rules_filter")
    30	    g.add_edge("rules_filter", "explain")
    31	    g.add_edge("explain", "decision_ready")
    32	    g.add_edge("decision_ready", "await_user_action")
    33	    g.add_edge("await_user_action", "generate_rfq_pdf")
    34	    g.add_edge("generate_rfq_pdf", "deliver_pdf")
    35	    g.add_edge("deliver_pdf", END)
    36	    return g
    37	
    38	def invoke_sealai(state: Dict[str, Any]) -> Dict[str, Any]:
    39	    mode = (state.get("mode") or "consult").lower()
    40	    g = build_sealai_graph().compile()
    41	    out = g.invoke(state)
    42	    if mode != "consult":
    43	        out.pop("rfq_pdf", None)
    44	        out["ui_events"] = [e for e in out.get("ui_events", []) if e.get("ui_action") != "rfq_ready"]
    45	    return out


===== FILE: backend/app/services/langgraph/graph/sealai_consult_flow.py =====
     1	# backend/app/services/langgraph/graph/sealai_consult_flow.py
     2	from __future__ import annotations
     3	
     4	import os
     5	import logging
     6	
     7	from .consult.io import invoke_consult as _invoke_consult_single
     8	
     9	log = logging.getLogger(__name__)
    10	
    11	_SUP_AVAILABLE = True
    12	try:
    13	    from .supervisor_graph import invoke_consult_supervisor as _invoke_consult_supervisor
    14	except Exception as e:
    15	    _SUP_AVAILABLE = False
    16	    log.warning("Supervisor graph not available, falling back to single-agent: %s", e)
    17	
    18	_MODE = os.getenv("CONSULT_MODE", "consult").strip().lower()
    19	
    20	def invoke_consult(prompt: str, *, thread_id: str) -> str:
    21	    use_supervisor = (_MODE == "supervisor" and _SUP_AVAILABLE)
    22	    if use_supervisor:
    23	        try:
    24	            return _invoke_consult_supervisor(prompt, thread_id=thread_id)
    25	        except Exception as e:
    26	            log.exception("Supervisor failed, falling back to single-agent: %s", e)
    27	    return _invoke_consult_single(prompt, thread_id=thread_id)


===== FILE: backend/app/services/langgraph/graph/state.py =====
     1	from __future__ import annotations
     2	from typing import Any, Dict, List, Optional, TypedDict
     3	from typing_extensions import Annotated
     4	from langgraph.graph import add_messages
     5	
     6	class Params(TypedDict, total=False):
     7	    shaft_d: float
     8	    housing_d: float
     9	    width: float
    10	    medium: str
    11	    pressure_bar: float
    12	    temp_min_c: float
    13	    temp_max_c: float
    14	    speed_rpm: float
    15	
    16	class Derived(TypedDict, total=False):
    17	    v_m_s: float
    18	    dn_value: float
    19	    pv_indicator_bar_ms: float
    20	    notes: List[str]
    21	
    22	class Candidate(TypedDict, total=False):
    23	    doc_id: str
    24	    vendor_id: str
    25	    title: str
    26	    profile: str
    27	    material: str
    28	    paid_tier: str
    29	    contract_valid_until: str
    30	    active: bool
    31	    score: float
    32	    url: Optional[str]
    33	
    34	class RFQPdfInfo(TypedDict, total=False):
    35	    path: str
    36	    created_at: str
    37	    download_token: str
    38	
    39	class UIEvent(TypedDict, total=False):
    40	    ui_action: str
    41	    payload: Dict[str, Any]
    42	
    43	class SealAIState(TypedDict, total=False):
    44	    messages: Annotated[List[Any], add_messages]
    45	    mode: str
    46	    params: Params
    47	    derived: Derived
    48	    candidates: List[Candidate]
    49	    sources: List[Dict[str, Any]]
    50	    user_action: Optional[str]
    51	    rfq_pdf: Optional[RFQPdfInfo]
    52	    ui_events: List[UIEvent]
    53	    telemetry: Dict[str, Any]
    54	    confidence: Optional[float]
    55	    red_flags: bool
    56	    regulatory: bool


===== FILE: backend/app/services/langgraph/graph/supervisor_graph.py =====
     1	from __future__ import annotations
     2	
     3	import logging
     4	from functools import lru_cache
     5	from typing import TypedDict, List, Literal, Optional
     6	
     7	from langchain_openai import ChatOpenAI
     8	from langchain_core.messages import BaseMessage, AIMessage
     9	from langchain_core.tools import tool
    10	from langgraph.graph import StateGraph
    11	from langgraph.constants import END
    12	
    13	from app.services.langgraph.tools import long_term_memory as ltm
    14	from .intent_router import classify_intent
    15	from .consult.build import build_consult_graph
    16	from app.services.langgraph.llm_factory import get_llm
    17	
    18	log = logging.getLogger(__name__)
    19	
    20	@lru_cache(maxsize=1)
    21	def create_llm() -> ChatOpenAI:
    22	    """Create a ChatOpenAI instance (streaming enabled) from the central LLM factory."""
    23	    return get_llm(streaming=True)
    24	
    25	@tool
    26	def ltm_search(query: str) -> str:
    27	    """Durchsucht das Long-Term-Memory (Qdrant) nach relevanten Erinnerungen (MMR, top-k=5) und gibt einen zusammenhängenden Kontext-Text zurück."""
    28	    ctx, _hits = ltm.ltm_query(query, strategy="mmr", top_k=5)
    29	    return ctx or "Keine relevanten Erinnerungen gefunden."
    30	
    31	@tool
    32	def ltm_store(user: str, chat_id: str, text: str, kind: str = "note") -> str:
    33	    """Speichert einen Text-Schnipsel im Long-Term-Memory (Qdrant). Parameter: user, chat_id, text, kind."""
    34	    try:
    35	        pid = ltm.upsert_memory(user=user, chat_id=chat_id, text=text, kind=kind)
    36	        return f"Memory gespeichert (ID={pid})"
    37	    except Exception as e:
    38	        return f"Fehler beim Speichern: {e}"
    39	
    40	TOOLS = [ltm_search, ltm_store]
    41	
    42	class ChatState(TypedDict, total=False):
    43	    messages: List[BaseMessage]
    44	    intent: Literal["consult", "chitchat"]
    45	
    46	@lru_cache(maxsize=1)
    47	def _compiled_consult_graph():
    48	    return build_consult_graph().compile()
    49	
    50	def build_chat_builder(llm: Optional[ChatOpenAI] = None) -> StateGraph:
    51	    """Erstellt den Supervisor-Graphen: Router -> (Consult|Chitchat)."""
    52	    log.info("[supervisor] Initialisiere…")
    53	    builder = StateGraph(ChatState)
    54	
    55	    base_llm = llm or create_llm()
    56	    llm_chitchat = base_llm.bind_tools(TOOLS)
    57	    consult_graph = _compiled_consult_graph()
    58	
    59	    def router_node(state: ChatState) -> ChatState:
    60	        intent = classify_intent(base_llm, state.get("messages", []))
    61	        return {"intent": intent}
    62	
    63	    def chitchat_node(state: ChatState) -> ChatState:
    64	        history = state.get("messages", [])
    65	        result = llm_chitchat.invoke(history)
    66	        ai_msg = result if isinstance(result, AIMessage) else AIMessage(content=getattr(result, "content", str(result)))
    67	        return {"messages": [ai_msg]}
    68	
    69	    def consult_node(state: ChatState) -> ChatState:
    70	        result = consult_graph.invoke({"messages": state.get("messages", [])})
    71	        out_msgs = result.get("messages") or []
    72	        ai_txt = ""
    73	        for m in reversed(out_msgs):
    74	            if isinstance(m, AIMessage):
    75	                ai_txt = (m.content or "").strip()
    76	                break
    77	        if not ai_txt:
    78	            ai_txt = "Die Beratung wurde abgeschlossen."
    79	        return {"messages": [AIMessage(content=ai_txt)]}
    80	
    81	    builder.add_node("router", router_node)
    82	    builder.add_node("chitchat", chitchat_node)
    83	    builder.add_node("consult", consult_node)
    84	
    85	    builder.set_entry_point("router")
    86	
    87	    def decide(state: ChatState) -> str:
    88	        intent = state.get("intent") or "chitchat"
    89	        return "consult" if intent == "consult" else "chitchat"
    90	
    91	    builder.add_conditional_edges("router", decide, {"consult": "consult", "chitchat": "chitchat"})
    92	    builder.add_edge("consult", END)
    93	    builder.add_edge("chitchat", END)
    94	
    95	    log.info("[supervisor] Bereit.")
    96	    return builder


===== FILE: backend/app/services/langgraph/__init__.py =====


===== FILE: backend/app/services/langgraph/llm_factory.py =====
     1	from __future__ import annotations
     2	from typing import Any, Optional, Dict
     3	from langchain_openai import ChatOpenAI
     4	import os
     5	
     6	def get_llm(
     7	    model: Optional[str] = None,
     8	    *,
     9	    temperature: Optional[float] = None,
    10	    top_p: Optional[float] = None,
    11	    max_tokens: Optional[int] = None,
    12	    streaming: bool = False,
    13	    timeout: Optional[float] = None,
    14	    max_retries: Optional[int] = None,
    15	) -> Any:
    16	    mdl = model or os.getenv("OPENAI_MODEL", "gpt-4o-mini")
    17	    kw: Dict[str, Any] = {"model": mdl}
    18	
    19	    # temperature optional
    20	    if temperature is not None:
    21	        kw["temperature"] = temperature
    22	    else:
    23	        try:
    24	            kw["temperature"] = float(os.getenv("OPENAI_TEMPERATURE", "0.2"))
    25	        except Exception:
    26	            pass
    27	
    28	    # top_p nur wenn EXPLIZIT erlaubt
    29	    if top_p is not None and os.getenv("OPENAI_ENABLE_TOP_P", "0") == "1":
    30	        kw["top_p"] = top_p
    31	
    32	    # max_tokens optional
    33	    if max_tokens is not None:
    34	        kw["max_tokens"] = max_tokens
    35	    else:
    36	        mt_env = os.getenv("OPENAI_MAX_TOKENS", "")
    37	        if mt_env.isdigit():
    38	            kw["max_tokens"] = int(mt_env)
    39	
    40	    if timeout is not None:
    41	        kw["timeout"] = timeout
    42	
    43	    mr_env = os.getenv("OPENAI_MAX_RETRIES", "")
    44	    kw["max_retries"] = max_retries if max_retries is not None else (int(mr_env) if mr_env.isdigit() else 3)
    45	    kw["streaming"] = streaming
    46	    return ChatOpenAI(**kw)
    47	
    48	create_llm = get_llm


===== FILE: backend/app/services/langgraph/llm_router.py =====
     1	# backend/app/services/langgraph/llm_router.py
     2	from __future__ import annotations
     3	
     4	import os
     5	from functools import lru_cache
     6	from langchain_openai import ChatOpenAI
     7	
     8	def _mk_router_llm(model: str) -> ChatOpenAI:
     9	    return ChatOpenAI(
    10	        model=model,
    11	        api_key=os.getenv("OPENAI_API_KEY"),
    12	        base_url=os.getenv("OPENAI_BASE_URL") or None,
    13	        streaming=False,
    14	        max_retries=1,
    15	        timeout=5,
    16	    )
    17	
    18	@lru_cache(maxsize=1)
    19	def get_router_llm() -> ChatOpenAI:
    20	    model = os.getenv("OPENAI_INTENT_MODEL", "gpt-4.1-mini")
    21	    return _mk_router_llm(model)
    22	
    23	@lru_cache(maxsize=1)
    24	def get_router_fallback_llm() -> ChatOpenAI:
    25	    model = os.getenv("OPENAI_INTENT_FALLBACK_MODEL", "gpt-4o-mini")
    26	    return _mk_router_llm(model)


===== FILE: backend/app/services/langgraph/nodes/__init__.py =====


===== FILE: backend/app/services/langgraph/pdf/__init__.py =====


===== FILE: backend/app/services/langgraph/pdf/rfq_renderer.py =====
     1	from __future__ import annotations
     2	from typing import Dict, Any
     3	
     4	def _ensure_reportlab():
     5	    # Lazy import to avoid startup crashes if reportlab not yet installed
     6	    global canvas, A4, mm, simpleSplit
     7	    try:
     8	        from reportlab.lib.pagesizes import A4
     9	        from reportlab.pdfgen import canvas
    10	        from reportlab.lib.units import mm
    11	        from reportlab.lib.utils import simpleSplit
    12	    except Exception as e:
    13	        raise RuntimeError("reportlab missing: pip install reportlab>=4.2.0") from e
    14	
    15	def _draw_multiline(c, text: str, x: float, y: float, max_width: float, leading: float = 14):
    16	    lines = simpleSplit(text, "Helvetica", 10, max_width)
    17	    for line in lines:
    18	        c.drawString(x, y, line)
    19	        y -= leading
    20	    return y
    21	
    22	def generate_rfq_pdf(data: Dict[str, Any], out_path: str) -> None:
    23	    _ensure_reportlab()
    24	    c = canvas.Canvas(out_path, pagesize=A4)
    25	    width, height = A4
    26	    x0, y = 20*mm, height - 25*mm
    27	
    28	    c.setFont("Helvetica-Bold", 14)
    29	    c.drawString(x0, y, "RFQ – Request for Quotation")
    30	    c.setFont("Helvetica", 9)
    31	    c.drawString(x0, y-14, "SealAI – Dichtungstechnik Beratung")
    32	    y -= 30
    33	
    34	    # Eingabedaten
    35	    c.setFont("Helvetica-Bold", 11); c.drawString(x0, y, "Eingabedaten"); y -= 12
    36	    c.setFont("Helvetica", 10)
    37	    for k, v in (data.get("params") or {}).items():
    38	        c.drawString(x0, y, f"- {k}: {v}"); y -= 12
    39	
    40	    # Abgeleitete Kennwerte
    41	    y -= 8; c.setFont("Helvetica-Bold", 11); c.drawString(x0, y, "Abgeleitete Kennwerte"); y -= 12
    42	    c.setFont("Helvetica", 10)
    43	    for k, v in (data.get("derived") or {}).items():
    44	        c.drawString(x0, y, f"- {k}: {v}"); y -= 12
    45	
    46	    # Kandidaten
    47	    y -= 8; c.setFont("Helvetica-Bold", 11); c.drawString(x0, y, "Top-Partnerprodukte"); y -= 12
    48	    c.setFont("Helvetica", 10); c.drawString(x0, y, "(Preise/LZ/MOQ durch Hersteller)"); y -= 14
    49	    for cand in (data.get("candidates") or [])[:10]:
    50	        line = f"- {cand.get('title')} | {cand.get('vendor_id')} | Material {cand.get('material')} | Profil {cand.get('profile')}"
    51	        y = _draw_multiline(c, line, x0, y, width - 40*mm)
    52	        if y < 40*mm:
    53	            c.showPage(); y = height - 25*mm; c.setFont("Helvetica", 10)
    54	
    55	    # Quellen
    56	    y -= 8; c.setFont("Helvetica-Bold", 11); c.drawString(x0, y, "Quellen"); y -= 12
    57	    c.setFont("Helvetica", 9)
    58	    for src in (data.get("sources") or [])[:12]:
    59	        y = _draw_multiline(c, f"- {src}", x0, y, width - 40*mm, leading=12)
    60	        if y < 40*mm:
    61	            c.showPage(); y = height - 25*mm; c.setFont("Helvetica", 9)
    62	
    63	    # Rechtshinweis
    64	    y -= 8; c.setFont("Helvetica", 9)
    65	    leg = data.get("legal_notice") or "Verbindliche Eignungszusage obliegt dem Hersteller."
    66	    _draw_multiline(c, f"Rechtshinweis: {leg}", x0, y, width - 40*mm, leading=12)
    67	
    68	    c.showPage()
    69	    c.save()


===== FILE: backend/app/services/langgraph/policies/__init__.py =====


===== FILE: backend/app/services/langgraph/policies/model_routing.py =====
     1	from __future__ import annotations
     2	from dataclasses import dataclass
     3	from typing import Optional, Literal, Dict, Any
     4	import os
     5	
     6	ModelName = Literal["gpt-5-nano", "gpt-5-mini", "gpt-5"]
     7	
     8	@dataclass
     9	class RoutingContext:
    10	    node: str
    11	    confidence: Optional[float] = None
    12	    red_flags: bool = False
    13	    regulatory: bool = False
    14	    ambiguous: bool = False
    15	    hint: Optional[ModelName] = None
    16	
    17	DEFAULTS: Dict[str, ModelName] = {
    18	    "normalize_intent": "gpt-5-nano",
    19	    "pre_extract": "gpt-5-nano",
    20	    "domain_router": "gpt-5-nano",
    21	    "ask_missing": "gpt-5-nano",
    22	    "critic_light": "gpt-5-mini",
    23	    "explain": "gpt-5-mini",
    24	    "info_graph": "gpt-5-mini",
    25	    "market_graph": "gpt-5-nano",
    26	    "service_graph": "gpt-5-mini",
    27	}
    28	
    29	def select_model(ctx: RoutingContext) -> ModelName:
    30	    if ctx.hint:
    31	        return ctx.hint
    32	    if ctx.red_flags or ctx.regulatory:
    33	        return "gpt-5"
    34	    if ctx.confidence is not None:
    35	        if ctx.confidence < 0.70:
    36	            return "gpt-5"
    37	        if 0.70 <= ctx.confidence <= 0.84:
    38	            return "gpt-5-mini"
    39	        return "gpt-5-nano"
    40	    if ctx.ambiguous and ctx.node in ("domain_router", "info_graph"):
    41	        return "gpt-5-mini"
    42	    return DEFAULTS.get(ctx.node, "gpt-5-mini")
    43	
    44	def should_use_llm(node: str) -> bool:
    45	    deterministic = {
    46	        "intake_validate", "calc_core", "calc_advanced",
    47	        "rag_retrieve", "rules_filter",
    48	        "generate_rfq_pdf", "deliver_pdf"
    49	    }
    50	    return node not in deterministic
    51	
    52	def llm_params_for(node: str, ctx: RoutingContext) -> Dict[str, Any]:
    53	    model = select_model(ctx)
    54	    temperature = float(os.getenv("SEALAI_LLM_TEMPERATURE", "0.2"))
    55	    top_p = float(os.getenv("SEALAI_LLM_TOP_P", "0.9"))
    56	    max_tokens = {"gpt-5-nano": 512, "gpt-5-mini": 2048, "gpt-5": 4096}[model]
    57	    return {"model": model, "temperature": temperature, "top_p": top_p, "max_tokens": max_tokens}


===== FILE: backend/app/services/langgraph/postgres_lifespan.py =====
     1	# backend/app/services/langgraph/postgres_lifespan.py
     2	"""
     3	Kompatibler Postgres-Checkpointer (LangGraph).
     4	– Neuer Namespace: langgraph_checkpoint.postgres
     5	– Alter Namespace: langgraph.checkpoint.postgres
     6	– Fallback: AsyncRedisSaver oder InMemorySaver
     7	Zusatz: prewarm Long-Term-Memory (Qdrant) beim Start.
     8	"""
     9	from __future__ import annotations
    10	
    11	import atexit
    12	import logging
    13	from contextlib import asynccontextmanager
    14	from typing import AsyncGenerator
    15	
    16	from langgraph.checkpoint.memory import InMemorySaver
    17	
    18	log = logging.getLogger(__name__)
    19	
    20	from app.core.config import settings
    21	
    22	POSTGRES_URL = settings.POSTGRES_SYNC_URL
    23	
    24	# LTM prewarm
    25	try:
    26	    from app.services.langgraph.tools import long_term_memory as _ltm
    27	except Exception:  # pragma: no cover
    28	    _ltm = None
    29	
    30	# ─────────────────────────────────────────────────────────────────────────────
    31	# Kompatibler Import für PostgresSaver
    32	# ─────────────────────────────────────────────────────────────────────────────
    33	try:
    34	    from langgraph_checkpoint.postgres.aio import AsyncPostgresSaver as _PgSaver
    35	    log.info("AsyncPostgresSaver importiert aus langgraph_checkpoint.postgres.aio")
    36	except ModuleNotFoundError:
    37	    try:
    38	        from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver as _PgSaver
    39	        log.info("AsyncPostgresSaver importiert aus langgraph.checkpoint.postgres.aio")
    40	    except ModuleNotFoundError:
    41	        _PgSaver = None
    42	        log.warning("❗ Postgres-Modul nicht gefunden – Priorisiere RedisSaver")
    43	
    44	
    45	@asynccontextmanager
    46	async def get_checkpointer(app) -> AsyncGenerator:
    47	    """Universal-Initialisierung (async) + LTM-Prewarm."""
    48	    # Prewarm LTM (nicht-blockierend)
    49	    try:
    50	        if _ltm:
    51	            _ltm.prewarm_ltm()
    52	            log.info("LTM prewarm gestartet.")
    53	    except Exception as e:
    54	        log.warning("LTM prewarm fehlgeschlagen (ignoriert): %s", e)
    55	
    56	    checkpointer = None
    57	    if _PgSaver:
    58	        try:
    59	            async with _PgSaver.from_conn_string(POSTGRES_URL) as saver:
    60	                await saver.setup()
    61	                checkpointer = saver
    62	                log.info("✅ AsyncPostgresSaver initialisiert")
    63	                yield saver
    64	                return
    65	        except Exception as e:
    66	            log.warning("PostgresSaver-Init fehlgeschlagen: %s – Fallback auf RedisSaver", e)
    67	
    68	    # Redis-Fallback (primär für Memory)
    69	    try:
    70	        from langgraph.checkpoint.redis.aio import AsyncRedisSaver
    71	        from redis.asyncio import Redis
    72	        redis_url = settings.REDIS_URL or "redis://redis:6379/0"
    73	        redis_client = Redis.from_url(redis_url)
    74	        saver = AsyncRedisSaver(redis_client)
    75	        await saver.setup()
    76	        checkpointer = saver
    77	        log.info("✅ AsyncRedisSaver als Fallback initialisiert")
    78	        yield saver
    79	        return
    80	    except Exception as e:
    81	        log.warning("RedisSaver-Init fehlgeschlagen: %s – Ultimativer Fallback: InMemorySaver", e)
    82	
    83	    saver = InMemorySaver()
    84	    yield saver
    85	    log.info("InMemorySaver initialisiert – keine persistente LangGraph-History")
    86	
    87	
    88	async def get_saver():
    89	    async with get_checkpointer(None) as saver:
    90	        return saver
    91	
    92	
    93	@asynccontextmanager
    94	async def lifespan(app) -> AsyncGenerator[None, None]:
    95	    async with get_checkpointer(app):
    96	        yield
    97	
    98	
    99	def cleanup():
   100	    pass
   101	
   102	
   103	atexit.register(cleanup)


===== FILE: backend/app/services/langgraph/prompting.py =====
     1	# backend/app/services/langgraph/prompting.py
     2	from __future__ import annotations
     3	
     4	import json
     5	import logging
     6	import os
     7	import re
     8	from pathlib import Path
     9	from typing import Any, Iterable, List, Dict
    10	
    11	from jinja2 import Environment, FileSystemLoader, StrictUndefined
    12	from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
    13	
    14	log = logging.getLogger(__name__)
    15	
    16	# -------------------------------------------------------------------
    17	# Template-Verzeichnisse einsammeln (mit ENV-Override)
    18	# -------------------------------------------------------------------
    19	_BASE = Path(__file__).resolve().parent
    20	_GLOBAL_PROMPTS = _BASE / "prompts"
    21	_GLOBAL_PROMPT_TEMPLATES = _BASE / "prompt_templates"
    22	_GRAPH_CONSULT_PROMPTS = _BASE / "graph" / "consult" / "prompts"
    23	_DOMAINS_ROOT = _BASE / "domains"
    24	
    25	
    26	def _collect_template_dirs() -> List[Path]:
    27	    # Optional: zusätzliche Pfade per ENV (z. B. "/app/custom_prompts:/mnt/prompts")
    28	    env_paths: List[Path] = []
    29	    raw = os.getenv("SEALAI_TEMPLATE_DIRS", "").strip()
    30	    if raw:
    31	        for p in raw.split(":"):
    32	            pp = Path(p).resolve()
    33	            if pp.is_dir():
    34	                env_paths.append(pp)
    35	
    36	    fixed: List[Path] = [
    37	        _GLOBAL_PROMPTS,
    38	        _GLOBAL_PROMPT_TEMPLATES,
    39	        _GRAPH_CONSULT_PROMPTS,
    40	    ]
    41	
    42	    domain_prompts: List[Path] = []
    43	    if _DOMAINS_ROOT.is_dir():
    44	        for p in _DOMAINS_ROOT.glob("**/prompts"):
    45	            if p.is_dir():
    46	                domain_prompts.append(p)
    47	
    48	    all_candidates = env_paths + fixed + domain_prompts
    49	
    50	    seen = set()
    51	    result: List[Path] = []
    52	    for p in all_candidates:
    53	        try:
    54	            rp = p.resolve()
    55	        except Exception:
    56	            rp = p
    57	        if rp.is_dir():
    58	            key = str(rp)
    59	            if key not in seen:
    60	                seen.add(key)
    61	                result.append(rp)
    62	
    63	    if not result:
    64	        result = [_BASE]
    65	        log.warning("[prompting] Keine Template-Verzeichnisse gefunden; Fallback=%s", _BASE)
    66	
    67	    try:
    68	        log.info("[prompting] template search dirs: %s", ", ".join(str(p) for p in result))
    69	    except Exception:
    70	        pass
    71	
    72	    return result
    73	
    74	
    75	_ENV = Environment(
    76	    loader=FileSystemLoader([str(p) for p in _collect_template_dirs()]),
    77	    autoescape=False,
    78	    undefined=StrictUndefined,  # Fail-fast
    79	    trim_blocks=True,
    80	    lstrip_blocks=True,
    81	)
    82	
    83	# -------------------------------------------------------------------
    84	# Jinja2 Filter
    85	# -------------------------------------------------------------------
    86	def _regex_search(value: Any, pattern: str) -> bool:
    87	    try:
    88	        return re.search(pattern, str(value or ""), flags=re.I) is not None
    89	    except Exception:
    90	        return False
    91	
    92	
    93	def _tojson_compact(value: Any) -> str:
    94	    return json.dumps(value, ensure_ascii=False, separators=(",", ":"))
    95	
    96	
    97	def _tojson_pretty(value: Any) -> str:
    98	    return json.dumps(value, ensure_ascii=False, indent=2)
    99	
   100	
   101	_ENV.filters["regex_search"] = _regex_search
   102	_ENV.filters["tojson_compact"] = _tojson_compact
   103	_ENV.filters["tojson_pretty"] = _tojson_pretty
   104	
   105	# -------------------------------------------------------------------
   106	# Public API
   107	# -------------------------------------------------------------------
   108	def render_template(name: str, /, **kwargs: Any) -> str:
   109	    """Rendert ein Jinja2-Template und loggt die Quelle; fügt params_json automatisch hinzu."""
   110	    if "params" in kwargs and "params_json" not in kwargs:
   111	        try:
   112	            kwargs["params_json"] = safe_json(kwargs["params"])
   113	        except Exception:
   114	            kwargs["params_json"] = "{}"
   115	
   116	    tpl = _ENV.get_template(name)
   117	    src_file = getattr(tpl, "filename", None)
   118	    log.info("[prompting] loaded template '%s' from '%s'", name, src_file or "?")
   119	    return tpl.render(**kwargs)
   120	
   121	
   122	def messages_for_template(seq: Iterable[Any]) -> List[Dict[str, str]]:
   123	    """Normalisiert Nachrichten in [{type, content}]."""
   124	    out: List[Dict[str, str]] = []
   125	
   126	    def _norm_one(m: Any) -> Dict[str, str]:
   127	        if isinstance(m, HumanMessage):
   128	            return {"type": "user", "content": (m.content or "").strip()}
   129	        if isinstance(m, AIMessage):
   130	            return {"type": "ai", "content": (m.content or "").strip()}
   131	        if isinstance(m, SystemMessage):
   132	            return {"type": "system", "content": (m.content or "").strip()}
   133	
   134	        if isinstance(m, dict):
   135	            role = (m.get("role") or m.get("type") or "").lower()
   136	            content = (m.get("content") or "").strip()
   137	            if role in ("user", "human"):
   138	                t = "user"
   139	            elif role in ("assistant", "ai"):
   140	                t = "ai"
   141	            elif role == "system":
   142	                t = "system"
   143	            else:
   144	                t = "user"
   145	            return {"type": t, "content": content}
   146	
   147	        return {"type": "user", "content": (str(m) if m is not None else "").strip()}
   148	
   149	    for m in (seq or []):
   150	        norm = _norm_one(m)
   151	        if norm["content"]:
   152	            out.append(norm)
   153	    return out
   154	
   155	
   156	# -------------------------------------------------------------------
   157	# JSON-Utilities
   158	# -------------------------------------------------------------------
   159	_CODE_FENCE_RX = re.compile(r"^```(?:json|JSON)?\s*(.*?)\s*```$", re.DOTALL)
   160	
   161	
   162	def _extract_balanced_json(s: str) -> str:
   163	    """Extrahiert den ersten ausgewogenen JSON-Block ({...} oder [...]) aus s."""
   164	    if not s:
   165	        return ""
   166	    start_idx = None
   167	    opener = None
   168	    closer = None
   169	    for i, ch in enumerate(s):
   170	        if ch in "{[":
   171	            start_idx = i
   172	            opener = ch
   173	            closer = "}" if ch == "{" else "]"
   174	            break
   175	    if start_idx is None:
   176	        return s.strip()
   177	
   178	    depth = 0
   179	    in_string = False
   180	    escape = False
   181	    for j in range(start_idx, len(s)):
   182	        ch = s[j]
   183	        if in_string:
   184	            if escape:
   185	                escape = False
   186	            elif ch == "\\":
   187	                escape = True
   188	            elif ch == '"':
   189	                in_string = False
   190	        else:
   191	            if ch == '"':
   192	                in_string = True
   193	            elif ch == opener:
   194	                depth += 1
   195	            elif ch == closer:
   196	                depth -= 1
   197	                if depth == 0:
   198	                    return s[start_idx : j + 1].strip()
   199	    return s[start_idx:].strip()
   200	
   201	
   202	def strip_json_fence(text: str) -> str:
   203	    """Entfernt ```json fences``` ODER extrahiert den ersten ausgewogenen JSON-Block."""
   204	    if not isinstance(text, str):
   205	        return ""
   206	    s = text.strip()
   207	
   208	    m = _CODE_FENCE_RX.match(s)
   209	    if m:
   210	        inner = m.group(1).strip()
   211	        if inner.startswith("{") or inner.startswith("["):
   212	            return _extract_balanced_json(inner)
   213	        return inner
   214	
   215	    if s.startswith("{") or s.startswith("["):
   216	        return _extract_balanced_json(s)
   217	
   218	    return _extract_balanced_json(s)
   219	
   220	
   221	def safe_json(obj: Any) -> str:
   222	    """Kompaktes JSON (UTF-8) für Prompt-Übergaben."""
   223	    return json.dumps(obj or {}, ensure_ascii=False, separators=(",", ":"))


===== FILE: backend/app/services/langgraph/prompt_registry.py =====
     1	from __future__ import annotations
     2	import functools
     3	from pathlib import Path
     4	from typing import Dict, List
     5	import yaml
     6	
     7	_BASE = Path(__file__).resolve().parent / "prompts"
     8	
     9	@functools.lru_cache(maxsize=64)
    10	def _load_registry() -> Dict:
    11	    p = _BASE / "registry.yaml"
    12	    if not p.exists():
    13	        return {"agents": {}}
    14	    with p.open("r", encoding="utf-8") as f:
    15	        return yaml.safe_load(f) or {"agents": {}}
    16	
    17	def _read(path: Path) -> str:
    18	    try:
    19	        return path.read_text(encoding="utf-8").strip()
    20	    except Exception:
    21	        return ""
    22	
    23	@functools.lru_cache(maxsize=256)
    24	def get_agent_prompt(agent_id: str, lang: str = "de") -> str:
    25	    reg = _load_registry()
    26	    agent = (reg.get("agents") or {}).get(agent_id) or (reg.get("agents") or {}).get("supervisor")
    27	    if not agent:
    28	        return ""
    29	    files: List[str] = agent.get("files") or []
    30	    parts: List[str] = []
    31	    for rel in files:
    32	        p = _BASE / rel
    33	        if p.suffix.lower() in {".md", ".txt", ".jinja2"} and p.exists():
    34	            parts.append(_read(p))
    35	    return "\n\n".join(x for x in parts if x)


===== FILE: backend/app/services/langgraph/prompts/agents/consult_supervisor.de.md =====
     1	**Rolle:** Supervisor für Dichtungstechnik-Beratung.  
     2	**Aufgabe:** Nutzeranliegen triagieren, Domäne wählen (z. B. „rwdr“, „hydraulics_rod“), fehlende Pflichtparameter **menschlich** abfragen und dann an die Domäne übergeben.
     3	
     4	**Ton & Stil (kurz):**
     5	- freundlich, respektvoll, charmant, leicht locker – fachlich präzise
     6	- kurz bedanken, Eingaben knapp spiegeln, dann 1–3 **proaktive** Rückfragen
     7	- keine Floskeln, keine Romane
     8	
     9	**Vorgehen:**
    10	1) Anliegen erkennen, Domäne festlegen.
    11	2) Fehlende Kernwerte **kompakt** nachfragen (eine Zeile oder kurze Spiegelstriche).
    12	3) Nur das Nötigste erfragen (Priorität: richtige Empfehlung).
    13	4) Wenn alles da: an die Domäne übergeben.


===== FILE: backend/app/services/langgraph/prompts/agents/hyd_rod_agent.de.md =====
     1	**Domäne:** Hydraulik-Stange (Rod)
     2	**Pflichtfelder:** falltyp, stange_mm, nut_d_mm, nut_b_mm, medium, temp_max_c, druck_bar, geschwindigkeit_m_s
     3	**Beispiel Eingabezeile:** `Stange 25, Nut D 32, Nut B 6, Medium Öl, Tmax 80, Druck 160 bar, v 0,3 m/s`


===== FILE: backend/app/services/langgraph/prompts/agents/__init__.py =====


===== FILE: backend/app/services/langgraph/prompts/agents/rwdr_agent.de.md =====
     1	**Domäne:** RWDR (Radial-Wellendichtringe)
     2	**Pflichtfelder:** falltyp, wellen_mm, gehause_mm, breite_mm, medium, temp_max_c, druck_bar, drehzahl_u_min
     3	**Hinweise:**
     4	- Maße in mm, Druck in bar, Temperatur in °C, Drehzahl in U/min.
     5	- Beispiel Eingabezeile: `Welle 25, Gehäuse 47, Breite 7, Medium Öl, Tmax 80, Druck 2 bar, n 1500`


===== FILE: backend/app/services/langgraph/prompts/ask_missing_followups.jinja2 =====
     1	{# backend/app/services/langgraph/prompts/ask_missing_followups.jinja2 #}
     2	{% set de = (lang or 'de')[:2].lower() == 'de' %}
     3	{% if de %}
     4	Danke dir – das Bild wird richtig gut! Zwei kurze Punkte noch, damit die Empfehlung **wirklich sitzt**:
     5	{% for q in followups %}- {{ q }}
     6	{% endfor %}
     7	Wenn das passt, lege ich direkt los. Oder ergänze es fix **in einer Zeile** (z. B. „Tmax 80 °C, Medium Öl, n 1500“). 🙂
     8	{% else %}
     9	Thanks — this is shaping up nicely! Two quick checks so the recommendation is **spot-on**:
    10	{% for q in followups %}- {{ q }}
    11	{% endfor %}
    12	If that’s correct, I’ll proceed right away. Or add it briefly **in one line** (e.g. “Tmax 80 °C, medium oil, n 1500”). 🙂
    13	{% endif %}


===== FILE: backend/app/services/langgraph/prompts/ask_missing.jinja2 =====
     1	{# backend/app/services/langgraph/prompts/ask_missing.jinja2 #}
     2	{% set de = (lang or 'de')[:2].lower() == 'de' %}
     3	{% if de %}
     4	Prima – das hilft mir schon deutlich weiter und macht die Aufgabe viel klarer. 😊
     5	Für eine **präzise, punktgenaue Empfehlung** brauche ich noch kurz:
     6	
     7	{{ friendly }}
     8	
     9	Pack die Werte gern **in eine Zeile**, z. B.:
    10	`{{ example }}`
    11	{% else %}
    12	Great — that already helps a lot. 😊
    13	For a **precise, spot-on recommendation**, I still need:
    14	
    15	{{ friendly }}
    16	
    17	Best in **one line**, e.g.:
    18	`{{ example }}`
    19	{% endif %}


===== FILE: backend/app/services/langgraph/prompts/calc_agent.jinja2 =====
     1	{# Prompt für den Berechnungsagenten #}
     2	Du unterstützt bei einfachen Berechnungen oder Abschätzungen im Bereich Dichtungstechnik. Sei konservativ und nenne Formeln und Einheiten.
     3	Nutze einfache, nachvollziehbare Schritte (kein Tool‑Call nötig) für die folgende Anfrage:
     4	„{{ query }}“


===== FILE: backend/app/services/langgraph/prompts/consult_extract_params.jinja2 =====
     1	{# RWDR/Hydraulics parameter extractor — OUTPUT MUST BE RAW JSON (no code fences). #}
     2	SYSTEM:
     3	You extract ONLY explicitly stated parameters for sealing applications (RWDR or hydraulics rod).
     4	Return EXACTLY one JSON object and nothing else (no prose, no markdown, no code fences).
     5	If a value is NOT explicitly provided, set it to the string "unknown". Do not guess.
     6	
     7	Required keys (numbers without units):
     8	- falltyp, bauform, abmessung,
     9	- wellen_mm, gehause_mm, breite_mm,
    10	- stange_mm, nut_d_mm, nut_b_mm,
    11	- medium, temp_max_c, druck_bar, drehzahl_u_min, geschwindigkeit_m_s,
    12	- lang, domain
    13	
    14	Rules:
    15	- Dimensions like "40/50x10" or "40x50x10" → wellen_mm=40, gehause_mm=50, breite_mm=10.
    16	- Pressure like "2 bar" → druck_bar=2.
    17	- Keep numbers as numbers (dot as decimal separator).
    18	- If ambiguous/unspecified → "unknown".
    19	- Output MUST be a valid JSON object.
    20	
    21	INPUT_MESSAGES:
    22	{{ messages | tojson_pretty }}
    23	
    24	SEED_PARAMS:
    25	{{ (params_json | default('{}')) }}
    26	
    27	OUTPUT:
    28	{ }


===== FILE: backend/app/services/langgraph/prompts/domain_router.jinja2 =====
     1	{# SealAI: Domain-Router (einzeiliges JSON mit Confidence) #}
     2	Du bist ein strikter Klassifizierer. Ordne die **letzte Nutzeräußerung** exakt **einer** Domain zu.
     3	Erlaubte Domains (klein geschrieben): {{ enabled_domains|join(", ") }}.
     4	
     5	Gib **ausschließlich** ein **einzeiliges JSON** aus:
     6	{"domain":"<eine der erlaubten Domains>","confidence":0.00-1.00}
     7	
     8	Harte Regeln (wichtiger als alles andere):
     9	- **RWDR (radial shaft seal / Wellendichtring)**, wenn einer der Punkte zutrifft:
    10	  - Erwähnung einer **RWDR-Bauform**: "BA", "BASL", "B2", "B1", "BASL", "B1/B2", "BASL-DUO", "B2KB" usw.
    11	  - **Abmessungsmuster** wie **"25x47x7"** (drei Maße mit 'x') in Kombination mit Ring/Wellendichtring/RWDR/BA etc.
    12	  - Wörter wie **"RWDR"**, **"Radialwellendichtring"**, **"Wellendichtring"**, **"Simmerring"**.
    13	- **hydraulics_rod (Stangendichtung)**, wenn einer der Punkte zutrifft:
    14	  - Begriffe wie **"Stange"**, **"Stangendichtung"**, **"Kolbenstange"**, **"Hydraulikzylinder"**, **"U-Ring"**, **"T-Seal"**, **"PTFE-Kombiring"**.
    15	  - **Nutmaße**/Geometrie: **"Nut d"**, **"Nut D"**, **"Nutbreite"**, **"Nut-Ø"**, **"Nut b"**.
    16	- Bei **Mischsignalen** gilt:
    17	  - Enthält die Eingabe **BA/BASL/B1/B2 + dreiteiliges Maß (z. B. 25x47x7)** ⇒ **rwdr** mit hoher Confidence (≥0.85).
    18	  - Enthält die Eingabe **U-Ring/T-Seal/Stange/Nut-Maße** ⇒ **hydraulics_rod** mit hoher Confidence (≥0.85).
    19	
    20	Weitere Hinweise:
    21	- Falls Medium/Temperatur/Drehzahl genannt werden, sind diese **kein** Unterscheidungsmerkmal – die Domain bestimmt sich primär über Bauteil/Profil/Geometrie.
    22	- Sei konservativ: bei echter Unklarheit die wahrscheinlichste Domain, aber mit niedrigerer Confidence.
    23	
    24	Kontext:
    25	- Bereits erkannte Parameter: {{ params_json }}
    26	- Chat (neueste zuerst):
    27	{% for m in messages %}- {{ m.type|lower }}: {{ m.content }}{% endfor %}


===== FILE: backend/app/services/langgraph/prompts/explain.jinja2 =====
     1	{# explain.jinja2 — sauber formatierte Markdown-Ausgabe, robust gegen fehlende Felder #}
     2	
     3	{% set _main = main if (main is defined and main) else {} %}
     4	{% set _d    = derived if (derived is defined and derived) else {} %}
     5	{% set c     = _d.get('calculated', {}) %}
     6	{% set vorteile = _main.get('vorteile') or [] %}
     7	{% set einschraenkungen = _main.get('einschraenkungen') or [] %}
     8	{% set geeignet_fuer = _main.get('geeignet_fuer') or [] %}
     9	{% set altern = alternativen if (alternativen is defined and alternativen) else [] %}
    10	{% set tips   = hinweise if (hinweise is defined and hinweise) else [] %}
    11	
    12	**Sehr gern – hier ist meine fachliche Empfehlung für Ihren Anwendungsfall:**
    13	
    14	## Empfehlung
    15	
    16	**Typ:** {{ _main.get('typ', '–') }}
    17	{% if _main.get('werkstoff') %}
    18	**Werkstoff:** {{ _main.get('werkstoff') }}
    19	{% endif %}
    20	
    21	{% if _main.get('begruendung') %}
    22	**Begründung (kurz):** {{ _main.get('begruendung') }}
    23	{% endif %}
    24	
    25	{% if 'umfangsgeschwindigkeit_m_s' in c %}
    26	**Abgeleiteter Wert:** v = {{ "%.3f"|format(c['umfangsgeschwindigkeit_m_s']) }} m/s
    27	{% endif %}
    28	
    29	{% if vorteile %}
    30	**Warum diese Wahl überzeugt**
    31	{% for v in vorteile %}
    32	- {{ v }}
    33	{% endfor %}
    34	{% endif %}
    35	
    36	{% if einschraenkungen %}
    37	**Worauf wir achten sollten**
    38	{% for e in einschraenkungen %}
    39	- {{ e }}
    40	{% endfor %}
    41	{% endif %}
    42	
    43	{% if geeignet_fuer %}
    44	**Besonders geeignet für**
    45	{% for g in geeignet_fuer %}
    46	- {{ g }}
    47	{% endfor %}
    48	{% endif %}
    49	
    50	{% if altern %}
    51	**Sinnvolle Alternativen**
    52	{% for a in altern %}
    53	- {{ a.get('typ','–') }}{% if a.get('werkstoff') %} ({{ a.get('werkstoff') }}){% endif %}{% if a.get('kurzbegruendung') %}: {{ a.get('kurzbegruendung') }}{% endif %}
    54	{% endfor %}
    55	{% endif %}
    56	
    57	{% if tips %}
    58	**Hinweise aus der Praxis**
    59	{% for h in tips %}
    60	- {{ h }}
    61	{% endfor %}
    62	{% endif %}
    63	
    64	---
    65	
    66	*Passt das für Sie? Wenn Medium, Temperaturfenster oder Drehzahl variieren, justiere ich die Empfehlung gern – zielgerichtet, sicher und langlebig.*


===== FILE: backend/app/services/langgraph/prompts/__init__.py =====


===== FILE: backend/app/services/langgraph/prompts/intake_triage.jinja2 =====
     1	{# Intake/Triage für SealAI – konsistente Keys mit dem Graph:
     2	   temp_max_c, drehzahl_u_min (NICHT tmax_c/drehzahl_rpm)
     3	   Liefert kompaktes JSON in EINER Zeile.
     4	#}
     5	
     6	# Rolle
     7	Du bist die Intake-Triage für Dichtungstechnik (z. B. RWDR, Hydraulik Stange). 
     8	Du extrahierst Kernparameter aus der letzten Nutzereingabe (und ggf. dem Dialog) 
     9	und gibst **nur** JSON in **einer Zeile** zurück – ohne Erklärungen/Markdown.
    10	
    11	# Dialog (neueste zuletzt)
    12	{% for m in messages %}- {{ m.type|lower }}: {{ m.content }}
    13	{% endfor %}
    14	
    15	# Bereits bekannte Parameter (nur ergänzen/überschreiben, wenn eindeutig genannt)
    16	{{ params_json }}
    17	
    18	# Zu extrahierende Kernparameter (nur setzen, wenn sicher):
    19	# - wellen_mm (mm)
    20	# - gehause_mm (mm)
    21	# - breite_mm (mm)
    22	# - medium (String)
    23	# - temp_max_c (°C)
    24	# - druck_bar (bar)
    25	# - drehzahl_u_min (U/min)
    26	
    27	# Regeln
    28	# - Zahlen: Tausenderpunkte/Leerzeichen entfernen; Komma als Dezimalpunkt interpretieren.
    29	# - drehzahl_u_min erkennt "U/min", "rpm".
    30	# - temp_max_c erkennt Schreibweisen wie "Tmax 80", "80 °C", "Temperatur max 80".
    31	# - Nichts erfinden. Wenn unklar → Feld weglassen.
    32	# - "intent" immer "consult" (die Detail-Orchestrierung übernimmt der Graph).
    33	# - "missing": Liste aller **oben genannten** Keys, die im Ergebnis fehlen.
    34	# - Antwort **ausschließlich** als kompaktes JSON in **einer** Zeile.
    35	
    36	{
    37	  "intent": "consult",
    38	  "params": {
    39	    "wellen_mm": null,
    40	    "gehause_mm": null,
    41	    "breite_mm": null,
    42	    "medium": null,
    43	    "temp_max_c": null,
    44	    "druck_bar": null,
    45	    "drehzahl_u_min": null
    46	  },
    47	  "missing": [],
    48	  "confidence": 0.8
    49	}


===== FILE: backend/app/services/langgraph/prompts/intent_router.jinja2 =====
     1	{# backend/app/services/langgraph/prompts/intent_router.jinja2 #}
     2	# Rolle
     3	Du bist ein Intent-Router für Dichtungstechnik (Radialwellendichtringe).
     4	
     5	# Eingabe
     6	{{ input_text|default('', true)|trim }}
     7	
     8	# Aufgabe
     9	Bestimme die Absicht und gib ausschließlich ein JSON-Objekt zurück.
    10	
    11	# Erlaubte Intents und Standard-Routing
    12	- "smalltalk"     → route "smalltalk"
    13	- "ask_missing"   → route "ask_missing"
    14	- "material"      → route "material"
    15	- "profil"        → route "profil"
    16	- "recommend"     → route "recommend"
    17	- "explain"       → route "explain"
    18	- "routing_error" → route "ask_missing"
    19	
    20	# Kernparameter, wenn erkennbar (sonst weglassen oder null):
    21	# welle_mm, gehaeuse_mm, breite_mm, medium, tmax_c, druck_bar, drehzahl_rpm
    22	
    23	# Regeln
    24	# - Antworte NUR mit JSON. Keine Erklärungen.
    25	# - Fülle immer "intent" und "route" aus.
    26	# - "missing": Liste fehlender Kernparameter aus
    27	#   ["welle_mm","gehaeuse_mm","breite_mm","medium","tmax_c","druck_bar","drehzahl_rpm"].
    28	# - "confidence": Zahl 0..1.
    29	
    30	{
    31	  "intent": "<smalltalk|ask_missing|material|profil|recommend|explain|routing_error>",
    32	  "route": "<smalltalk|ask_missing|material|profil|recommend|explain>",
    33	  "params": {
    34	    "welle_mm": null,
    35	    "gehaeuse_mm": null,
    36	    "breite_mm": null,
    37	    "medium": null,
    38	    "tmax_c": null,
    39	    "druck_bar": null,
    40	    "drehzahl_rpm": null
    41	  },
    42	  "missing": [],
    43	  "confidence": 0.8
    44	}


===== FILE: backend/app/services/langgraph/prompts/material_agent.jinja2 =====
     1	{# Prompt für den Materialagenten #}
     2	Du bist ein Werkstoffberater für Dichtungstechnik. Nutze, wenn sinnvoll, bisherige Gesprächsangaben aus der History.
     3	Nenne eine knappe Empfehlung zum Material (mit Annahmen) für die folgende Anfrage:
     4	„{{ query }}“


===== FILE: backend/app/services/langgraph/prompts/partials/__init__.py =====


===== FILE: backend/app/services/langgraph/prompts/partials/safety.de.md =====
     1	Sicherheit & Compliance:
     2	- Keine vertraulichen Daten erfragen oder speichern, außer der Nutzer liefert sie aktiv.
     3	- Bei Unsicherheiten: **Nachfragen** statt raten.
     4	- Keine technischen Empfehlungen ohne erforderliche Randbedingungen; weise ggf. auf Annahmen hin.


===== FILE: backend/app/services/langgraph/prompts/partials/tone.de.md =====
     1	Du sprichst **menschlich, respektvoll, charmant und leicht locker** – aber fachlich präzise.
     2	- Kurzer, wertschätzender Opener („Prima, das hilft mir schon weiter.“ / „Danke dir!“).
     3	- **Proaktiv** 1–3 Rückfragen stellen, die direkt zur Empfehlung führen.
     4	- **Kurz & klar**, keine Floskeln; gern ein leichtes Emoji (😊/🙂) wo passend.
     5	- **Immer** ein Einzeilen-Beispiel für die Eingabe anbieten („z. B.: Welle 25, Gehäuse 47, …“).


===== FILE: backend/app/services/langgraph/prompts/profile_agent.jinja2 =====
     1	{# Prompt für den Profil‑/Geometrie‑Agenten #}
     2	Du berätst zur Profil‑ und Dichtungsgeometrie (O‑Ring, X‑Ring, U‑Profil etc.). Beziehe bekannte Nutzervorgaben aus der History ein.
     3	Empfehle knapp ein Profil (mit Annahmen) für die folgende Anfrage:
     4	„{{ query }}“


===== FILE: backend/app/services/langgraph/prompts/recommend.jinja2 =====
     1	{# Empfehlungen – striktes JSON nur hier #}
     2	Du bist ein industrieller Dichtungsberater.
     3	ANTWORTE AUSSCHLIESSLICH mit VALIDEM JSON in EINER Zeile (keine Backticks, kein Markdown, keine Zeilenumbrüche).
     4	
     5	Kontext:
     6	- Domain: {{ domain }}
     7	- Parameter: {{ params | tojson_compact }}
     8	- Abgeleitete Werte/Flags: {{ derived | tojson_compact }}
     9	- Technischer Kontext (RAG/LTM): {{ context | default("", true) }}
    10	- Letzte Nutzereingabe: {{ recent_user | default("", true) }}
    11	
    12	Ausgabeformat (EXAKT so, nur Inhalte anpassen):
    13	{"empfehlungen":[{"typ":"<Typ>","werkstoff":"<Werkstoff>","begruendung":"<kurz>","vorteile":["..."],"einschraenkungen":["..."],"geeignet_fuer":["..."]}]}
    14	
    15	Regeln:
    16	- Gib 1–3 Elemente in `empfehlungen` aus.
    17	- Domain-Grenzen strikt:
    18	  - Wenn Domain "rwdr": `typ` MUSS wie "BA 45x62x7" o. ä. sein; KEINE Stangendichtungen.
    19	  - Wenn Domain "hydraulics_rod": `typ` MUSS Stangendichtung sein; KEIN RWDR.
    20	- Nutze den Kontext (z. B. Marken wie "Kyrolon 79X") in `begruendung` und verweise kurz auf Quellen (z. B. "siehe PTFE.docx"), sofern vorhanden.
    21	- Keine Freitexte außerhalb des JSON; keine Zeilenumbrüche innerhalb der JSON-Zeile.


===== FILE: backend/app/services/langgraph/prompts/registry.yaml =====
     1	agents:
     2	  supervisor:
     3	    lang: de
     4	    files:
     5	      - partials/tone.de.md
     6	      - partials/safety.de.md
     7	      - agents/consult_supervisor.de.md
     8	
     9	  rwdr:
    10	    lang: de
    11	    files:
    12	      - partials/tone.de.md
    13	      - agents/rwdr_agent.de.md
    14	
    15	  hydraulics_rod:
    16	    lang: de
    17	    files:
    18	      - partials/tone.de.md
    19	      - agents/hyd_rod_agent.de.md


===== FILE: backend/app/services/langgraph/prompts/report_agent.jinja2 =====
     1	{# Prompt für den Berichtagenten #}
     2	Du erstellst eine kurze, strukturierte Beratungs­zusammenfassung als Stichpunkte (max. 10 Zeilen).
     3	Erzeuge eine kompakte Zusammenfassung bzw. einen Report‑Entwurf für die folgende Anfrage:
     4	„{{ query }}“


===== FILE: backend/app/services/langgraph/prompt_templates/explain.jinja2 =====
     1	{# explain.jinja2 — sauber formatierte Markdown-Ausgabe, robust gegen fehlende Felder #}
     2	
     3	{% set _main = main if (main is defined and main) else {} %}
     4	{% set _d    = derived if (derived is defined and derived) else {} %}
     5	{% set c     = _d.get('calculated', {}) %}
     6	{% set vorteile = _main.get('vorteile') or [] %}
     7	{% set einschraenkungen = _main.get('einschraenkungen') or [] %}
     8	{% set geeignet_fuer = _main.get('geeignet_fuer') or [] %}
     9	{% set altern = alternativen if (alternativen is defined and alternativen) else [] %}
    10	{% set tips   = hinweise if (hinweise is defined and hinweise) else [] %}
    11	
    12	**Sehr gern – hier ist meine fachliche Empfehlung für Ihren Anwendungsfall:**
    13	
    14	## Empfehlung
    15	
    16	**Typ:** {{ _main.get('typ', '–') }}
    17	{% if _main.get('werkstoff') %}
    18	**Werkstoff:** {{ _main.get('werkstoff') }}
    19	{% endif %}
    20	
    21	{% if _main.get('begruendung') %}
    22	**Begründung (kurz):** {{ _main.get('begruendung') }}
    23	{% endif %}
    24	
    25	{% if 'umfangsgeschwindigkeit_m_s' in c %}
    26	**Abgeleiteter Wert:** v = {{ "%.3f"|format(c['umfangsgeschwindigkeit_m_s']) }} m/s
    27	{% endif %}
    28	
    29	{% if vorteile %}
    30	**Warum diese Wahl überzeugt**
    31	{% for v in vorteile %}
    32	- {{ v }}
    33	{% endfor %}
    34	{% endif %}
    35	
    36	{% if einschraenkungen %}
    37	**Worauf wir achten sollten**
    38	{% for e in einschraenkungen %}
    39	- {{ e }}
    40	{% endfor %}
    41	{% endif %}
    42	
    43	{% if geeignet_fuer %}
    44	**Besonders geeignet für**
    45	{% for g in geeignet_fuer %}
    46	- {{ g }}
    47	{% endfor %}
    48	{% endif %}
    49	
    50	{% if altern %}
    51	**Sinnvolle Alternativen**
    52	{% for a in altern %}
    53	- {{ a.get('typ','–') }}{% if a.get('werkstoff') %} ({{ a.get('werkstoff') }}){% endif %}{% if a.get('kurzbegruendung') %}: {{ a.get('kurzbegruendung') }}{% endif %}
    54	{% endfor %}
    55	{% endif %}
    56	
    57	{% if tips %}
    58	**Hinweise aus der Praxis**
    59	{% for h in tips %}
    60	- {{ h }}
    61	{% endfor %}
    62	{% endif %}
    63	
    64	---
    65	
    66	*Passt das für Sie? Wenn Medium, Temperaturfenster oder Drehzahl variieren, justiere ich die Empfehlung gern – zielgerichtet, sicher und langlebig.*


===== FILE: backend/app/services/langgraph/prompt_templates/global_system.jinja2 =====
     1	{# SealAI – Globaler Systemprompt #}
     2	Rolle: Du bist ein präziser, sachlicher KI-Berater für Industrie & Dichtungstechnik
     3	
     4	Ziel: Liefere korrekte, nachvollziehbare Antworten. Erfrage fehlende Parameter immer höflich und respektvoll.
     5	Bevorzuge deutsche Sprache ("de-DE"), außer der Nutzer verlangt anderes.
     6	
     7	Arbeitsprinzipien:
     8	- Keine Halluzinationen. Unklare Fakten → als unsicher kennzeichnen oder Rückfrage stellen.
     9	- Einheiten immer nennen. SI bevorzugen. Annahmen explizit markieren.
    10	- Schrittweise denken, aber nur Endergebnis ausgeben, außer der Nutzer fordert den Rechenweg.
    11	- Bei Listen: kurz, nummeriert. Bei Prozessen: Input → Schritte → Output.
    12	- Wenn Domänen-Agent aktiv ist, dessen Domänen-Regeln strikt befolgen.
    13	- Sicherheit: keine Anleitungen zu Schadaktivitäten, kein personenbezogenes Profiling.
    14	
    15	Formatregeln:
    16	- Antworte präzise und respektvoll. Keine Floskeln. Keine Ausrufezeichen.
    17	- Code immer vollständig und lauffähig, inkl. Pfad.
    18	- Tabellen wenn sinnvoll.
    19	
    20	Kontextvariablen:
    21	- Unternehmen: {{ company_name | default("SealAI") }}
    22	- Domäne: {{ domain | default("Sealing Technology") }}
    23	- Sprache: {{ language | default("de") }}
    24	- Datum: {{ today | default("{{DATE}}") }}
    25	
    26	Wenn Informationen fehlen:
    27	- Stelle 1–3 gezielte Rückfragen mit maximaler Hebelwirkung.
    28	
    29	Wenn RAG/Quellen genutzt:
    30	- Zitiere Quellen knapp am Ende.


===== FILE: backend/app/services/langgraph/prompt_templates/__init__.py =====


===== FILE: backend/app/services/langgraph/prompt_templates/supervisor_prompt.jinja2 =====
     1	{# =====================================================================
     2	  SealAI – Supervisor Prompt (Lite, RAG-First, PROD)
     3	  Zweck: Intent + Pflichtparameter + deterministische RAG-Entscheidung
     4	         + kompakte RAG-Query + Minimal-Plan.
     5	  Ausgabe: STRICTEINZEILIGES JSON (keine Backticks/Markdown).
     6	  Optional-Variablen: user_text, thread_memory, user_profile, rag_context,
     7	                      params_json, domain, locale
     8	===================================================================== #}
     9	
    10	Du bist der **Supervisor** von *SealAI – Sealing Intelligence*.
    11	Antworte **ausschließlich** mit einem **validen JSON-Objekt in EINER Zeile**.
    12	
    13	# Eingaben (falls gesetzt):
    14	- user_text: {{ (user_text or "") | trim }}
    15	- domain: {{ (domain or "auto") | trim }}
    16	- params: {{ params_json | default("{}", true) }}
    17	- memory: {{ (thread_memory or "") | trim }}
    18	- user_profile: {{ (user_profile or "") | trim }}
    19	- rag_context (Top-Dokumente, optional): {{ (rag_context or "") | trim }}
    20	
    21	# Ziele
    22	1) Intent bestimmen (z. B. smalltalk, materialvergleich, profilwahl, normencheck, produktrecherche, fehlerdiagnose, einkauf).
    23	2) Fehlende **Pflichtparameter** erkennen (Medium, Temperatur [°C], Druck [bar], Abmessungen, Bewegung/Drehzahl).
    24	3) **Deterministische RAG-Policy** setzen:
    25	   - `should_query_rag = true`, **außer** Intent ist klar `smalltalk`/`greeting`/`thanks`.
    26	   - Insbesondere **true**, wenn einer der folgenden Hinweise vorliegt:
    27	     • Marken/Materiale: PTFE, FKM/Viton, NBR, EPDM, PU, PEEK, Graphit, Kyrolon 79X, etc.
    28	     • Produkt-/Domänenterme: RWDR, BA/BASL/B1/B2, U-Ring, T-Seal, Gleitring, DIN/ISO/FDA.
    29	     • Mess-/Betriebsdaten: Muster wie `\d+x\d+x\d+`, °C, bar, rpm/U/min, m/s, μm.
    30	4) Kompakte **rag_query** erzeugen (keine Prosa):
    31	   - Schema: `[Domain|RWDR/Hydraulik] + Maße + Medium + Tmax °C + Druck bar + rpm/m/s + markante Keywords/Marken`.
    32	   - Beispiel: `RWDR 45x62x7, Öl, Tmax 90°C, 1 bar, 1500 U/min, Kyrolon 79X`.
    33	5) **Plan** minimal halten:
    34	   - Wenn `should_query_rag=true`: Schritt 1=rag; danach nur nötige Agenten (`material`, `normen`, `konstruktion`, `produkt`, `safety`) mit kurzem `goal`.
    35	   - Wenn Pflichtparameter fehlen: Plan zuerst auf **Rückfragen** (ask_missing) ausrichten.
    36	6) **Kein Freitext** außerhalb des JSON.
    37	
    38	# Ausgabe-Schema (eine Zeile, keine Zeilenumbrüche):
    39	{"intent":"<string>","missing_params":[{"name":"<string>","why":"<kurz>","example":"<konkret>","priority":1}],"should_query_rag":<true|false>,"rag_query":"<kompakt oder leer>","plan":[{"step":1,"agent":"<rag|material|normen|konstruktion|produkt|markt|safety|ask_missing>","goal":"<kurz>","parallel_group":"<A|B|null>"}]}
    40	
    41	# Hinweise
    42	- `missing_params` nur die wirklich fehlenden Essentials (z. B. Medium, Tmax, Druck, Maße, Drehzahl/Bewegung).
    43	- `rag_query` **leer**, wenn `should_query_rag=false`.
    44	- `parallel_group` nur setzen, wenn Schritte unabhängig sind; sonst "null".
    45	- Antworte **in EINER JSON-Zeile**. Keine zusätzlichen Felder, keine Erklärungen.
    46	
    47	# Mini-Beispiele
    48	
    49	## A) Fachfrage mit Marke
    50	INPUT: "RWDR 45x62x7, Öl, 90 °C, 1 bar, 1500 U/min. Kyrolon 79X?"
    51	OUTPUT:
    52	{"intent":"materialvergleich","missing_params":[],"should_query_rag":true,"rag_query":"RWDR 45x62x7, Öl, Tmax 90°C, 1 bar, 1500 U/min, Kyrolon 79X","plan":[{"step":1,"agent":"rag","goal":"Kontext+Quellen abrufen","parallel_group":"null"},{"step":2,"agent":"material","goal":"Werkstoffwahl/Temp-Grenzen","parallel_group":"A"},{"step":3,"agent":"normen","goal":"Grenzen/Regeln prüfen","parallel_group":"A"}]}
    53	
    54	## B) Unklar, Pflichtdaten fehlen
    55	INPUT: "Brauche Dichtung für Hydraulikzylinder."
    56	OUTPUT:
    57	{"intent":"profilwahl","missing_params":[{"name":"Abmessungen","why":"Profil/Passung abhängig von Maßen","example":"z. B. 40×45×6 mm","priority":1},{"name":"Medium","why":"Werkstoffauswahl","example":"HLP46","priority":1},{"name":"Temperatur","why":"Grenzwerte","example":"80 °C","priority":2},{"name":"Druck","why":"Extrusionssicherheit","example":"200 bar","priority":2}],"should_query_rag":true,"rag_query":"Hydraulik Stangendichtung, Medium ?, Tmax ?, Druck ?, Maße ?","plan":[{"step":1,"agent":"ask_missing","goal":"Pflichtparameter erfragen","parallel_group":"null"},{"step":2,"agent":"rag","goal":"Norm-/Profilhinweise abrufen","parallel_group":"null"}]}
    58	
    59	## C) Smalltalk
    60	INPUT: "Danke dir!"
    61	OUTPUT:
    62	{"intent":"smalltalk","missing_params":[],"should_query_rag":false,"rag_query":"","plan":[{"step":1,"agent":"ask_missing","goal":"Kurz antworten/abschließen","parallel_group":"null"}]}


===== FILE: backend/app/services/langgraph/rag/example_products.jsonl =====
     1	{"doc_id":"rwdr-0001","vendor_id":"VEND_A","title":"RWDR TC 16x22x4 FKM","profile":"TC","material":"FKM","paid_tier":"gold","contract_valid_until":"2026-12-31","active":true,"score":0.92,"url":"https://partner-a.example/rwdr-tc-16x22x4"}
     2	{"doc_id":"rwdr-0002","vendor_id":"VEND_B","title":"RWDR TB 16x22x4 NBR","profile":"TB","material":"NBR","paid_tier":"silver","contract_valid_until":"2026-03-31","active":true,"score":0.88,"url":"https://partner-b.example/rwdr-tb-16x22x4"}
     3	{"doc_id":"rwdr-0004","vendor_id":"VEND_C","title":"RWDR PTFE 16x22x4","profile":"Custom","material":"PTFE","paid_tier":"basic","contract_valid_until":"2025-12-31","active":true,"score":0.86,"url":"https://partner-c.example/rwdr-ptfe-16x22x4"}


===== FILE: backend/app/services/langgraph/rag/__init__.py =====


===== FILE: backend/app/services/langgraph/rag/schemas.py =====
     1	from __future__ import annotations
     2	from dataclasses import dataclass
     3	from datetime import date
     4	from typing import Optional
     5	
     6	@dataclass
     7	class VendorMeta:
     8	    vendor_id: str
     9	    paid_tier: str
    10	    contract_valid_until: date
    11	    active: bool
    12	
    13	    def is_partner(self, today: Optional[date] = None) -> bool:
    14	        t = today or date.today()
    15	        return self.paid_tier != "none" and self.active and self.contract_valid_until >= t


===== FILE: backend/app/services/langgraph/redis_lifespan.py =====
     1	# backend/app/services/langgraph/redis_lifespan.py
     2	from __future__ import annotations
     3	
     4	import os
     5	import logging
     6	from typing import Optional, Any
     7	
     8	log = logging.getLogger("app.redis_checkpointer")
     9	
    10	# RedisSaver (sync). Unterschiedliche Versionen haben verschiedene __init__-Signaturen.
    11	try:
    12	    from langgraph.checkpoint.redis import RedisSaver  # type: ignore
    13	except Exception as e:  # pragma: no cover
    14	    RedisSaver = None  # type: ignore
    15	    log.warning("LangGraph RedisSaver nicht importierbar: %s", e)
    16	
    17	
    18	def _redis_url() -> str:
    19	    """
    20	    Liefert eine normierte REDIS_URL (inkl. DB-Index), Default: redis://redis:6379/0
    21	    """
    22	    url = (os.getenv("REDIS_URL") or "redis://redis:6379/0").strip()
    23	    return url or "redis://redis:6379/0"
    24	
    25	
    26	def _namespace() -> tuple[str, Optional[int]]:
    27	    """
    28	    Erzeugt einen einheitlichen Namespace/Key-Präfix (kompatibel zu namespace|key_prefix).
    29	    TTL optional.
    30	    """
    31	    raw_ns = (os.getenv("LANGGRAPH_CHECKPOINT_NS") or os.getenv("CHECKPOINT_NS") or "chat.supervisor.v1").strip()
    32	    prefix = (os.getenv("LANGGRAPH_CHECKPOINT_PREFIX") or "lg:cp").strip()
    33	    ttl_env = (os.getenv("LANGGRAPH_CHECKPOINT_TTL") or "").strip()
    34	    ttl = int(ttl_env) if ttl_env.isdigit() else None
    35	    ns = f"{prefix}:{raw_ns}"
    36	    return ns, ttl
    37	
    38	
    39	def _try_construct_redis_saver(redis_url: str, ns: str, ttl: Optional[int]) -> Any:
    40	    """
    41	    Probiert mehrere Konstruktor-Varianten – kompatibel zu alten/neuen Paketen.
    42	    Gibt den ersten erfolgreichen Saver zurück, sonst Exception.
    43	    """
    44	    errors: list[tuple[dict, Exception]] = []
    45	
    46	    # 1) Neuere Pakete – URL-basiert
    47	    for kwargs in (
    48	        {"redis_url": redis_url, "namespace": ns, "ttl_seconds": ttl},
    49	        {"redis_url": redis_url, "key_prefix": ns, "ttl_seconds": ttl},
    50	        {"redis_url": redis_url, "ttl_seconds": ttl},
    51	        {"redis_url": redis_url},
    52	    ):
    53	        try:
    54	            return RedisSaver(**{k: v for k, v in kwargs.items() if v is not None})  # type: ignore[misc]
    55	        except Exception as e:
    56	            errors.append((kwargs, e))
    57	
    58	    # 2) Ältere Pakete – Client-basiert
    59	    try:
    60	        from redis import Redis as _Redis
    61	        client = _Redis.from_url(redis_url)
    62	        for kwargs in (
    63	            {"redis": client, "namespace": ns, "ttl_seconds": ttl},
    64	            {"redis": client, "key_prefix": ns, "ttl_seconds": ttl},
    65	            {"redis": client, "ttl_seconds": ttl},
    66	            {"redis": client},
    67	        ):
    68	            try:
    69	                return RedisSaver(**{k: v for k, v in kwargs.items() if v is not None})  # type: ignore[misc]
    70	            except Exception as e:
    71	                errors.append((kwargs, e))
    72	    except Exception as e:
    73	        errors.append(({"redis_client_build": True}, e))
    74	
    75	    # Letzte Fehlermeldung ausgeben und erneut werfen
    76	    if errors:
    77	        kw, err = errors[-1]
    78	        log.warning("RedisSaver-Konstruktion fehlgeschlagen. Letzter Versuch %s: %r", kw, err)
    79	        raise err
    80	    raise RuntimeError("Unbekannter Fehler bei RedisSaver-Konstruktion")
    81	
    82	
    83	def get_redis_checkpointer(app=None) -> Optional["RedisSaver"]:
    84	    """
    85	    Erzeugt einen LangGraph-RedisSaver (Checkpointer).
    86	    Gibt None zurück, wenn Paket fehlt oder Konstruktion scheitert.
    87	    ENV:
    88	      REDIS_URL, LANGGRAPH_CHECKPOINT_NS | CHECKPOINT_NS,
    89	      LANGGRAPH_CHECKPOINT_PREFIX, LANGGRAPH_CHECKPOINT_TTL
    90	    """
    91	    if RedisSaver is None:
    92	        log.warning("LangGraph RedisSaver nicht verfügbar. Installiere 'langgraph-checkpoint-redis'.")
    93	        return None
    94	
    95	    redis_url = _redis_url()
    96	    ns, ttl = _namespace()
    97	
    98	    try:
    99	        saver = _try_construct_redis_saver(redis_url, ns, ttl)
   100	        log.info("LangGraph Checkpointer aktiv: url=%s ns/prefix=%s ttl=%s", redis_url, ns, ttl)
   101	        return saver
   102	    except Exception as e:
   103	        log.warning("RedisSaver nicht nutzbar (%r). Fallback: None (In-Memory-Graph).", e)
   104	        return None


===== FILE: backend/app/services/langgraph/rules/common.yaml =====
     1	version: 1
     2	materials:
     3	  water:
     4	    max_temp_c: 120
     5	    notes: ["Korrosionsschutz beachten", "Trinkwasser: WRAS/KTW beachten"]
     6	  oil:
     7	    max_temp_c: 150
     8	    notes: ["Additive beachten", "Viskosität beeinflusst Reibung"]
     9	profiles:
    10	  rwdr:
    11	    dn_limit: 300000
    12	    v_limit_m_s: 20
    13	pressure:
    14	  high_bar_threshold: 5
    15	  notes_above: ["Stützelement/Extrusionsschutz prüfen"]


===== FILE: backend/app/services/langgraph/rules/__init__.py =====


===== FILE: backend/app/services/langgraph/rules/rwdr.yaml =====
     1	version: 1
     2	medium_map:
     3	  water: ["EPDM", "FKM", "PTFE"]
     4	  oil: ["NBR", "FKM", "HNBR", "PTFE"]
     5	pv_bands:
     6	  - max_pv: 1.5
     7	    recommend: ["NBR", "EPDM"]
     8	  - max_pv: 3.0
     9	    recommend: ["HNBR", "FKM"]
    10	  - max_pv: 10.0
    11	    recommend: ["PTFE"]
    12	speed_bands_m_s:
    13	  - max_v: 5.0
    14	    recommend: ["NBR", "HNBR"]
    15	  - max_v: 15.0
    16	    recommend: ["FKM"]
    17	  - max_v: 30.0
    18	    recommend: ["PTFE"]


===== FILE: backend/app/services/langgraph/tests/__init__.py =====


===== FILE: backend/app/services/langgraph/tests/test_ask_missing_prompt.py =====
     1	from __future__ import annotations
     2	from app.services.langgraph.graph.consult.nodes.ask_missing import ask_missing_node
     3	from app.services.langgraph.prompting import render_template
     4	
     5	def test_rwdr_missing_prompt_contains_expected_labels():
     6	    state = {"consult_required": True, "domain": "rwdr", "params": {}, "messages":[{"role":"user","content":"rwdr"}]}
     7	    res = ask_missing_node(state)
     8	    msg = res["messages"][0].content
     9	    assert "Welle (mm)" in msg
    10	    assert "Gehäuse (mm)" in msg
    11	    assert "Breite (mm)" in msg
    12	    assert "Druck (bar)" in msg
    13	    assert res.get("ui_event", {}).get("form_id") == "rwdr_params_v1"
    14	    assert res.get("phase") == "ask_missing"
    15	
    16	def test_hyd_missing_prompt_contains_expected_labels():
    17	    state = {"consult_required": True, "domain": "hydraulics_rod", "params": {}, "messages":[{"role":"user","content":"hyd"}]}
    18	    res = ask_missing_node(state)
    19	    msg = res["messages"][0].content
    20	    assert "Stange (mm)" in msg
    21	    assert "Nut-Ø D (mm)" in msg
    22	    assert "Nutbreite B (mm)" in msg
    23	    assert "Relativgeschwindigkeit (m/s)" in msg
    24	    assert res.get("ui_event", {}).get("form_id") == "hydraulics_rod_params_v1"
    25	
    26	def test_followups_template_renders_list():
    27	    out = render_template("ask_missing_followups.jinja2",
    28	                          followups=["Tmax plausibel bei v≈3 m/s?", "Druck > 200 bar bestätigt?"])
    29	    assert "Bevor ich empfehle" in out
    30	    assert "- Tmax plausibel bei v≈3 m/s?" in out
    31	    assert "- Druck > 200 bar bestätigt?" in out
    32	    assert "Passt das so?" in out


===== FILE: backend/app/services/langgraph/tools/hitl.py =====
     1	from __future__ import annotations
     2	from typing import Dict, Any
     3	
     4	def hitl_required(reason: str) -> Dict[str, Any]:
     5	    return {"hitl_required": True, "reason": reason, "status": "pending_review"}


===== FILE: backend/app/services/langgraph/tools/__init__.py =====


===== FILE: backend/app/services/langgraph/tools/long_term_memory.py =====
     1	# backend/app/services/langgraph/tools/long_term_memory.py
     2	# -*- coding: utf-8 -*-
     3	"""
     4	Long-Term Memory (Qdrant) – non-blocking, production-friendly.
     5	
     6	- HuggingFace BAAI/bge-m3 (default) mit normalisierten Embeddings (Cosine)
     7	- Qdrant über langchain-qdrant
     8	- Hintergrund-Init (prewarm) + explizite Readiness-Prüfung
     9	- Defensive Suche mit MMR/Similarity, Timeouts & stabilen Defaults
    10	- Schreib-API (Upsert) für User-Memories
    11	"""
    12	
    13	from __future__ import annotations
    14	
    15	import os
    16	import time
    17	import threading
    18	import logging
    19	from dataclasses import dataclass
    20	from typing import List, Optional, Literal
    21	
    22	from qdrant_client import QdrantClient
    23	from langchain_qdrant import QdrantVectorStore as LCQdrant
    24	from langchain_huggingface import HuggingFaceEmbeddings
    25	
    26	log = logging.getLogger(__name__)
    27	
    28	# ─────────────────────────────────────────────────────────────
    29	# Konfiguration (mit robusten Defaults)
    30	# ─────────────────────────────────────────────────────────────
    31	
    32	def _bool(env: str, default: bool) -> bool:
    33	    v = os.getenv(env, str(int(default))).strip().lower()
    34	    return v in ("1", "true", "yes", "y", "on")
    35	
    36	def _int(env: str, default: int) -> int:
    37	    try:
    38	        return int(os.getenv(env, str(default)))
    39	    except Exception:
    40	        return default
    41	
    42	def _float(env: str, default: float) -> float:
    43	    try:
    44	        return float(os.getenv(env, str(default)))
    45	    except Exception:
    46	        return default
    47	
    48	_LTM_ENABLED: bool = _bool("LTM_ENABLED", True)
    49	
    50	_PROVIDER: str = os.getenv("LTM_PROVIDER", "huggingface").strip().lower()
    51	_HF_MODEL: str = os.getenv("LTM_HF_MODEL", "BAAI/bge-m3").strip()
    52	
    53	_QDRANT_URL: str = os.getenv("LTM_QDRANT_URL", os.getenv("QDRANT_URL", "http://qdrant:6333")).strip()
    54	_COLLECTION: str = os.getenv("LTM_QDRANT_COLLECTION", os.getenv("QDRANT_COLLECTION", "sealai-docs-bge-m3")).strip()
    55	
    56	_TOP_K: int = _int("LTM_TOP_K", 4)
    57	_SEARCH_TIMEOUT_MS: int = _int("LTM_SEARCH_TIMEOUT_MS", 300)   # Wartedauer bis Vektorstore bereit ist (pro Anfrage)
    58	_MMR_FETCH_MULT: int = _int("LTM_MMR_FETCH_MULT", 4)           # fetch_k = max(k * mult, 20)
    59	
    60	# Initialisierungs-Retry (nur für Warmup)
    61	_INIT_RETRIES: int = _int("LTM_INIT_RETRIES", 2)
    62	_INIT_RETRY_DELAY_MS: int = _int("LTM_INIT_RETRY_DELAY_MS", 400)
    63	
    64	# ─────────────────────────────────────────────────────────────
    65	# Globals (lazy, thread-safe)
    66	# ─────────────────────────────────────────────────────────────
    67	
    68	_ready = threading.Event()
    69	_init_lock = threading.Lock()
    70	
    71	_client: Optional[QdrantClient] = None
    72	_embeddings: Optional[HuggingFaceEmbeddings] = None
    73	_vector: Optional[LCQdrant] = None
    74	
    75	@dataclass
    76	class LtmHit:
    77	    text: str
    78	    score: Optional[float] = None
    79	
    80	
    81	# ─────────────────────────────────────────────────────────────
    82	# Init-Helfer
    83	# ─────────────────────────────────────────────────────────────
    84	
    85	def _init_hf_embeddings() -> HuggingFaceEmbeddings:
    86	    """
    87	    Für BGE-Modelle ist normalize_embeddings=True empfohlen (Cosine-Similarity).
    88	    """
    89	    log.info("LTM: using HuggingFaceEmbeddings model=%s", _HF_MODEL)
    90	    return HuggingFaceEmbeddings(
    91	        model_name=_HF_MODEL,
    92	        encode_kwargs={"normalize_embeddings": True},
    93	    )
    94	
    95	def _warm_qdrant(client: QdrantClient) -> None:
    96	    # Einfacher Warmup-Call – löst Verbindung auf & prüft Collections-Endpoint
    97	    client.get_collections()
    98	
    99	def _do_init_once() -> None:
   100	    """Ein Versuch der Heavy-Init (ohne Retries)."""
   101	    global _client, _embeddings, _vector
   102	
   103	    if _PROVIDER != "huggingface":
   104	        raise RuntimeError(f"LTM provider '{_PROVIDER}' wird aktuell nicht unterstützt")
   105	
   106	    log.info("LTM: Connecting Qdrant at %s", _QDRANT_URL)
   107	    _client = QdrantClient(url=_QDRANT_URL)
   108	    _warm_qdrant(_client)
   109	
   110	    _embeddings = _init_hf_embeddings()
   111	
   112	    _vector = LCQdrant(
   113	        client=_client,
   114	        collection_name=_COLLECTION,
   115	        embedding=_embeddings,
   116	    )
   117	
   118	def _do_init() -> None:
   119	    """Heavy init (runs in background once, mit leichten Retries)."""
   120	    try:
   121	        for attempt in range(_INIT_RETRIES + 1):
   122	            try:
   123	                _do_init_once()
   124	                _ready.set()
   125	                log.info(
   126	                    "LTM: ready (provider=%s, collection=%s, model=%s)",
   127	                    _PROVIDER, _COLLECTION, _HF_MODEL
   128	                )
   129	                return
   130	            except Exception as e:
   131	                if attempt >= _INIT_RETRIES:
   132	                    raise
   133	                delay = max(0, _INIT_RETRY_DELAY_MS) / 1000.0
   134	                log.warning("LTM init attempt %d failed: %s – retrying in %sms",
   135	                            attempt + 1, e, _INIT_RETRY_DELAY_MS)
   136	                time.sleep(delay)
   137	    except Exception as e:
   138	        log.exception("LTM init failed: %s", e)
   139	
   140	
   141	def prewarm_ltm() -> None:
   142	    """Kick off background initialization (non-blocking)."""
   143	    if not _LTM_ENABLED:
   144	        log.info("LTM disabled via LTM_ENABLED=0")
   145	        return
   146	    if _ready.is_set():
   147	        return
   148	    with _init_lock:
   149	        if _ready.is_set():
   150	            return
   151	        t = threading.Thread(target=_do_init, name="ltm-prewarm", daemon=True)
   152	        t.start()
   153	
   154	def wait_until_ready(timeout_ms: int = 20000) -> bool:
   155	    """Prewarm anstoßen und bis zu timeout_ms warten. Gibt True zurück, wenn LTM bereit ist."""
   156	    if not _LTM_ENABLED:
   157	        return False
   158	    prewarm_ltm()
   159	    _ready.wait(max(0.0, timeout_ms / 1000.0))
   160	    return _ready.is_set()
   161	
   162	def is_ready() -> bool:
   163	    return _ready.is_set()
   164	
   165	
   166	# ─────────────────────────────────────────────────────────────
   167	# Write API (Upsert)
   168	# ─────────────────────────────────────────────────────────────
   169	
   170	def upsert_memory(*, user: str, chat_id: Optional[str], text: str, kind: str = "note") -> str:
   171	    """
   172	    Speichert einen Memory-Schnipsel in Qdrant.
   173	    Gibt die (stringifizierte) Point-ID zurück.
   174	    """
   175	    if not _LTM_ENABLED:
   176	        raise RuntimeError("LTM disabled")
   177	
   178	    if not (text or "").strip():
   179	        raise ValueError("empty text")
   180	
   181	    if not wait_until_ready(timeout_ms=_SEARCH_TIMEOUT_MS):
   182	        raise RuntimeError("LTM not ready")
   183	
   184	    if _vector is None:
   185	        raise RuntimeError("Vector store unavailable")
   186	
   187	    meta = {
   188	        "user": user,
   189	        "chat_id": (chat_id or "").strip(),
   190	        "kind": kind,
   191	        "text": text.strip(),
   192	    }
   193	    ids = _vector.add_texts([text], metadatas=[meta])
   194	    return str(ids[0]) if ids else ""
   195	
   196	
   197	# ─────────────────────────────────────────────────────────────
   198	# Read API
   199	# ─────────────────────────────────────────────────────────────
   200	
   201	def build_context_text(hits: List[LtmHit]) -> str:
   202	    if not hits:
   203	        return ""
   204	    return "\n\n---\n\n".join(
   205	        (h.text or "").strip() for h in hits if (h.text or "").strip()
   206	    )
   207	
   208	def search(
   209	    query: str,
   210	    top_k: Optional[int] = None,
   211	    timeout_ms: Optional[int] = None,
   212	    *,
   213	    with_scores: bool = False,
   214	    strategy: Literal["similarity", "mmr"] = "similarity",
   215	    mmr_lambda: float = 0.5,
   216	    fetch_k: Optional[int] = None,
   217	) -> List[LtmHit]:
   218	    """
   219	    Schnelle, defensive Suche gegen Qdrant.
   220	    - returnt [] wenn LTM nicht rechtzeitig bereit ist (timeout_ms)
   221	    - strategy="mmr": holt vielfältigere Kontexte (reduziert Near-Duplicates)
   222	    """
   223	    if not _LTM_ENABLED or not (query or "").strip():
   224	        return []
   225	
   226	    prewarm_ltm()
   227	
   228	    wait = _SEARCH_TIMEOUT_MS if timeout_ms is None else max(0, int(timeout_ms))
   229	    _ready.wait(max(0.0, wait / 1000.0))
   230	
   231	    if not _ready.is_set() or _vector is None:
   232	        return []
   233	
   234	    try:
   235	        k = max(1, int(top_k or _TOP_K))
   236	
   237	        if strategy == "mmr":
   238	            fk = int(fetch_k) if fetch_k is not None else max(k * _MMR_FETCH_MULT, 20)
   239	            docs = _vector.max_marginal_relevance_search(
   240	                query, k=k, fetch_k=fk, lambda_mult=mmr_lambda
   241	            )
   242	            return [LtmHit(text=d.page_content, score=None) for d in docs]
   243	
   244	        if with_scores:
   245	            docs_scores = _vector.similarity_search_with_score(query, k=k)
   246	            return [LtmHit(text=d.page_content, score=float(s)) for d, s in docs_scores]
   247	        else:
   248	            docs = _vector.similarity_search(query, k=k)
   249	            return [LtmHit(text=d.page_content, score=None) for d in docs]
   250	
   251	    except Exception as e:
   252	        log.warning("LTM search failed (ignored): %s", e)
   253	        return []
   254	
   255	def ltm_query(
   256	    user_query: str,
   257	    *,
   258	    top_k: int | None = None,
   259	    timeout_ms: int | None = None,
   260	    strategy: Literal["similarity", "mmr"] = "similarity",
   261	    with_scores: bool = False,
   262	) -> tuple[str, List[LtmHit]]:
   263	    """Convenience: führt search() aus und gibt (context_text, hits) zurück."""
   264	    hits = search(
   265	        user_query,
   266	        top_k=top_k,
   267	        timeout_ms=timeout_ms,
   268	        strategy=strategy,
   269	        with_scores=with_scores,
   270	    )
   271	    return build_context_text(hits), hits


===== FILE: backend/app/services/langgraph/tools/rag_search_tool.py =====
     1	# backend/app/services/langgraph/tools/rag_search_tool.py
     2	from __future__ import annotations
     3	
     4	from typing import Any, Dict, List, Optional, TypedDict
     5	from langchain_core.tools import tool
     6	from ...rag.rag_orchestrator import hybrid_retrieve
     7	
     8	class RagSearchInput(TypedDict, total=False):
     9	    query: str
    10	    tenant: Optional[str]
    11	    k: int
    12	    filters: Dict[str, Any]
    13	
    14	@tool("rag_search", return_direct=False)
    15	def rag_search_tool(query: str, tenant: Optional[str] = None, k: int = 6, **filters: Any) -> List[Dict[str, Any]]:
    16	    """Hybrid Retrieval (Qdrant + BM25 + Rerank). Returns top-k docs with metadata and fused scores."""
    17	    docs = hybrid_retrieve(query=query, tenant=tenant, k=k, metadata_filters=filters or None, use_rerank=True)
    18	    return docs


===== FILE: backend/app/services/langgraph/tools/telemetry.py =====
     1	from __future__ import annotations
     2	import os
     3	try:
     4	    import redis
     5	except Exception:
     6	    redis = None
     7	
     8	class Telemetry:
     9	    def __init__(self) -> None:
    10	        self.client = None
    11	        url = os.getenv("REDIS_URL") or os.getenv("REDIS_HOST")
    12	        if redis and url:
    13	            try:
    14	                self.client = redis.Redis.from_url(url) if "://" in url else redis.Redis(host=url, port=int(os.getenv("REDIS_PORT", "6379")))
    15	            except Exception:
    16	                self.client = None
    17	
    18	    def incr(self, key: str, amount: int = 1) -> None:
    19	        if self.client:
    20	            try:
    21	                self.client.incr(key, amount)
    22	            except Exception:
    23	                pass
    24	
    25	    def set_gauge(self, key: str, value: float) -> None:
    26	        if self.client:
    27	            try:
    28	                self.client.set(key, value)
    29	            except Exception:
    30	                pass
    31	
    32	telemetry = Telemetry()
    33	RFQ_GENERATED = "rfq_generated_count"
    34	PARTNER_COVERAGE = "partner_coverage_rate"
    35	MODEL_USAGE = "model_usage_distribution"
    36	NO_MATCH_RATE = "no_match_rate"


===== FILE: backend/app/services/langgraph/tools/ui_events.py =====
     1	from __future__ import annotations
     2	from typing import Dict, Any
     3	
     4	UI = dict(
     5	    DECISION_READY="decision_ready",
     6	    RFQ_READY="rfq_ready",
     7	    NO_PARTNER_AVAILABLE="no_partner_available",
     8	    OPEN_FORM="open_form",
     9	)
    10	
    11	def make_event(action: str, **payload: Any) -> Dict[str, Any]:
    12	    return {"ui_action": action, "payload": payload}


===== FILE: backend/app/services/memory/conversation_memory.py =====
     1	# backend/app/services/memory/conversation_memory.py
     2	"""
     3	Conversation STM (Short-Term Memory) auf Redis
     4	----------------------------------------------
     5	- Speichert JEDE Chatnachricht (user|assistant|system) chronologisch.
     6	- Ring-Buffer per LTRIM (STM_MAX_MSG).
     7	- TTL wird bei jedem Push erneuert (STM_TTL_SEC).
     8	"""
     9	
    10	from __future__ import annotations
    11	import os
    12	import json
    13	import time
    14	from typing import Literal, List, Dict, Any
    15	from redis import Redis
    16	
    17	# ───────────────────────── Config ──────────────────────────
    18	REDIS_URL = os.getenv("REDIS_URL", "redis://redis:6379/0")
    19	TTL_SEC   = int(os.getenv("STM_TTL_SEC", "604800"))           # 7 Tage
    20	MAX_MSG   = int(os.getenv("STM_MAX_MSG", "200"))              # max. Messages/Chat
    21	PREFIX    = os.getenv("STM_PREFIX", "chat:stm")               # Key-Namespace
    22	# ────────────────────────────────────────────────────────────
    23	
    24	def _r() -> Redis:
    25	    return Redis.from_url(REDIS_URL, decode_responses=True)
    26	
    27	def _key(chat_id: str) -> str:
    28	    return f"{PREFIX}:{chat_id}:messages"
    29	
    30	def _touch(chat_id: str) -> None:
    31	    _r().expire(_key(chat_id), TTL_SEC)
    32	
    33	Role = Literal["user", "assistant", "system"]
    34	
    35	def add_message(chat_id: str, role: Role, content: str) -> None:
    36	    """Hängt eine Nachricht an das Chat-Log (Ring-Buffer) an."""
    37	    if not chat_id or not isinstance(content, str) or not content.strip():
    38	        return
    39	    doc: Dict[str, Any] = {
    40	        "role": role,
    41	        "content": content,
    42	        "ts": time.time(),
    43	    }
    44	    r = _r()
    45	    r.rpush(_key(chat_id), json.dumps(doc, ensure_ascii=False))
    46	    r.ltrim(_key(chat_id), -MAX_MSG, -1)  # Ring-Buffer begrenzen
    47	    _touch(chat_id)
    48	
    49	def get_history(chat_id: str, limit: int = 20) -> List[Dict[str, Any]]:
    50	    """Liest die letzten N Nachrichten (chronologisch)."""
    51	    if limit <= 0:
    52	        return []
    53	    r = _r()
    54	    raw = r.lrange(_key(chat_id), -limit, -1)
    55	    out: List[Dict[str, Any]] = []
    56	    for row in raw:
    57	        try:
    58	            out.append(json.loads(row))
    59	        except Exception:
    60	            continue
    61	    return out


===== FILE: backend/app/services/memory/__init__.py =====


===== FILE: backend/app/services/memory/memory_core.py =====
     1	"""
     2	Memory Core: Kapselt Long-Term-Memory (Qdrant) für Export/Löschen.
     3	Kurz-/Mittelzeit (Redis/Summary) laufen separat über LangGraph-Checkpointer.
     4	
     5	Payload-Felder pro Eintrag:
     6	- user: str                  (Pflicht für Filterung pro Benutzer)
     7	- chat_id: str               (optional; für Export/Löschen pro Chat)
     8	- kind: str                  (z. B. "preference", "fact", "note", …)
     9	- text: str                  (Inhalt)
    10	- created_at: float|int|str  (optional: Unix-Zeit oder ISO)
    11	Weitere Felder erlaubt – werden unverändert mit exportiert.
    12	"""
    13	
    14	from __future__ import annotations
    15	
    16	from typing import Any, Dict, List, Optional, Tuple
    17	
    18	from qdrant_client import QdrantClient, models
    19	from qdrant_client.http.models import FilterSelector
    20	
    21	from app.core.config import settings
    22	
    23	
    24	# ---------------------------------------------------------------------------
    25	# Qdrant Client & Collection
    26	# ---------------------------------------------------------------------------
    27	
    28	def _get_qdrant_client() -> QdrantClient:
    29	    kwargs = {"url": settings.qdrant_url}
    30	    if settings.qdrant_api_key:
    31	        kwargs["api_key"] = settings.qdrant_api_key
    32	    return QdrantClient(**kwargs)
    33	
    34	
    35	def _ltm_collection_name() -> str:
    36	    """
    37	    Eigene LTM-Collection verwenden, um keine Vektorgrößen-Konflikte mit der
    38	    RAG-Collection zu riskieren. Fallback: "<qdrant_collection>-ltm".
    39	    """
    40	    return (settings.qdrant_collection_ltm or f"{settings.qdrant_collection}-ltm").strip()
    41	
    42	
    43	def ensure_ltm_collection(client: QdrantClient) -> None:
    44	    """
    45	    Stellt sicher, dass die LTM-Collection existiert. Wir verwenden einen
    46	    Dummy-Vektor (size=1), da wir nur Payload-basierte Scroll/Filter-Operationen
    47	    benötigen. (Qdrant verlangt einen Vektorspace pro Collection.)
    48	    """
    49	    coll = _ltm_collection_name()
    50	    try:
    51	        client.get_collection(coll)
    52	    except Exception:
    53	        client.recreate_collection(
    54	            collection_name=coll,
    55	            vectors_config=models.VectorParams(size=1, distance=models.Distance.COSINE),
    56	        )
    57	
    58	
    59	# ---------------------------------------------------------------------------
    60	# Export / Delete
    61	# ---------------------------------------------------------------------------
    62	
    63	def _build_user_filter(user: str, chat_id: Optional[str] = None) -> models.Filter:
    64	    must: List[models.FieldCondition] = [
    65	        models.FieldCondition(key="user", match=models.MatchValue(value=user))
    66	    ]
    67	    if chat_id:
    68	        must.append(models.FieldCondition(key="chat_id", match=models.MatchValue(value=chat_id)))
    69	    return models.Filter(must=must)
    70	
    71	
    72	def ltm_export_all(
    73	    user: str,
    74	    chat_id: Optional[str] = None,
    75	    limit: int = 10000,
    76	) -> List[Dict[str, Any]]:
    77	    """
    78	    Exportiert bis zu `limit` LTM-Items für den User (optional gefiltert nach chat_id).
    79	    Liefert Liste aus {id, payload}.
    80	    """
    81	    if not settings.ltm_enable:
    82	        return []
    83	
    84	    client = _get_qdrant_client()
    85	    ensure_ltm_collection(client)
    86	
    87	    flt = _build_user_filter(user, chat_id)
    88	    out: List[Dict[str, Any]] = []
    89	
    90	    next_page = None
    91	    fetched = 0
    92	    page_size = 512
    93	    coll = _ltm_collection_name()
    94	
    95	    while fetched < limit:
    96	        points, next_page = client.scroll(
    97	            collection_name=coll,
    98	            scroll_filter=flt,
    99	            with_payload=True,
   100	            with_vectors=False,
   101	            limit=min(page_size, limit - fetched),
   102	            offset=next_page,
   103	        )
   104	        if not points:
   105	            break
   106	        for p in points:
   107	            out.append({
   108	                "id": str(p.id),
   109	                "payload": dict(p.payload or {}),
   110	            })
   111	        fetched += len(points)
   112	        if next_page is None:
   113	            break
   114	
   115	    return out
   116	
   117	
   118	def ltm_delete_all(
   119	    user: str,
   120	    chat_id: Optional[str] = None,
   121	) -> int:
   122	    """
   123	    Löscht alle LTM-Items für User (optional gefiltert nach chat_id).
   124	    Gibt die Anzahl der gelöschten Punkte (approx.) zurück.
   125	    """
   126	    if not settings.ltm_enable:
   127	        return 0
   128	
   129	    client = _get_qdrant_client()
   130	    ensure_ltm_collection(client)
   131	
   132	    flt = _build_user_filter(user, chat_id)
   133	    coll = _ltm_collection_name()
   134	
   135	    # Vorab zählen (für Response)
   136	    to_delete = 0
   137	    next_page = None
   138	    while True:
   139	        points, next_page = client.scroll(
   140	            collection_name=coll,
   141	            scroll_filter=flt,
   142	            with_payload=False,
   143	            with_vectors=False,
   144	            limit=1024,
   145	            offset=next_page,
   146	        )
   147	        if not points:
   148	            break
   149	        to_delete += len(points)
   150	        if next_page is None:
   151	            break
   152	
   153	    # Delete via Filter (serverseitig)
   154	    client.delete(
   155	        collection_name=coll,
   156	        points_selector=FilterSelector(filter=flt),
   157	        wait=True,
   158	    )
   159	    return to_delete


===== FILE: backend/app/services/prompt_templates/intent_router.jinja2 =====
     1	# Rolle
     2	Du agierst als Intent-Router für Dichtungstechnik (Radialwellendichtringe).
     3	
     4	# Eingabe
     5	{{ (input_text or input or query or user_input or message or prompt)|trim }}
     6	
     7	# Aufgabe
     8	Bestimme die Absicht und gib **ausschließlich** ein JSON-Objekt zurück.
     9	
    10	# Erlaubte Intents und Standard-Routing
    11	# - "smalltalk"     → route "smalltalk"
    12	# - "ask_missing"   → route "ask_missing"   (wenn wesentliche Parameter fehlen)
    13	# - "material"      → route "material"      (Werkstoffberatung)
    14	# - "profil"        → route "profil"        (Profil-/Bauformberatung)
    15	# - "recommend"     → route "recommend"     (wenn alle Kernparameter vorliegen)
    16	# - "explain"       → route "explain"       (wenn der Nutzer explizit Erklärung will)
    17	# - "routing_error" → route "ask_missing"   (Fallback bei Unklarheit)
    18	
    19	# Kernparameter (falls erkennbar, sonst weglassen oder null):
    20	# - welle_mm, gehaeuse_mm, breite_mm, medium, tmax_c, druck_bar, drehzahl_rpm
    21	
    22	# Regeln
    23	# - Antworte NUR mit JSON. Keine Erklärungen, keine zusätzlichen Zeichen.
    24	# - Fülle immer "intent" und "route" aus.
    25	# - "missing": Liste der fehlenden Kernparameter aus
    26	#   ["welle_mm","gehaeuse_mm","breite_mm","medium","tmax_c","druck_bar","drehzahl_rpm"].
    27	# - "confidence": Zahl 0..1.
    28	
    29	{
    30	  "intent": "<smalltalk|ask_missing|material|profil|recommend|explain|routing_error>",
    31	  "route": "<smalltalk|ask_missing|material|profil|recommend|explain>",
    32	  "params": {
    33	    "welle_mm": null,
    34	    "gehaeuse_mm": null,
    35	    "breite_mm": null,
    36	    "medium": null,
    37	    "tmax_c": null,
    38	    "druck_bar": null,
    39	    "drehzahl_rpm": null
    40	  },
    41	  "missing": [],
    42	  "confidence": 0.8
    43	}


===== FILE: backend/app/services/rag/__init__.py =====


===== FILE: backend/app/services/rag/rag_ingest.py =====
     1	import os, sys
     2	from langchain_qdrant import QdrantVectorStore
     3	from langchain_text_splitters import RecursiveCharacterTextSplitter
     4	from langchain_huggingface import HuggingFaceEmbeddings
     5	from langchain_community.document_loaders import (
     6	    PDFPlumberLoader, Docx2txtLoader, TextLoader, UnstructuredFileLoader
     7	)
     8	
     9	QDRANT_URL = os.getenv("QDRANT_URL", "http://qdrant:6333")
    10	QDRANT_API_KEY = os.getenv("QDRANT_API_KEY", None)
    11	QDRANT_COLLECTION = os.getenv("QDRANT_COLLECTION", "sealai-docs-bge-m3")
    12	EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "BAAI/bge-m3")
    13	SUPPORTED_EXTENSIONS = [".pdf", ".txt", ".docx", ".md"]
    14	
    15	def load_document(file_path: str):
    16	    ext = os.path.splitext(file_path)[1].lower()
    17	    if ext == ".pdf":
    18	        return PDFPlumberLoader(file_path).load()
    19	    elif ext == ".docx":
    20	        return Docx2txtLoader(file_path).load()
    21	    elif ext in (".txt", ".md"):
    22	        return TextLoader(file_path).load()
    23	    else:
    24	        return UnstructuredFileLoader(file_path).load()
    25	
    26	def ingest_file(file_path: str, chunk_size: int = 700, chunk_overlap: int = 80):
    27	    print(f"[INGEST] Lade: {file_path}")
    28	    docs = load_document(file_path)
    29	    print(f"[INGEST] Split: size={chunk_size}, overlap={chunk_overlap}")
    30	    splitter = RecursiveCharacterTextSplitter(
    31	        chunk_size=chunk_size, chunk_overlap=chunk_overlap,
    32	        separators=["\\n\\n", "\\n", ".", " ", ""],
    33	    )
    34	    split_docs = splitter.split_documents(docs)
    35	
    36	    print(f"[INGEST] HF-Embeddings: {EMBEDDING_MODEL} (normalize=True)")
    37	    embeddings = HuggingFaceEmbeddings(
    38	        model_name=EMBEDDING_MODEL,
    39	        encode_kwargs={"normalize_embeddings": True},
    40	    )
    41	
    42	    print(f"[INGEST] Schreibe nach Qdrant: {QDRANT_COLLECTION}")
    43	    _ = QdrantVectorStore.from_documents(
    44	        split_docs, embeddings,
    45	        url=QDRANT_URL, api_key=QDRANT_API_KEY,
    46	        collection_name=QDRANT_COLLECTION,
    47	    )
    48	    print(f"[INGEST] OK: {file_path}")
    49	
    50	def ingest_directory(directory: str):
    51	    files = [
    52	        os.path.join(directory, f)
    53	        for f in os.listdir(directory)
    54	        if os.path.splitext(f)[1].lower() in SUPPORTED_EXTENSIONS
    55	    ]
    56	    if not files:
    57	        print(f"[INGEST] Keine unterstützten Dateien in {directory}")
    58	    for fp in files:
    59	        ingest_file(fp)
    60	
    61	if __name__ == "__main__":
    62	    if len(sys.argv) < 2:
    63	        print("Nutzung: python rag_ingest.py <file_or_directory>")
    64	        sys.exit(1)
    65	    target = sys.argv[1]
    66	    if os.path.isdir(target):
    67	        ingest_directory(target)
    68	    else:
    69	        ingest_file(target)


===== FILE: backend/app/services/rag/rag_orchestrator.py =====
     1	# backend/app/services/rag/rag_orchestrator.py
     2	from __future__ import annotations
     3	
     4	import math
     5	import os
     6	import time
     7	import re
     8	from dataclasses import dataclass, asdict
     9	from typing import Any, Dict, List, Optional
    10	
    11	import structlog
    12	from qdrant_client import QdrantClient
    13	from qdrant_client.http.models import (
    14	    Distance,
    15	    FieldCondition,
    16	    Filter as QFilter,
    17	    MatchValue,
    18	    VectorParams,
    19	)
    20	from sentence_transformers import SentenceTransformer
    21	from sentence_transformers.cross_encoder import CrossEncoder
    22	
    23	log = structlog.get_logger(__name__)
    24	
    25	# ---- Konfiguration ----
    26	QDRANT_HOST = os.getenv("QDRANT_HOST", "qdrant")
    27	QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))
    28	QDRANT_API_KEY = os.getenv("QDRANT_API_KEY") or None
    29	
    30	COLL_PREFIX = os.getenv("QDRANT_COLLECTION_PREFIX", "sealai-docs")
    31	DEFAULT_COLL = os.getenv("QDRANT_DEFAULT_COLLECTION", COLL_PREFIX)
    32	
    33	EMB_MODEL_NAME = os.getenv("EMB_MODEL_NAME", "intfloat/multilingual-e5-base")
    34	RERANK_MODEL_NAME = os.getenv("RERANK_MODEL_NAME", "cross-encoder/ms-marco-MiniLM-L-6-v2")
    35	
    36	HYBRID_K = int(os.getenv("RAG_HYBRID_K", "12"))
    37	FINAL_K = int(os.getenv("RAG_FINAL_K", "6"))
    38	RRF_K = int(os.getenv("RAG_RRF_K", "60"))
    39	VEC_SCORE_THRESHOLD = float(os.getenv("RAG_SCORE_THRESHOLD", "0.0"))
    40	
    41	TENANT_FIELD = os.getenv("RAG_TENANT_FIELD", "tenant")
    42	REDIS_BM25_INDEX = os.getenv("REDIS_BM25_INDEX", "sealai:docs")
    43	REDIS_URL = os.getenv("REDIS_URL", "redis://redis:6379/0")
    44	
    45	# ---- Lazy Singletons ----
    46	_qdrant: Optional[QdrantClient] = None
    47	_emb: Optional[SentenceTransformer] = None
    48	_reranker: Optional[CrossEncoder] = None
    49	_redis_search: Optional[Dict[str, Any]] = None  # {"mode":"raw"/"redisvl", ...}
    50	
    51	@dataclass
    52	class RetrievedDoc:
    53	    id: str
    54	    text: str
    55	    source: Optional[str] = None
    56	    metadata: Optional[Dict[str, Any]] = None
    57	    vector_score: Optional[float] = None
    58	    keyword_score: Optional[float] = None
    59	    fused_score: Optional[float] = None
    60	    def to_payload(self) -> Dict[str, Any]:
    61	        d = asdict(self)
    62	        if self.metadata and len(str(self.metadata)) > 5000:
    63	            d["metadata"] = {"_truncated": True}
    64	        return d
    65	
    66	# ---------------- Qdrant / Embeddings ----------------
    67	def _get_qdrant() -> QdrantClient:
    68	    global _qdrant
    69	    if _qdrant is None:
    70	        _qdrant = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT, api_key=QDRANT_API_KEY, timeout=30.0)
    71	    return _qdrant
    72	
    73	def _get_emb() -> SentenceTransformer:
    74	    global _emb
    75	    if _emb is None:
    76	        t0 = time.time()
    77	        _emb = SentenceTransformer(EMB_MODEL_NAME, device="cpu")
    78	        log.info("embeddings_loaded", model=EMB_MODEL_NAME, ms=round((time.time() - t0) * 1000))
    79	    return _emb
    80	
    81	def _get_reranker() -> CrossEncoder:
    82	    global _reranker
    83	    if _reranker is None:
    84	        t0 = time.time()
    85	        _reranker = CrossEncoder(RERANK_MODEL_NAME)
    86	        log.info("reranker_loaded", model=RERANK_MODEL_NAME, ms=round((time.time() - t0) * 1000))
    87	    return _reranker
    88	
    89	def _collection_for(tenant: Optional[str]) -> str:
    90	    return f"{COLL_PREFIX}-{tenant}" if tenant else DEFAULT_COLL
    91	
    92	def _embed(texts: List[str]) -> List[List[float]]:
    93	    emb = _get_emb()
    94	    prepped = [f"query: {t}" for t in texts]
    95	    return emb.encode(prepped, normalize_embeddings=True).tolist()
    96	
    97	def _ensure_collection(coll: str) -> None:
    98	    client = _get_qdrant()
    99	    try:
   100	        names = {c.name for c in (client.get_collections().collections or [])}
   101	        if coll in names:
   102	            return
   103	    except Exception:
   104	        pass
   105	    dim = len(_embed(["ping"])[0])
   106	    log.info("qdrant_create_collection", collection=coll, dim=dim)
   107	    client.create_collection(collection_name=coll, vectors_config=VectorParams(size=dim, distance=Distance.COSINE))
   108	    for field in (TENANT_FIELD, "material", "profile", "domain", "norm", "lang", "source", "doc_sha1"):
   109	        try:
   110	            client.create_payload_index(collection_name=coll, field_name=field, field_schema="keyword")
   111	        except Exception:
   112	            pass
   113	
   114	def _qdrant_vector_search(query: str, tenant: Optional[str], k: int, metadata_filters: Optional[Dict[str, Any]]) -> List[RetrievedDoc]:
   115	    client = _get_qdrant()
   116	    coll = _collection_for(tenant)
   117	    _ensure_collection(coll)
   118	    vec = _embed([query])[0]
   119	    conditions: List[FieldCondition] = []
   120	    if tenant:
   121	        conditions.append(FieldCondition(key=TENANT_FIELD, match=MatchValue(value=tenant)))
   122	    if metadata_filters:
   123	        for kf, val in metadata_filters.items():
   124	            conditions.append(FieldCondition(key=kf, match=MatchValue(value=val)))
   125	    qfilter: Optional[QFilter] = QFilter(must=conditions) if conditions else None
   126	    try:
   127	        hits = client.search(
   128	            collection_name=coll,
   129	            query_vector=vec,
   130	            limit=k,
   131	            with_payload=True,
   132	            with_vectors=False,
   133	            score_threshold=VEC_SCORE_THRESHOLD if VEC_SCORE_THRESHOLD > 0 else None,
   134	            query_filter=qfilter,
   135	        )
   136	    except Exception as e:
   137	        log.warning("qdrant_search_failed", collection=coll, error=str(e))
   138	        return []
   139	    out: List[RetrievedDoc] = []
   140	    for h in hits:
   141	        payload = h.payload or {}
   142	        text = payload.get("text") or payload.get("content") or ""
   143	        src = payload.get("source") or payload.get("doc_uri") or payload.get("path")
   144	        out.append(
   145	            RetrievedDoc(
   146	                id=str(h.id),
   147	                text=text,
   148	                source=src,
   149	                metadata=payload,
   150	                vector_score=float(h.score) if h.score is not None else None,
   151	            )
   152	        )
   153	    return out
   154	
   155	# ---------------- Redis BM25 ----------------
   156	def _maybe_bind_redis_index() -> None:
   157	    global _redis_search
   158	    if _redis_search is not None:
   159	        return
   160	    index_name = (os.getenv("REDIS_BM25_INDEX") or REDIS_BM25_INDEX or "").strip()
   161	    if not index_name:
   162	        _redis_search = None
   163	        return
   164	    redis_url = os.getenv("REDIS_URL", REDIS_URL)
   165	    try:
   166	        from redisvl.index import SearchIndex  # type: ignore
   167	        try:
   168	            idx = SearchIndex.from_existing(index_name, redis_url=redis_url)
   169	        except Exception:
   170	            idx = SearchIndex.from_existing(index_name)
   171	            idx.connect(redis_url)
   172	        _redis_search = {"mode": "redisvl", "index": idx}
   173	        log.info("redis_bm25_bound", mode="redisvl", index=index_name, url=redis_url)
   174	        return
   175	    except Exception as e:
   176	        log.info("redis_bm25_unavailable", reason=str(e))
   177	    try:
   178	        import redis  # type: ignore
   179	        r = redis.Redis.from_url(redis_url)
   180	        r.execute_command("FT.INFO", index_name)
   181	        _redis_search = {"mode": "raw", "client": r, "name": index_name}
   182	        log.info("redis_bm25_bound", mode="raw", index=index_name, url=redis_url)
   183	    except Exception as e:
   184	        log.info("redis_bm25_unavailable", reason=str(e))
   185	        _redis_search = None
   186	
   187	def _normalize_redisvl_result(res: Any) -> List[Dict[str, Any]]:
   188	    docs: List[Any] = []
   189	    if isinstance(res, dict):
   190	        for key in ("documents", "docs", "results", "data"):
   191	            if key in res and isinstance(res[key], (list, tuple)):
   192	                docs = list(res[key]); break
   193	        else:
   194	            for v in res.values():
   195	                if isinstance(v, (list, tuple)) and v and isinstance(v[0], (dict, object)):
   196	                    docs = list(v); break
   197	    elif isinstance(res, (list, tuple)):
   198	        docs = list(res)
   199	    else:
   200	        for attr in ("documents", "docs", "results", "data"):
   201	            if hasattr(res, attr):
   202	                maybe = getattr(res, attr)
   203	                if isinstance(maybe, (list, tuple)):
   204	                    docs = list(maybe); break
   205	    out: List[Dict[str, Any]] = []
   206	    for d in docs:
   207	        if isinstance(d, dict):
   208	            out.append(d); continue
   209	        tmp: Dict[str, Any] = {}
   210	        for k in ("id", "pk", "text", "source", "__score", TENANT_FIELD):
   211	            if hasattr(d, k):
   212	                tmp[k] = getattr(d, k)
   213	        if hasattr(d, "payload") and isinstance(getattr(d, "payload"), dict):
   214	            tmp.update(getattr(d, "payload"))
   215	        if not tmp and hasattr(d, "__dict__"):
   216	            tmp.update({k: v for k, v in d.__dict__.items() if not k.startswith("_")})
   217	        out.append(tmp)
   218	    return out
   219	
   220	def _redisvl_search(idx, q: str, k: int, return_fields: List[str]) -> List[Dict[str, Any]]:
   221	    variants = [
   222	        {"num_results": k, "return_fields": return_fields},
   223	        {"top_k": k, "return_fields": return_fields},
   224	        {"k": k, "return_fields": return_fields},
   225	        {"paging": {"offset": 0, "limit": k}, "return_fields": return_fields},
   226	        {"return_fields": return_fields},
   227	    ]
   228	    last_err: Optional[Exception] = None
   229	    for kwargs in variants:
   230	        try:
   231	            res = idx.search(query=q, **kwargs)  # type: ignore
   232	            docs = _normalize_redisvl_result(res)
   233	            return docs[:k]
   234	        except TypeError as e:
   235	            last_err = e; continue
   236	        except Exception as e:
   237	            last_err = e; continue
   238	    try:
   239	        res = idx.search(q)  # type: ignore
   240	        docs = _normalize_redisvl_result(res)
   241	        return docs[:k]
   242	    except Exception as e:
   243	        raise RuntimeError(f"Unexpected error while searching: {e}") from last_err
   244	
   245	# NEW: defensives Quoten/Escapen – RediSearch-Sonderzeichen
   246	_RS_SPECIAL = re.compile(r'([\-@{}\[\]:"|><()~*?+^$\\])')
   247	def _sanitize_redis_query(q: str) -> str:
   248	    q = q.replace('"', r"\"")
   249	    q = _RS_SPECIAL.sub(r"\\\1", q)
   250	    return f"\"{q}\""  # als Phrase
   251	
   252	def _redis_bm25_search(query: str, k: int, tenant: Optional[str]) -> List[RetrievedDoc]:
   253	    _maybe_bind_redis_index()
   254	    if _redis_search is None:
   255	        return []
   256	    user_q = _sanitize_redis_query(query)
   257	    q = f'@{TENANT_FIELD}:{{{tenant}}} {user_q}' if tenant else user_q
   258	    rf = ["id", "text", "source", TENANT_FIELD]
   259	    try:
   260	        if _redis_search.get("mode") == "redisvl":
   261	            idx = _redis_search["index"]
   262	            rows = _redisvl_search(idx, q, k, rf)
   263	            docs: List[RetrievedDoc] = []
   264	            for r in rows:
   265	                docs.append(
   266	                    RetrievedDoc(
   267	                        id=str(r.get("id") or r.get("pk") or ""),
   268	                        text=r.get("text") or "",
   269	                        source=r.get("source"),
   270	                        keyword_score=float(r.get("__score", 0.0)) if "__score" in r else None,
   271	                        metadata={kk: vv for kk, vv in r.items() if kk not in {"id", "text", "source"}},
   272	                    )
   273	                )
   274	            return docs
   275	        r = _redis_search["client"]
   276	        name = _redis_search["name"]
   277	        res = r.execute_command("FT.SEARCH", name, q, "RETURN", 3, "text", "source", TENANT_FIELD, "LIMIT", 0, k)
   278	        if not res or len(res) < 2:
   279	            return []
   280	        items = res[1:]; docs: List[RetrievedDoc] = []
   281	        def _dec(x): return x.decode("utf-8") if isinstance(x, (bytes, bytearray)) else str(x)
   282	        for i in range(0, len(items), 2):
   283	            doc_id = _dec(items[i])
   284	            fields = items[i + 1]
   285	            data = {_dec(fields[j]): _dec(fields[j + 1]) for j in range(0, len(fields), 2)}
   286	            docs.append(RetrievedDoc(id=doc_id, text=data.get("text", ""), source=data.get("source"), keyword_score=None, metadata=data))
   287	        return docs
   288	    except Exception as e:
   289	        log.info("redis_search_failed", reason=str(e))
   290	        return []
   291	
   292	# ---------------- Fusion & Reranking ----------------
   293	def _rrf_fuse(vector_docs: List[RetrievedDoc], keyword_docs: List[RetrievedDoc], rrf_k: int, final_k: int) -> List[RetrievedDoc]:
   294	    by_id: Dict[str, RetrievedDoc] = {}
   295	    ranks: Dict[str, float] = {}
   296	    def add_rank(items: List[RetrievedDoc], weight: float = 1.0):
   297	        for idx, d in enumerate(items):
   298	            rid = d.id or f"{hash(d.text)}"
   299	            if rid not in by_id:
   300	                by_id[rid] = d
   301	            ranks[rid] = ranks.get(rid, 0.0) + weight * (1.0 / (rrf_k + (idx + 1)))
   302	    add_rank(vector_docs, 1.0); add_rank(keyword_docs, 1.0)
   303	    fused: List[RetrievedDoc] = []
   304	    for rid, score in sorted(ranks.items(), key=lambda x: x[1], reverse=True):
   305	        doc = by_id[rid]; doc.fused_score = score; fused.append(doc)
   306	    return fused[:final_k]
   307	
   308	def _logistic(x: float) -> float: return 1.0 / (1.0 + math.exp(-x))
   309	
   310	def _rerank(query: str, docs: List[RetrievedDoc]) -> List[RetrievedDoc]:
   311	    if not docs: return docs
   312	    try:
   313	        rer = _get_reranker()
   314	        pairs = [(query, d.text) for d in docs]
   315	        scores = rer.predict(pairs).tolist()
   316	        for d, s in zip(docs, scores):
   317	            d.fused_score = _logistic(float(s))
   318	        docs.sort(key=lambda d: d.fused_score or 0.0, reverse=True)
   319	    except Exception as e:
   320	        log.info("rerank_failed", reason=str(e))
   321	    return docs
   322	
   323	# ---------------- Public API ----------------
   324	def hybrid_retrieve(query: str, tenant: Optional[str], k: int = FINAL_K, metadata_filters: Optional[Dict[str, Any]] = None, use_rerank: bool = True) -> List[Dict[str, Any]]:
   325	    t0 = time.time()
   326	    vec_docs = _qdrant_vector_search(query, tenant=tenant, k=HYBRID_K, metadata_filters=metadata_filters)
   327	    kw_docs = _redis_bm25_search(query, k=HYBRID_K, tenant=tenant)
   328	    fused = _rrf_fuse(vec_docs, kw_docs, rrf_k=RRF_K, final_k=k)
   329	    if use_rerank:
   330	        fused = _rerank(query, fused)
   331	    ms = round((time.time() - t0) * 1000)
   332	    log.info("hybrid_retrieve", tenant=tenant, q=query[:120], vec=len(vec_docs), kw=len(kw_docs), fused=len(fused), ms=ms)
   333	    return [d.to_payload() for d in fused]
   334	
   335	# --------- Warmup (beim App-Start aufrufen) ---------
   336	def prewarm() -> None:
   337	    """Lädt Redis/Qdrant/Embedding/Reranker einmalig in den Speicher."""
   338	    try: _maybe_bind_redis_index()
   339	    except Exception: pass
   340	    try: _get_qdrant()
   341	    except Exception: pass
   342	    try: _get_emb()
   343	    except Exception: pass
   344	    try: _get_reranker()
   345	    except Exception: pass


===== FILE: backend/app/ws_stream_test.py =====
     1	import os
     2	import json
     3	import asyncio
     4	import inspect
     5	import websockets
     6	
     7	# --- Konfiguration über ENV ---
     8	WS_BASE   = os.getenv("WS_BASE", "ws://127.0.0.1:8000")
     9	WS_PATH   = os.getenv("WS_PATH", "/api/v1/ai/ws")
    10	WS_ORIGIN = os.getenv("WS_ORIGIN", "http://localhost:3000")
    11	WS_URL    = os.getenv("WS_URL")  # komplette URL (optional)
    12	TOKEN     = os.getenv("TOKEN", "")
    13	FORCE_STREAM = os.getenv("WS_FORCE_STREAM", "1") not in ("0", "false", "False")
    14	RAW_DEBUG    = os.getenv("WS_RAW_DEBUG", "0") in ("1", "true", "True")
    15	
    16	PROMPTS = [
    17	    ("ws1", "Guten Morgen, kurze Frage zu RWDR."),
    18	    ("ws1", "Ich brauche eine optimale Dichtungsempfehlung für RWDR 25x47x7, Öl, 2 bar, 1500 rpm."),
    19	]
    20	
    21	# --- Helpers ---
    22	def _connect_kwargs(headers):
    23	    """Kompatibel zu websockets-Versionen mit additional_headers/extra_headers."""
    24	    params = {"subprotocols": ["json"], "ping_interval": 20, "ping_timeout": 20, "max_size": None}
    25	    sig = inspect.signature(websockets.connect)
    26	    if "additional_headers" in sig.parameters:
    27	        params["additional_headers"] = headers
    28	    elif "extra_headers" in sig.parameters:
    29	        params["extra_headers"] = headers
    30	    return params
    31	
    32	def _empty(s: str) -> bool:
    33	    if not s: return True
    34	    t = s.strip()
    35	    return t in ("", "{}", "null", "[]")
    36	
    37	async def _drain(ws):
    38	    """Alle Frames lesen; bei event=done beenden."""
    39	    got_delta = False
    40	    while True:
    41	        raw = await ws.recv()
    42	        if RAW_DEBUG:
    43	            print(f"[raw] {raw!r}", flush=True)
    44	
    45	        if isinstance(raw, (bytes, bytearray)):
    46	            raw = raw.decode("utf-8", errors="ignore")
    47	        if _empty(raw):
    48	            continue
    49	
    50	        try:
    51	            msg = json.loads(raw)
    52	        except Exception:
    53	            # Falls der Server reine Textframes schickt (nicht JSON)
    54	            print(raw, end="", flush=True)
    55	            continue
    56	
    57	        if not isinstance(msg, dict):
    58	            continue
    59	
    60	        if msg.get("phase") == "starting":
    61	            print(f"\n[{msg.get('thread_id','?')}] starting\n", flush=True)
    62	
    63	        # Token-Streaming (delta)
    64	        if "delta" in msg and msg["delta"]:
    65	            got_delta = True
    66	            print(str(msg["delta"]), end="", flush=True)
    67	
    68	        # Fallback auf Ganztext, wenn kein delta-Feld benutzt wird
    69	        if not got_delta and msg.get("content"):
    70	            print(str(msg["content"]), end="", flush=True)
    71	
    72	        if msg.get("error"):
    73	            print(f"\n[error] {msg['error']}\n", flush=True)
    74	
    75	        if msg.get("event") == "done":
    76	            print("\n— done —\n", flush=True)
    77	            break
    78	
    79	async def _send(ws, chat_id: str, text: str):
    80	    payload = {
    81	        "chat_id": chat_id,
    82	        "input": text,
    83	    }
    84	    # Wichtig: Server explizit um Token-Streaming bitten
    85	    if FORCE_STREAM:
    86	        payload["stream"] = True
    87	        payload["emit_delta"] = True  # einige Backends nutzen diesen Schlüssel
    88	
    89	    await ws.send(json.dumps(payload))
    90	    await _drain(ws)
    91	
    92	async def main():
    93	    uri = WS_URL or (f"{WS_BASE}{WS_PATH}?token={TOKEN}" if TOKEN else f"{WS_BASE}{WS_PATH}")
    94	    headers = [("Origin", WS_ORIGIN)]
    95	    if TOKEN:
    96	        headers.append(("Authorization", f"Bearer {TOKEN}"))
    97	
    98	    async with websockets.connect(uri, **_connect_kwargs(headers)) as ws:
    99	        for chat_id, text in PROMPTS:
   100	            await _send(ws, chat_id, text)
   101	
   102	if __name__ == "__main__":
   103	    asyncio.run(main())


===== FILE: frontend/src/app/api/ai/chat/stream/route.ts =====
     1	/**
     2	 * Edge-Proxy für SSE zum FastAPI-Backend.
     3	 * Korrigiert auf /api/v1/langgraph/chat/stream2.
     4	 */
     5	import { NextRequest } from "next/server";
     6	
     7	export const runtime = "edge";
     8	
     9	const BASE = (process.env.BACKEND_URL ||
    10	  process.env.NEXT_PUBLIC_BACKEND_URL ||
    11	  "http://localhost:8000").replace(/\/$/, "");
    12	
    13	export async function POST(req: NextRequest) {
    14	  const body = await req.text();
    15	
    16	  const headers = new Headers({
    17	    "Content-Type": "application/json",
    18	    Accept: "text/event-stream",
    19	  });
    20	  const auth = req.headers.get("authorization");
    21	  if (auth) headers.set("authorization", auth);
    22	
    23	  const backendResp = await fetch(`${BASE}/api/v1/langgraph/chat/stream2`, {
    24	    method: "POST",
    25	    headers,
    26	    body,
    27	  });
    28	
    29	  return new Response(backendResp.body, {
    30	    status: backendResp.status,
    31	    headers: {
    32	      "Content-Type": "text/event-stream",
    33	      "Cache-Control": "no-cache, no-transform, no-store",
    34	      Connection: "keep-alive",
    35	      "X-Accel-Buffering": "no",
    36	    },
    37	  });
    38	}


===== FILE: frontend/src/app/api/auth/custom-logout/route.ts =====
     1	import { NextRequest, NextResponse } from "next/server";
     2	
     3	export async function GET(req: NextRequest) {
     4	  // Legacy-Route -> auf neue SSO-Logout-Route umleiten
     5	  const base = process.env.NEXTAUTH_URL || req.nextUrl.origin;
     6	  return NextResponse.redirect(`${base}/api/auth/sso-logout`);
     7	}
     8	
     9	export const dynamic = "force-dynamic";


===== FILE: frontend/src/app/api/auth/[...nextauth]/route.ts =====
     1	import NextAuth, { type NextAuthOptions } from "next-auth";
     2	import KeycloakProvider from "next-auth/providers/keycloak";
     3	
     4	function must(v: string | undefined, fallback: string): string {
     5	  const s = (v ?? "").trim().replace(/\/+$/, "");
     6	  if (!s || s === "http://localhost" || s === "https://localhost") return fallback;
     7	  return s;
     8	}
     9	
    10	const ISSUER = must(process.env.KEYCLOAK_ISSUER, "https://auth.sealai.net/realms/sealAI");
    11	const NEXTAUTH_URL = must(process.env.NEXTAUTH_URL, "https://sealai.net");
    12	
    13	async function refreshAccessToken(token: any) {
    14	  try {
    15	    const url = `${ISSUER}/protocol/openid-connect/token`;
    16	    const form = new URLSearchParams({
    17	      grant_type: "refresh_token",
    18	      refresh_token: token.refreshToken,
    19	      client_id: process.env.KEYCLOAK_CLIENT_ID as string,
    20	    });
    21	    if (process.env.KEYCLOAK_CLIENT_SECRET) form.set("client_secret", process.env.KEYCLOAK_CLIENT_SECRET);
    22	    const res = await fetch(url, { method: "POST", headers: { "Content-Type": "application/x-www-form-urlencoded" }, body: form });
    23	    const refreshed = await res.json();
    24	    if (!res.ok) throw new Error(refreshed?.error_description || "refresh_failed");
    25	    return {
    26	      ...token,
    27	      accessToken: refreshed.access_token,
    28	      accessTokenExpires: Date.now() + (refreshed.expires_in ?? 60) * 1000,
    29	      refreshToken: refreshed.refresh_token ?? token.refreshToken,
    30	      idToken: refreshed.id_token ?? token.idToken,
    31	      error: undefined,
    32	    };
    33	  } catch {
    34	    return { ...token, error: "RefreshAccessTokenError" };
    35	  }
    36	}
    37	
    38	const authOptions: NextAuthOptions = {
    39	  secret: process.env.NEXTAUTH_SECRET,
    40	  session: { strategy: "jwt", maxAge: 60 * 60 * 24 },
    41	  providers: [
    42	    KeycloakProvider({
    43	      issuer: ISSUER,
    44	      clientId: process.env.KEYCLOAK_CLIENT_ID!,
    45	      clientSecret: process.env.KEYCLOAK_CLIENT_SECRET!,
    46	      authorization: { params: { scope: "openid email profile" } },
    47	      profile(p) {
    48	        return {
    49	          id: p.sub,
    50	          name: p.name || `${p.given_name ?? ""} ${p.family_name ?? ""}`.trim() || p.preferred_username,
    51	          email: p.email,
    52	        };
    53	      },
    54	    }),
    55	  ],
    56	  callbacks: {
    57	    async jwt({ token, account }) {
    58	      if (account) {
    59	        (token as any).accessToken = account.access_token;
    60	        (token as any).accessTokenExpires = account.expires_at ? account.expires_at * 1000 : Date.now() + 60 * 1000;
    61	        (token as any).refreshToken = account.refresh_token;
    62	        (token as any).idToken = (account as any).id_token;
    63	        return token;
    64	      }
    65	      const exp = (token as any).accessTokenExpires as number | undefined;
    66	      if (exp && Date.now() < exp - 30_000) return token;
    67	      return await refreshAccessToken(token);
    68	    },
    69	    async session({ session, token }) {
    70	      (session as any).accessToken = (token as any).accessToken;
    71	      (session as any).accessTokenExpires = (token as any).accessTokenExpires;
    72	      (session as any).idToken = (token as any).idToken;
    73	      (session as any).error = (token as any).error;
    74	      return session;
    75	    },
    76	    async redirect({ url, baseUrl }) {
    77	      try { const u = new URL(url); if (u.origin === baseUrl) return url; } catch {}
    78	      return `${NEXTAUTH_URL}/dashboard`;
    79	    },
    80	  },
    81	  pages: { error: "/auth/error" },
    82	  debug: process.env.NODE_ENV !== "production",
    83	};
    84	
    85	const handler = NextAuth(authOptions);
    86	export { handler as GET, handler as POST };


===== FILE: frontend/src/app/api/auth/sso-logout/route.ts =====
     1	import { NextRequest, NextResponse } from "next/server";
     2	import { getToken } from "next-auth/jwt";
     3	
     4	export const dynamic = "force-dynamic";
     5	
     6	export async function GET(req: NextRequest) {
     7	  try {
     8	    const issuer = (process.env.KEYCLOAK_ISSUER ?? "").replace(/\/$/, "");
     9	    const base = process.env.NEXTAUTH_URL || req.nextUrl.origin;
    10	    const clientId = process.env.KEYCLOAK_CLIENT_ID!;
    11	    if (!issuer || !clientId) throw new Error("Missing KEYCLOAK_ISSUER or KEYCLOAK_CLIENT_ID");
    12	
    13	    const jwt = (await getToken({ req }).catch(() => null)) as any;
    14	    const idToken = jwt?.idToken;
    15	
    16	    // Nach Keycloak-Logout auf Seite leiten, die NextAuth signOut automatisch POSTet
    17	    const postLogout = new URL("/auth/signed-out", base);
    18	
    19	    const url = new URL(`${issuer}/protocol/openid-connect/logout`);
    20	    url.searchParams.set("client_id", clientId);
    21	    url.searchParams.set("post_logout_redirect_uri", postLogout.toString());
    22	    if (idToken) url.searchParams.set("id_token_hint", idToken);
    23	
    24	    return NextResponse.redirect(url.toString());
    25	  } catch (e: any) {
    26	    return NextResponse.json({ error: e?.message || "logout_build_failed" }, { status: 500 });
    27	  }
    28	}
    29	
    30	// Akzeptiere POST ebenfalls
    31	export const POST = GET;


===== FILE: frontend/src/app/api/ccx/jobs/[jobId]/events/route.js =====
     1	export const dynamic = "force-dynamic";
     2	
     3	export async function GET() {
     4	  const enc = new TextEncoder();
     5	  const stream = new ReadableStream({
     6	    start(controller) {
     7	      const send = (obj) => controller.enqueue(enc.encode(`data: ${JSON.stringify(obj)}\n\n`));
     8	      send({ status: "running" });
     9	      setTimeout(() => { send({ status: "finished", converged: true }); controller.close(); }, 500);
    10	    }
    11	  });
    12	  return new Response(stream, {
    13	    headers: {
    14	      "Content-Type": "text/event-stream",
    15	      "Cache-Control": "no-cache, no-transform",
    16	      "Connection": "keep-alive"
    17	    }
    18	  });
    19	}


===== FILE: frontend/src/app/api/ccx/jobs/[jobId]/summary/route.js =====
     1	import fs from "node:fs";
     2	import path from "node:path";
     3	
     4	export const dynamic = "force-dynamic";
     5	
     6	function fileUrlIfExists(dir, jobId, ext) {
     7	  const p = path.join(dir, `${jobId}.${ext}`);
     8	  return fs.existsSync(p) ? `/files/${jobId}.${ext}` : undefined;
     9	}
    10	
    11	function tailLines(p, n = 50) {
    12	  try {
    13	    const txt = fs.readFileSync(p, "utf8");
    14	    const lines = txt.split(/\r?\n/).filter(Boolean);
    15	    return lines.slice(-n);
    16	  } catch {
    17	    return [];
    18	  }
    19	}
    20	
    21	export async function GET(_req, { params }) {
    22	  const jobId = params?.jobId ?? "unknown";
    23	  const filesDir = process.env.JOB_FILES_DIR
    24	    ? path.resolve(process.env.JOB_FILES_DIR)
    25	    : path.join(process.cwd(), "public", "files");
    26	
    27	  const datP = path.join(filesDir, `${jobId}.dat`);
    28	  const frdP = path.join(filesDir, `${jobId}.frd`);
    29	  const msgP = path.join(filesDir, `${jobId}.msg`);
    30	
    31	  const logTail =
    32	    tailLines(msgP, 50).length ? tailLines(msgP, 50)
    33	    : tailLines(datP, 50).length ? tailLines(datP, 50)
    34	    : tailLines(frdP, 50).length ? tailLines(frdP, 50)
    35	    : ["kein Log verfügbar"];
    36	
    37	  const body = {
    38	    jobId,
    39	    jobName: jobId,
    40	    version: "2.22",
    41	    status: "finished",
    42	    runtimeSec: 0.01,
    43	    converged: true,
    44	    iterations: 1,
    45	    lastUpdated: new Date().toISOString(),
    46	    files: {
    47	      dat: fileUrlIfExists(filesDir, jobId, "dat"),
    48	      frd: fileUrlIfExists(filesDir, jobId, "frd"),
    49	      vtu: fileUrlIfExists(filesDir, jobId, "vtu"),
    50	    },
    51	    logTail,
    52	  };
    53	
    54	  return new Response(JSON.stringify(body), {
    55	    headers: { "Content-Type": "application/json" },
    56	  });
    57	}


===== FILE: frontend/src/app/api/ccx/jobs/route.js =====
     1	import fs from "node:fs";
     2	import path from "node:path";
     3	
     4	export const dynamic = "force-dynamic";
     5	
     6	/**
     7	 * Listet vorhandene CCX-Jobs aus JOB_FILES_DIR oder ./public/files.
     8	 * Ein Job gilt als finished, wenn .frd existiert.
     9	 */
    10	export async function GET() {
    11	  const filesDir = process.env.JOB_FILES_DIR
    12	    ? path.resolve(process.env.JOB_FILES_DIR)
    13	    : path.join(process.cwd(), "public", "files");
    14	
    15	  let entries = [];
    16	  try {
    17	    entries = fs.readdirSync(filesDir);
    18	  } catch {
    19	    return new Response(JSON.stringify({ jobs: [] }), {
    20	      headers: { "Content-Type": "application/json" },
    21	    });
    22	  }
    23	
    24	  const jobIds = new Set(
    25	    entries
    26	      .map((f) => /^(.+)\.(dat|frd|vtu|msg)$/i.exec(f)?.[1])
    27	      .filter(Boolean)
    28	  );
    29	
    30	  const jobs = Array.from(jobIds).map((jobId) => {
    31	    const p = (ext) => path.join(filesDir, `${jobId}.${ext}`);
    32	    const has = (ext) => fs.existsSync(p(ext));
    33	    const mtimes = ["msg", "dat", "frd", "vtu"]
    34	      .filter(has)
    35	      .map((ext) => fs.statSync(p(ext)).mtimeMs);
    36	    const lastUpdated =
    37	      mtimes.length ? new Date(Math.max(...mtimes)).toISOString() : null;
    38	
    39	    return {
    40	      jobId,
    41	      status: has("frd") ? "finished" : has("dat") ? "running" : "queued",
    42	      lastUpdated,
    43	      files: {
    44	        dat: has("dat") ? `/files/${jobId}.dat` : undefined,
    45	        frd: has("frd") ? `/files/${jobId}.frd` : undefined,
    46	        vtu: has("vtu") ? `/files/${jobId}.vtu` : undefined,
    47	      },
    48	    };
    49	  });
    50	
    51	  return new Response(JSON.stringify({ jobs }), {
    52	    headers: { "Content-Type": "application/json" },
    53	  });
    54	}


===== FILE: frontend/src/app/api/langgraph/chat/[conversationId]/chat_stream/route.ts =====
     1	export const runtime = "edge";
     2	export const dynamic = "force-dynamic";
     3	
     4	export async function POST(request: Request, context: any) {
     5	  const conversationId: string | undefined = context?.params?.conversationId;
     6	  if (!conversationId) {
     7	    return new Response(JSON.stringify({ error: "Missing conversationId" }), {
     8	      status: 400,
     9	      headers: { "Content-Type": "application/json", "Cache-Control": "no-store" },
    10	    });
    11	  }
    12	
    13	  const authHeader = request.headers.get("authorization");
    14	  const token = authHeader?.split(" ")[1];
    15	  if (!token) {
    16	    return new Response(JSON.stringify({ error: "Unauthorized – token missing" }), {
    17	      status: 401,
    18	      headers: { "Content-Type": "application/json", "Cache-Control": "no-store" },
    19	    });
    20	  }
    21	
    22	  const body = await request.text();
    23	
    24	  const base =
    25	    (process.env.BACKEND_URL ||
    26	      process.env.NEXT_PUBLIC_BACKEND_URL ||
    27	      "http://localhost:8000").replace(/\/$/, "");
    28	
    29	  const backendUrl = `${base}/api/v1/langgraph/chat/${conversationId}/chat_stream`;
    30	
    31	  const backendRes = await fetch(backendUrl, {
    32	    method: "POST",
    33	    headers: {
    34	      "Content-Type": "application/json",
    35	      Accept: "text/event-stream",
    36	      Authorization: `Bearer ${token}`,
    37	    },
    38	    body,
    39	  });
    40	
    41	  return new Response(backendRes.body, {
    42	    status: backendRes.status,
    43	    headers: {
    44	      "Content-Type": "text/event-stream",
    45	      "Cache-Control": "no-cache, no-transform, no-store",
    46	      Connection: "keep-alive",
    47	      "X-Accel-Buffering": "no",
    48	    },
    49	  });
    50	}


===== FILE: frontend/src/app/api/langgraph/chat/route.ts =====
     1	import { NextRequest } from "next/server";
     2	import { getToken } from "next-auth/jwt";
     3	
     4	export const dynamic = "force-dynamic";
     5	
     6	export async function POST(req: NextRequest) {
     7	  const authHeader = req.headers.get("authorization");
     8	  let accessToken: string | undefined = undefined;
     9	
    10	  if (authHeader?.startsWith("Bearer ")) {
    11	    accessToken = authHeader.replace("Bearer ", "");
    12	  } else {
    13	    const token = await getToken({ req });
    14	    if (token && typeof token === "object") {
    15	      accessToken = (token as any).accessToken || (token as any).access_token;
    16	    }
    17	  }
    18	
    19	  if (!accessToken) {
    20	    return new Response("Unauthorized", { status: 401, headers: { "Cache-Control": "no-store" } });
    21	  }
    22	
    23	  const json = await req.text();
    24	
    25	  const base =
    26	    (process.env.BACKEND_URL ||
    27	      process.env.NEXT_PUBLIC_BACKEND_URL ||
    28	      "http://localhost:8000").replace(/\/$/, "");
    29	
    30	  const backendRes = await fetch(`${base}/api/v1/langgraph/chat/stream`, {
    31	    method: "POST",
    32	    headers: {
    33	      Authorization: `Bearer ${accessToken}`,
    34	      "Content-Type": "application/json",
    35	      Accept: "text/event-stream",
    36	    },
    37	    body: json,
    38	  });
    39	
    40	  return new Response(backendRes.body, {
    41	    status: backendRes.status,
    42	    headers: {
    43	      "Content-Type": "text/event-stream",
    44	      "Cache-Control": "no-cache, no-transform, no-store",
    45	      Connection: "keep-alive",
    46	      "X-Accel-Buffering": "no",
    47	    },
    48	  });
    49	}


===== FILE: frontend/src/app/auth/error/error-client.tsx =====
     1	'use client'
     2	
     3	import { useSearchParams, useRouter } from 'next/navigation'
     4	
     5	export default function ErrorClient() {
     6	  const params = useSearchParams()
     7	  const router = useRouter()
     8	  const error = params?.get('error') || 'Unbekannter Fehler'
     9	
    10	  return (
    11	    <div className="flex h-screen items-center justify-center bg-gray-100 p-4">
    12	      <div className="max-w-md bg-white rounded shadow-lg p-8 text-center">
    13	        <h1 className="text-2xl font-bold mb-4">Anmeldefehler</h1>
    14	        <p className="mb-4 text-red-600">{error}</p>
    15	        <button
    16	          onClick={() => router.replace('/auth/signin')}
    17	          className="mt-4 bg-blue-600 hover:bg-blue-700 text-white px-4 py-2 rounded"
    18	        >
    19	          Zurück zur Anmeldung
    20	        </button>
    21	      </div>
    22	    </div>
    23	  )
    24	}


===== FILE: frontend/src/app/auth/error/page.tsx =====
     1	// src/app/auth/error/page.tsx
     2	import React, { Suspense } from 'react'
     3	import ErrorClient from './error-client'
     4	
     5	export const dynamic = 'force-dynamic'  // zwingt dynamisches Rendering
     6	
     7	export default function ErrorPage() {
     8	  return (
     9	    <Suspense fallback={<div>Lade Fehlerseite…</div>}>
    10	      <ErrorClient />
    11	    </Suspense>
    12	  )
    13	}


===== FILE: frontend/src/app/auth/signin/page.tsx =====
     1	'use client';
     2	
     3	import { signIn } from 'next-auth/react';
     4	
     5	export default function SignIn() {
     6	  return (
     7	    <div className="flex items-center justify-center min-h-[50vh]">
     8	      <button
     9	        onClick={() => signIn('keycloak', { callbackUrl: '/dashboard' })}
    10	        className="px-6 py-3 rounded bg-blue-600 text-white hover:bg-blue-700"
    11	      >
    12	        Sign in with Keycloak
    13	      </button>
    14	    </div>
    15	  );
    16	}


===== FILE: frontend/src/app/auth/signin/signin-client.tsx =====
     1	'use client'
     2	
     3	import { signIn } from 'next-auth/react'
     4	
     5	export default function SignInButton() {
     6	  return (
     7	    <button
     8	      onClick={() =>
     9	        signIn('keycloak', { callbackUrl: '/dashboard' })  // ← wichtig!
    10	      }
    11	      className="px-6 py-3 rounded bg-blue-600 text-white hover:bg-blue-700"
    12	    >
    13	      <span className="mr-2 inline-block">
    14	        <img src="/keycloak.svg" alt="" width={20} height={20} />
    15	      </span>
    16	      Sign in with Keycloak
    17	    </button>
    18	  )
    19	}


===== FILE: frontend/src/app/components/ui/card.tsx =====
     1	// 📁 frontend/src/app/components/ui/card.tsx
     2	
     3	import * as React from "react";
     4	// vorher: import { cn } from "@lib/utils";
     5	// korrekt mit dem Slash nach @:
     6	import { cn } from "@/lib/utils";
     7	
     8	const Card = React.forwardRef<
     9	  HTMLDivElement,
    10	  React.HTMLAttributes<HTMLDivElement>
    11	>(({ className, ...props }, ref) => (
    12	  <div
    13	    ref={ref}
    14	    className={cn(
    15	      "rounded-lg border border-gray-200 bg-white p-4 shadow-sm dark:border-gray-700 dark:bg-gray-800",
    16	      className
    17	    )}
    18	    {...props}
    19	  />
    20	));
    21	Card.displayName = "Card";
    22	
    23	export { Card };


===== FILE: frontend/src/app/dashboard/ccx/[jobId]/page.tsx =====
     1	import CcxResultCard from "../../components/CcxResultCard";
     2	
     3	export default async function Page({
     4	  params,
     5	}: {
     6	  params: Promise<{ jobId: string }>;
     7	}) {
     8	  const { jobId } = await params;
     9	  return (
    10	    <div className="p-6">
    11	      <CcxResultCard jobId={jobId} />
    12	    </div>
    13	  );
    14	}


===== FILE: frontend/src/app/dashboard/ccx/page.tsx =====
     1	'use client';
     2	import Link from "next/link";
     3	import { useEffect, useState } from "react";
     4	
     5	type Job = {
     6	  jobId: string;
     7	  status: "queued" | "running" | "finished" | "error";
     8	  lastUpdated?: string | null;
     9	  files: { dat?: string; frd?: string; vtu?: string };
    10	};
    11	type JobsResp = { jobs: Job[] };
    12	
    13	export default function Page() {
    14	  const [jobs, setJobs] = useState<Job[]>([]);
    15	  const [err, setErr] = useState<string | null>(null);
    16	  const [loading, setLoading] = useState(true);
    17	
    18	  useEffect(() => {
    19	    (async () => {
    20	      try {
    21	        setLoading(true);
    22	        const res = await fetch("/api/ccx/jobs", { cache: "no-store" });
    23	        if (!res.ok) throw new Error(`HTTP ${res.status}`);
    24	        const { jobs }: JobsResp = await res.json();
    25	        setJobs(jobs);
    26	      } catch (e: any) {
    27	        setErr(e?.message ?? "Fetch error");
    28	      } finally {
    29	        setLoading(false);
    30	      }
    31	    })();
    32	  }, []);
    33	
    34	  if (err) return <div className="p-6 text-sm text-red-600">Fehler: {err}</div>;
    35	
    36	  return (
    37	    <div className="p-6 space-y-4">
    38	      <h1 className="text-xl font-semibold">CalculiX Jobs</h1>
    39	      {loading ? (
    40	        <div className="text-sm text-gray-500">lädt…</div>
    41	      ) : (
    42	        <ul className="divide-y divide-gray-200 rounded-xl border border-gray-200 bg-white">
    43	          {jobs.length === 0 && (
    44	            <li className="p-4 text-sm text-gray-500">Keine Jobs gefunden</li>
    45	          )}
    46	          {jobs.map((j) => (
    47	            <li key={j.jobId} className="p-4 flex items-center justify-between">
    48	              <div className="space-y-1">
    49	                <div className="font-medium">{j.jobId}</div>
    50	                <div className="text-xs text-gray-500">
    51	                  Status: {j.status}
    52	                  {j.lastUpdated ? ` · ${new Date(j.lastUpdated).toLocaleString()}` : ""}
    53	                </div>
    54	              </div>
    55	              <div className="flex items-center gap-2">
    56	                {j.files.dat && <a className="text-sm underline" href={j.files.dat}>.dat</a>}
    57	                {j.files.frd && <a className="text-sm underline" href={j.files.frd}>.frd</a>}
    58	                {j.files.vtu && <a className="text-sm underline" href={j.files.vtu}>.vtu</a>}
    59	                <Link className="text-sm px-3 py-1 rounded-lg border" href={`/dashboard/ccx/${encodeURIComponent(j.jobId)}`}>Details</Link>
    60	              </div>
    61	            </li>
    62	          ))}
    63	        </ul>
    64	      )}
    65	    </div>
    66	  );
    67	}


===== FILE: frontend/src/app/dashboard/ChatScreen.tsx =====
     1	'use client';
     2	import Chat from './components/Chat/ChatContainer';
     3	
     4	export default function ChatScreen() {
     5	  return <Chat />;
     6	}


===== FILE: frontend/src/app/dashboard/components/CcxResultCard.tsx =====
     1	'use client';
     2	import React, { useEffect, useMemo, useState } from "react";
     3	
     4	type CcxSummary = {
     5	  jobId: string;
     6	  jobName: string;
     7	  version?: string;
     8	  status: "queued" | "running" | "finished" | "error";
     9	  runtimeSec?: number;
    10	  converged?: boolean;
    11	  iterations?: number;
    12	  lastUpdated?: string; // ISO-8601
    13	  files?: { dat?: string; frd?: string; vtu?: string };
    14	  logTail?: string[]; // last N lines
    15	};
    16	
    17	function fmtSec(s?: number): string {
    18	  if (s === undefined || s === null) return "–";
    19	  if (s < 60) return `${s.toFixed(2)} s`;
    20	  const m = Math.floor(s / 60);
    21	  const r = s - m * 60;
    22	  return `${m}m ${r.toFixed(1)}s`;
    23	}
    24	
    25	function Pill({
    26	  label,
    27	  tone,
    28	}: {
    29	  label: string;
    30	  tone: "ok" | "warn" | "err" | "muted";
    31	}) {
    32	  const map = {
    33	    ok: "bg-green-100 text-green-700",
    34	    warn: "bg-amber-100 text-amber-700",
    35	    err: "bg-red-100 text-red-700",
    36	    muted: "bg-gray-100 text-gray-600",
    37	  } as const;
    38	  return (
    39	    <span className={`px-2 py-0.5 rounded-full text-xs font-medium ${map[tone]}`}>
    40	      {label}
    41	    </span>
    42	  );
    43	}
    44	
    45	export default function CcxResultCard({ jobId }: { jobId: string }) {
    46	  const [data, setData] = useState<CcxSummary | null>(null);
    47	  const [loading, setLoading] = useState(true);
    48	  const [err, setErr] = useState<string | null>(null);
    49	  const [showLog, setShowLog] = useState(false);
    50	
    51	  async function load() {
    52	    try {
    53	      setLoading(true);
    54	      setErr(null);
    55	      const res = await fetch(
    56	        `/api/ccx/jobs/${encodeURIComponent(jobId)}/summary`,
    57	        { cache: "no-store" }
    58	      );
    59	      if (!res.ok) throw new Error(`HTTP ${res.status}`);
    60	      const j = (await res.json()) as CcxSummary;
    61	      setData(j);
    62	    } catch (e: any) {
    63	      setErr(e?.message ?? "Fetch error");
    64	    } finally {
    65	      setLoading(false);
    66	    }
    67	  }
    68	
    69	  useEffect(() => {
    70	    load();
    71	    const es = new EventSource(
    72	      `/api/ccx/jobs/${encodeURIComponent(jobId)}/events`
    73	    );
    74	    es.onmessage = (ev) => {
    75	      try {
    76	        const patch = JSON.parse(ev.data) as Partial<CcxSummary>;
    77	        setData((prev) => ({ ...(prev ?? ({} as CcxSummary)), ...patch }));
    78	      } catch {
    79	        /* ignore */
    80	      }
    81	    };
    82	    es.onerror = () => {
    83	      /* auto-retry by browser */
    84	    };
    85	    return () => es.close();
    86	  }, [jobId]);
    87	
    88	  const statusPill = useMemo(() => {
    89	    if (!data) return <Pill label="lädt…" tone="muted" />;
    90	    const map: Record<CcxSummary["status"], JSX.Element> = {
    91	      queued: <Pill label="Wartend" tone="muted" />,
    92	      running: <Pill label="Läuft" tone="warn" />,
    93	      finished: (
    94	        <Pill
    95	          label={data.converged ? "Fertig · konvergiert" : "Fertig"}
    96	          tone={data.converged ? "ok" : "muted"}
    97	        />
    98	      ),
    99	      error: <Pill label="Fehler" tone="err" />,
   100	    };
   101	    return map[data.status];
   102	  }, [data]);
   103	
   104	  const files = data?.files ?? {};
   105	
   106	  return (
   107	    <div className="rounded-2xl border border-gray-200 p-4 shadow-sm bg-white">
   108	      <div className="flex items-start justify-between gap-3">
   109	        <div>
   110	          <h3 className="text-sm text-gray-500">CalculiX Ergebnis</h3>
   111	          <div className="mt-0.5 text-lg font-semibold">
   112	            {data?.jobName ?? jobId}
   113	          </div>
   114	        </div>
   115	        {statusPill}
   116	      </div>
   117	
   118	      <div className="mt-3 grid grid-cols-2 sm:grid-cols-4 gap-3 text-sm">
   119	        <div className="p-2 rounded-xl bg-gray-50">
   120	          <div className="text-gray-500">Version</div>
   121	          <div className="font-medium">{data?.version ?? "–"}</div>
   122	        </div>
   123	        <div className="p-2 rounded-xl bg-gray-50">
   124	          <div className="text-gray-500">Laufzeit</div>
   125	          <div className="font-medium">{fmtSec(data?.runtimeSec)}</div>
   126	        </div>
   127	        <div className="p-2 rounded-xl bg-gray-50">
   128	          <div className="text-gray-500">Iterationen</div>
   129	          <div className="font-medium">{data?.iterations ?? "–"}</div>
   130	        </div>
   131	        <div className="p-2 rounded-xl bg-gray-50">
   132	          <div className="text-gray-500">Aktualisiert</div>
   133	          <div className="font-medium">
   134	            {data?.lastUpdated
   135	              ? new Date(data.lastUpdated).toLocaleString()
   136	              : "–"}
   137	          </div>
   138	        </div>
   139	      </div>
   140	
   141	      <div className="mt-4 flex flex-wrap gap-2">
   142	        <a
   143	          className={`px-3 py-1.5 rounded-lg text-sm ${
   144	            files.dat
   145	              ? "border hover:bg-gray-50"
   146	              : "border-dashed border text-gray-400 cursor-not-allowed"
   147	          }`}
   148	          href={files.dat ?? "#"}
   149	          onClick={(e) => {
   150	            if (!files.dat) e.preventDefault();
   151	          }}
   152	        >
   153	          {files.dat ? "Download .dat" : "kein .dat"}
   154	        </a>
   155	        <a
   156	          className={`px-3 py-1.5 rounded-lg text-sm ${
   157	            files.frd
   158	              ? "border hover:bg-gray-50"
   159	              : "border-dashed border text-gray-400 cursor-not-allowed"
   160	          }`}
   161	          href={files.frd ?? "#"}
   162	          onClick={(e) => {
   163	            if (!files.frd) e.preventDefault();
   164	          }}
   165	        >
   166	          {files.frd ? "Download .frd" : "kein .frd"}
   167	        </a>
   168	        <a
   169	          className={`px-3 py-1.5 rounded-lg text-sm ${
   170	            files.vtu
   171	              ? "border hover:bg-gray-50"
   172	              : "border-dashed border text-gray-400 cursor-not-allowed"
   173	          }`}
   174	          href={files.vtu ?? "#"}
   175	          onClick={(e) => {
   176	            if (!files.vtu) e.preventDefault();
   177	          }}
   178	        >
   179	          {files.vtu ? "Download .vtu" : "kein .vtu (Export nötig)"}
   180	        </a>
   181	      </div>
   182	
   183	      <button
   184	        onClick={() => setShowLog((v) => !v)}
   185	        className="mt-4 text-xs text-gray-600 underline"
   186	        type="button"
   187	      >
   188	        {showLog ? "Log ausblenden" : "Log einblenden"}
   189	      </button>
   190	
   191	      {showLog && (
   192	        <pre className="mt-2 max-h-48 overflow-auto text-xs bg-black text-green-200 p-3 rounded-xl">
   193	{(data?.logTail ??
   194	  (loading ? ["lade…"] : err ? [err] : ["kein Log verfügbar"])
   195	).join("\n")}
   196	        </pre>
   197	      )}
   198	    </div>
   199	  );
   200	}


===== FILE: frontend/src/app/dashboard/components/Chat/ChatContainer.tsx =====
     1	"use client";
     2	
     3	import { useSession } from "next-auth/react";
     4	import { useEffect, useRef, useState } from "react";
     5	import ChatHistory from "./ChatHistory";
     6	import ChatInput from "./ChatInput";
     7	import type { Message } from "@/types/chat";
     8	import { useChatWs } from "@/lib/useChatWs";
     9	
    10	export default function ChatContainer() {
    11	  const { data: session, status } = useSession();
    12	  const isAuthed = status === "authenticated";
    13	
    14	  const chatId = "default";
    15	  const { connected, streaming, text, lastError, send, cancel } = useChatWs({ chatId });
    16	
    17	  const [messages, setMessages] = useState<Message[]>([]);
    18	  const [inputValue, setInputValue] = useState("");
    19	  const [hasStarted, setHasStarted] = useState(false);
    20	
    21	  const endRef = useRef<HTMLDivElement>(null);
    22	  useEffect(() => {
    23	    endRef.current?.scrollIntoView({ behavior: "smooth", block: "nearest" });
    24	  }, [messages, text, streaming]);
    25	
    26	  // Stream in letzte Assistant-Message mergen
    27	  useEffect(() => {
    28	    if (!streaming && !text) return;
    29	    setMessages((prev) => {
    30	      const next = [...prev];
    31	      const lastIdx = next.length - 1;
    32	      if (lastIdx >= 0 && next[lastIdx].role === "assistant") {
    33	        next[lastIdx] = { ...next[lastIdx], content: text };
    34	        return next;
    35	      }
    36	      return [...next, { role: "assistant", content: text }];
    37	    });
    38	  }, [text, streaming]);
    39	
    40	  // Externe Chat-Add-Events (z. B. Formular-Übernahme)
    41	  useEffect(() => {
    42	    const onAdd = (ev: Event) => {
    43	      const detail: any = (ev as CustomEvent).detail;
    44	      const t = (detail?.text ?? detail ?? "").toString().trim();
    45	      if (!t) return;
    46	      setMessages((m) => [...m, { role: "user", content: t }]);
    47	    };
    48	    window.addEventListener("sealai:chat:add", onAdd as EventListener);
    49	    return () => window.removeEventListener("sealai:chat:add", onAdd as EventListener);
    50	  }, []);
    51	
    52	  const firstName = session?.user?.name?.split(" ")[0];
    53	  const sendingDisabled = !isAuthed || !connected;
    54	  const isInitial = messages.length === 0 && !hasStarted;
    55	
    56	  const handleSend = (msg: string) => {
    57	    if (sendingDisabled) return;
    58	    const content = msg.trim();
    59	    if (!content) return;
    60	    setMessages((m) => [...m, { role: "user", content }]);
    61	    setHasStarted(true);
    62	    send(content);
    63	    setInputValue("");
    64	  };
    65	
    66	  const handleStop = () => cancel();
    67	
    68	  if (isInitial) {
    69	    return (
    70	      <div className="flex min-h-[80vh] w-full items-center justify-center">
    71	        <div className="w-full max-w-[768px] px-4">
    72	          <div className="text-2xl md:text-3xl font-bold text-gray-800 text-center leading-tight select-none">
    73	            Willkommen zurück{firstName ? `, ${firstName}` : ""}!
    74	          </div>
    75	          <div className="text-base md:text-lg text-gray-500 mb-3 text-center leading-snug font-medium select-none">
    76	            Schön, dass du hier bist.
    77	          </div>
    78	          <div className="text-xs text-center mb-4">
    79	            {isAuthed ? (
    80	              connected ? (
    81	                <span className="text-emerald-600">WebSocket verbunden</span>
    82	              ) : (
    83	                <span className="text-amber-600">Verbinde WebSocket…</span>
    84	              )
    85	            ) : (
    86	              <span className="text-gray-500">Bitte anmelden</span>
    87	            )}
    88	          </div>
    89	
    90	          <ChatInput
    91	            value={inputValue}
    92	            setValue={setInputValue}
    93	            onSend={handleSend}
    94	            onStop={handleStop}
    95	            disabled={sendingDisabled}
    96	            streaming={streaming}
    97	            placeholder={
    98	              isAuthed ? (connected ? "Was möchtest du wissen?" : "Verbinde…") : "Bitte anmelden, um zu schreiben"
    99	            }
   100	          />
   101	
   102	          {!isAuthed && (
   103	            <div className="mt-2 text-xs text-gray-500 text-center">
   104	              Du musst angemeldet sein, um Nachrichten zu senden.
   105	            </div>
   106	          )}
   107	          {lastError && (
   108	            <div className="mt-2 text-xs text-red-500 text-center select-none">
   109	              Fehler: {lastError}
   110	            </div>
   111	          )}
   112	        </div>
   113	      </div>
   114	    );
   115	  }
   116	
   117	  return (
   118	    <div className="flex flex-col h-full w-full bg-transparent relative">
   119	      <div className="flex-1 overflow-y-auto w-full pb-36" style={{ minHeight: 0 }}>
   120	        <ChatHistory messages={messages} />
   121	        <div ref={endRef} />
   122	      </div>
   123	
   124	      <div className="sticky bottom-0 left-0 right-0 z-20 flex justify-center bg-transparent pb-0 w-full">
   125	        <div className="w-full max-w-[768px] pointer-events-auto">
   126	          <ChatInput
   127	            value={inputValue}
   128	            setValue={setInputValue}
   129	            onSend={handleSend}
   130	            onStop={handleStop}
   131	            disabled={sendingDisabled}
   132	            streaming={streaming}
   133	            placeholder={
   134	              isAuthed ? (connected ? "Was möchtest du wissen?" : "Verbinde…") : "Bitte anmelden, um zu schreiben"
   135	            }
   136	          />
   137	          {!isAuthed && (
   138	            <div className="mt-2 text-xs text-gray-500">
   139	              Du musst angemeldet sein, um Nachrichten zu senden.
   140	            </div>
   141	          )}
   142	          {lastError && (
   143	            <div className="mt-2 text-xs text-red-500 select-none">
   144	              Fehler: {lastError}
   145	            </div>
   146	          )}
   147	        </div>
   148	      </div>
   149	    </div>
   150	  );
   151	}


===== FILE: frontend/src/app/dashboard/components/Chat/ChatHistory.tsx =====
     1	'use client';
     2	
     3	import MarkdownMessage from './MarkdownMessage';
     4	import type { Message } from '@/types/chat';
     5	
     6	/**
     7	 * Kompaktere, grok/ChatGPT-nahe Bubble-Abstände.
     8	 */
     9	export default function ChatHistory({ messages }: { messages: Message[] }) {
    10	  return (
    11	    <div className="flex flex-col gap-5 w-full max-w-[720px] mx-auto pb-3 scroll-mt-[64px]">
    12	      {messages.map((m, i) => {
    13	        const isUser = m.role === 'user';
    14	        const isSystem = m.role === 'system';
    15	
    16	        return (
    17	          <div
    18	            key={i}
    19	            className={isUser ? 'flex justify-end' : 'flex justify-start'}
    20	          >
    21	            <div
    22	              className={[
    23	                'px-3 py-2 leading-[1.45] transition-all duration-150',
    24	                isUser
    25	                  ? 'max-w-[66%] bg-[#f6f8fc] rounded-2xl border border-gray-200 shadow-sm'
    26	                  : 'w-full',
    27	                isSystem ? 'opacity-80 italic' : '',
    28	              ].join(' ')}
    29	            >
    30	              <MarkdownMessage isUser={isUser} isTool={false}>
    31	                {m.content}
    32	              </MarkdownMessage>
    33	            </div>
    34	          </div>
    35	        );
    36	      })}
    37	      <div id="chat-end" />
    38	    </div>
    39	  );
    40	}


===== FILE: frontend/src/app/dashboard/components/Chat/ChatInput.tsx =====
     1	'use client';
     2	
     3	import React, { useRef, useEffect, useCallback } from 'react';
     4	
     5	interface ChatInputProps {
     6	  value: string;
     7	  setValue: (v: string) => void;
     8	  onSend?: (value: string) => void;
     9	  onStop?: () => void;
    10	  /** Bedeutet: Senden-Button sperren – NICHT das Tippen */
    11	  disabled?: boolean;
    12	  streaming?: boolean;
    13	  placeholder?: string;
    14	}
    15	
    16	/**
    17	 * ChatInput – tippen immer möglich, auch offline.
    18	 * Nur Senden/Stop werden je nach Status deaktiviert.
    19	 */
    20	export default function ChatInput({
    21	  value,
    22	  setValue,
    23	  onSend,
    24	  onStop,
    25	  disabled = false,   // -> sperrt NUR Buttons
    26	  streaming = false,  // -> sperrt Textarea (während Stream)
    27	  placeholder = 'Was möchtest du wissen?',
    28	}: ChatInputProps) {
    29	  const textareaRef = useRef<HTMLTextAreaElement>(null);
    30	
    31	  // --- Autosize Textarea, max 4 Zeilen (~104px) ---
    32	  const autosize = useCallback(() => {
    33	    const el = textareaRef.current;
    34	    if (!el) return;
    35	    el.style.height = 'auto';
    36	    const next = Math.min(el.scrollHeight, 104);
    37	    el.style.height = `${next}px`;
    38	  }, []);
    39	
    40	  useEffect(() => {
    41	    autosize();
    42	  }, [value, autosize]);
    43	
    44	  const focusTextarea = useCallback(() => {
    45	    requestAnimationFrame(() => textareaRef.current?.focus());
    46	  }, []);
    47	
    48	  const doSend = useCallback(() => {
    49	    const text = value.trim();
    50	    if (!onSend || !text) return;
    51	    onSend(text);
    52	    setValue('');
    53	    focusTextarea();
    54	  }, [onSend, setValue, value, focusTextarea]);
    55	
    56	  const doStop = useCallback(() => {
    57	    if (onStop) onStop();
    58	    focusTextarea();
    59	  }, [onStop, focusTextarea]);
    60	
    61	  const handleKeyDown = (e: React.KeyboardEvent) => {
    62	    // Während Streaming nicht senden
    63	    if (streaming) return;
    64	
    65	    // Shift+Enter = Zeilenumbruch
    66	    if (e.key === 'Enter' && e.shiftKey) return;
    67	
    68	    // Enter / Ctrl+Enter senden – aber nur, wenn Buttons nicht gesperrt
    69	    if ((e.key === 'Enter' && !e.shiftKey) || (e.key === 'Enter' && (e.ctrlKey || e.metaKey))) {
    70	      e.preventDefault();
    71	      if (!disabled) doSend();
    72	    }
    73	  };
    74	
    75	  const canSend = !disabled && !streaming && value.trim().length > 0;
    76	  const canStop = !disabled && streaming;
    77	
    78	  return (
    79	    <div
    80	      className="flex flex-col w-full items-center"
    81	      style={{ maxWidth: '768px', minWidth: '320px', width: '100%' }}
    82	    >
    83	      <div
    84	        className={[
    85	          'bg-white rounded-3xl',
    86	          'border border-gray-200',
    87	          'shadow-[0_8px_28px_rgba(60,80,120,0.10)]',
    88	          'flex flex-col justify-between',
    89	          'transition-all',
    90	          // kompaktere Innenabstände
    91	          'px-5 pt-4 pb-3',
    92	          streaming ? 'opacity-90' : '',
    93	        ].join(' ')}
    94	        style={{ minHeight: '92px', maxWidth: '768px', width: '100%' }}
    95	      >
    96	        {/* Eingabe (Textarea): nur während Streaming gesperrt */}
    97	        <textarea
    98	          ref={textareaRef}
    99	          className={[
   100	            'w-full resize-none border-none outline-none bg-transparent',
   101	            'text-[0.97rem] leading-[1.5]',
   102	            'text-gray-900 placeholder-gray-400',
   103	            'min-h-[26px] max-h-[104px]',
   104	            'pr-2 pl-2',
   105	            'transition',
   106	            'scrollbar-thin',
   107	            'overflow-y-auto',
   108	            streaming ? 'cursor-not-allowed' : '',
   109	          ].join(' ')}
   110	          rows={1}
   111	          maxLength={3000}
   112	          autoFocus
   113	          value={value}
   114	          disabled={streaming}             // <-- wichtig: NICHT mehr „disabled || streaming“
   115	          placeholder={
   116	            disabled ? 'Offline – du kannst tippen, Senden ist deaktiviert' : placeholder
   117	          }
   118	          onChange={(e) => setValue(e.target.value)}
   119	          onKeyDown={handleKeyDown}
   120	          aria-label="Chat-Eingabe"
   121	          aria-disabled={streaming}
   122	          style={{
   123	            borderRadius: 0,
   124	            fontSize: '0.97rem',
   125	            background: 'transparent',
   126	            minHeight: '26px',
   127	            maxHeight: '104px',
   128	            boxSizing: 'border-box',
   129	            paddingTop: 2,
   130	            paddingBottom: 2,
   131	            paddingLeft: 6,
   132	            paddingRight: 10,
   133	          }}
   134	        />
   135	
   136	        {/* Bottom Row */}
   137	        <div className="flex flex-row justify-between items-center mt-2">
   138	          {/* Platzhalter-Button links */}
   139	          <button
   140	            type="button"
   141	            tabIndex={-1}
   142	            className="flex items-center gap-1 px-3 py-1.5 rounded-full text-[11.5px] bg-gray-100 text-gray-700 font-normal select-none shadow-sm hover:bg-gray-200 transition"
   143	            disabled
   144	            aria-disabled="true"
   145	            title="Kompetenz wählen (Demo)"
   146	          >
   147	            🧑‍💼 Kompetenz wählen [Demo]
   148	          </button>
   149	
   150	          {/* Rechts: Stop- oder Send-Button */}
   151	          {streaming ? (
   152	            <button
   153	              type="button"
   154	              onClick={doStop}
   155	              disabled={!canStop}
   156	              className={[
   157	                'flex items-center justify-center',
   158	                'h-8 px-3 ml-2',
   159	                'rounded-full',
   160	                'shadow',
   161	                'transition',
   162	                canStop
   163	                  ? 'bg-red-500 hover:bg-red-600 text-white'
   164	                  : 'bg-gray-200 text-gray-400 cursor-not-allowed',
   165	              ].join(' ')}
   166	              style={{ zIndex: 20 }}
   167	              aria-label="Stopp"
   168	              title="Stopp"
   169	            >
   170	              <svg xmlns="http://www.w3.org/2000/svg" className="h-[18px] w-[18px]" viewBox="0 0 24 24" fill="currentColor">
   171	                <rect x="6" y="6" width="12" height="12" rx="2" />
   172	              </svg>
   173	            </button>
   174	          ) : (
   175	            <button
   176	              type="button"
   177	              onClick={doSend}
   178	              disabled={!canSend}
   179	              className={[
   180	                'flex items-center justify-center',
   181	                'h-8 w-8 ml-2',
   182	                'rounded-full',
   183	                'shadow',
   184	                'transition',
   185	                canSend
   186	                  ? 'bg-[#343541] hover:bg-[#202123] text-white'
   187	                  : 'bg-gray-200 text-gray-400 cursor-not-allowed',
   188	              ].join(' ')}
   189	              style={{ zIndex: 20 }}
   190	              aria-label="Senden"
   191	              title={disabled ? 'Offline – Senden deaktiviert' : 'Senden (Enter)'}
   192	            >
   193	              <svg xmlns="http://www.w3.org/2000/svg" className="h-[18px] w-[18px]" fill="none" viewBox="0 0 24 24" stroke="currentColor">
   194	                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M5 12h14M12 5l7 7-7 7" />
   195	              </svg>
   196	            </button>
   197	          )}
   198	        </div>
   199	      </div>
   200	
   201	      {/* kleine Shortcut-Hilfe */}
   202	      <div className="mt-1.5 text-[11px] text-gray-500">
   203	        {disabled ? 'Offline – du kannst schon tippen; Senden ist aus.' : 'Enter: senden · Shift+Enter: neue Zeile · Strg/⌘+Enter: senden'}
   204	      </div>
   205	    </div>
   206	  );
   207	}


===== FILE: frontend/src/app/dashboard/components/Chat/MarkdownMessage.tsx =====
     1	'use client';
     2	
     3	import React, { ReactNode, useState } from 'react';
     4	import ReactMarkdown from 'react-markdown';
     5	import remarkGfm from 'remark-gfm';
     6	import rehypeRaw from 'rehype-raw';
     7	import rehypeHighlight from 'rehype-highlight';
     8	
     9	// dezentes Dark-Theme für Code (grok/chatgpt-ähnlich)
    10	import 'highlight.js/styles/github-dark-dimmed.css';
    11	
    12	// Chat-Markdown Styles (angepasst, kompakter)
    13	import '@/styles/chat-markdown.css';
    14	
    15	interface CodeProps {
    16	  inline?: boolean;
    17	  className?: string;
    18	  children?: ReactNode;
    19	}
    20	
    21	function CodeBlock({ inline, className, children, ...rest }: CodeProps) {
    22	  const [copied, setCopied] = useState(false);
    23	
    24	  if (inline) {
    25	    return (
    26	      <code className="cm-inline" {...rest}>
    27	        {children}
    28	      </code>
    29	    );
    30	  }
    31	
    32	  const text =
    33	    typeof children === 'string'
    34	      ? children
    35	      : Array.isArray(children)
    36	      ? children.join('')
    37	      : String(children ?? '');
    38	
    39	  const match = /language-([\w-]+)/.exec(className || '');
    40	  const lang = (match?.[1] || 'text').toLowerCase();
    41	
    42	  const doCopy = async () => {
    43	    try {
    44	      await navigator.clipboard.writeText(text);
    45	      setCopied(true);
    46	      setTimeout(() => setCopied(false), 1100);
    47	    } catch {
    48	      /* noop */
    49	    }
    50	  };
    51	
    52	  return (
    53	    <div className="cm-codeblock">
    54	      <div className="cm-codeblock__toolbar">
    55	        <span className="cm-lang">{lang}</span>
    56	        <button className="cm-copy" onClick={doCopy} type="button">
    57	          {copied ? 'Copied' : 'Copy'}
    58	        </button>
    59	      </div>
    60	      <pre className="cm-pre">
    61	        <code className={`language-${lang}`} {...rest}>
    62	          {text}
    63	        </code>
    64	      </pre>
    65	    </div>
    66	  );
    67	}
    68	
    69	export default function MarkdownMessage({
    70	  children,
    71	  isUser,
    72	  isTool,
    73	}: {
    74	  children: ReactNode;
    75	  isUser?: boolean;
    76	  isTool?: boolean;
    77	}) {
    78	  const content =
    79	    typeof children === 'string'
    80	      ? children
    81	      : Array.isArray(children)
    82	      ? children.filter(Boolean).join('')
    83	      : String(children ?? '');
    84	
    85	  const toneClass = isTool ? 'cm-tool' : isUser ? 'cm-user' : 'cm-assistant';
    86	
    87	  return (
    88	    <div className={`chat-markdown ${toneClass}`} aria-live="polite">
    89	      <ReactMarkdown
    90	        remarkPlugins={[remarkGfm]}
    91	        rehypePlugins={[rehypeRaw, rehypeHighlight]}
    92	        components={{
    93	          h1: (p) => <h1 className="cm-h1" {...p} />,
    94	          h2: (p) => <h2 className="cm-h2" {...p} />,
    95	          h3: (p) => <h3 className="cm-h3" {...p} />,
    96	          h4: (p) => <h4 className="cm-h4" {...p} />,
    97	          p: ({ node, ...props }) => <p className="cm-p" {...props} />,
    98	          a: ({ node, ...props }) => (
    99	            <a className="cm-a" target="_blank" rel="noreferrer" {...props} />
   100	          ),
   101	          ul: (p) => <ul className="cm-ul" {...p} />,
   102	          ol: (p) => <ol className="cm-ol" {...p} />,
   103	          li: (p) => <li className="cm-li" {...p} />,
   104	          blockquote: (p) => <blockquote className="cm-quote" {...p} />,
   105	          hr: () => <hr className="cm-hr" />,
   106	          table: (p) => (
   107	            <div className="cm-tablewrap">
   108	              <table className="cm-table" {...p} />
   109	            </div>
   110	          ),
   111	          thead: (p) => <thead className="cm-thead" {...p} />,
   112	          th: (p) => <th className="cm-th" {...p} />,
   113	          td: (p) => <td className="cm-td" {...p} />,
   114	          img: (p) => <img className="cm-img" {...p} />,
   115	          code: CodeBlock,
   116	        }}
   117	      >
   118	        {content}
   119	      </ReactMarkdown>
   120	    </div>
   121	  );
   122	}


===== FILE: frontend/src/app/dashboard/components/InfoBar.tsx =====
     1	'use client';
     2	
     3	import { useState } from 'react';
     4	import clsx from 'clsx';
     5	import { XMarkIcon, Bars3Icon } from '@heroicons/react/24/solid';
     6	
     7	export default function InfoBar() {
     8	  const [open, setOpen] = useState(false);
     9	
    10	  return (
    11	    <>
    12	      {/* Drawer */}
    13	      <aside
    14	        className={clsx(
    15	          'fixed right-0 top-0 h-full w-72 bg-white shadow-lg z-40',
    16	          'transition-transform duration-300',
    17	          open ? 'translate-x-0' : 'translate-x-full'
    18	        )}
    19	      >
    20	        <div className="flex items-center justify-between p-4 border-b">
    21	          <h3 className="font-semibold">Info</h3>
    22	          <button onClick={() => setOpen(false)}>
    23	            <XMarkIcon className="w-5 h-5 text-gray-500" />
    24	          </button>
    25	        </div>
    26	        <div className="p-4 text-sm leading-relaxed">
    27	          {/* z. B. RAG-Treffer, System-Status, Token-Verbrauch … */}
    28	          Noch keine Inhalte.
    29	        </div>
    30	      </aside>
    31	
    32	      {/* Toggle-FAB */}
    33	      <button
    34	        onClick={() => setOpen(!open)}
    35	        className="fixed bottom-6 right-6 z-50 rounded-full p-3 shadow-lg
    36	                   bg-blue-600 text-white hover:bg-blue-700 transition-colors"
    37	      >
    38	        {open ? <XMarkIcon className="w-6 h-6" /> : <Bars3Icon className="w-6 h-6" />}
    39	      </button>
    40	    </>
    41	  );
    42	}


===== FILE: frontend/src/app/dashboard/components/LogoutButton.tsx =====
     1	"use client";
     2	
     3	export default function LogoutButton() {
     4	  return (
     5	    <div className="fixed top-4 right-4 z-50">
     6	      <button
     7	        onClick={() => window.location.assign("/api/auth/sso-logout")}
     8	        aria-label="Abmelden"
     9	        className="backdrop-blur-sm bg-white/70 hover:bg-white/90 active:bg-white
    10	                   border border-black/10 shadow-sm rounded-full px-3.5 h-8
    11	                   inline-flex items-center gap-2 text-[13px] font-medium text-gray-800 transition"
    12	      >
    13	        <svg width="14" height="14" viewBox="0 0 24 24" fill="none" aria-hidden="true">
    14	          <path d="M12 3v7" stroke="currentColor" strokeWidth="1.8" strokeLinecap="round"/>
    15	          <path d="M6.3 7.5a7.5 7.5 0 1 0 11.4 0" stroke="currentColor" strokeWidth="1.8" strokeLinecap="round"/>
    16	        </svg>
    17	        <span>Abmelden</span>
    18	      </button>
    19	    </div>
    20	  );
    21	}


===== FILE: frontend/src/app/dashboard/components/Sidebar/AccordionTabs.tsx =====
     1	'use client';
     2	
     3	import { useState, ReactNode } from 'react';
     4	import {
     5	  ChevronDown,
     6	  ClipboardList,
     7	  MessageCircle,
     8	  Settings,
     9	} from 'lucide-react';
    10	
    11	/* -------------------------------------------------
    12	   Typdefinition für eine Accordion-Sektion
    13	--------------------------------------------------*/
    14	interface Section {
    15	  id: 'form' | 'history' | 'settings';
    16	  title: string;
    17	  icon: ReactNode;
    18	  content: ReactNode;
    19	}
    20	
    21	/* -------------------------------------------------
    22	   AccordionTabs – vertikale Tabs, die nach unten
    23	   ausklappen. Vollständig animiert mit Tailwind.
    24	--------------------------------------------------*/
    25	export default function AccordionTabs() {
    26	  /* ---------- FIX: null zulassen, damit man alles einklappen kann ---------- */
    27	  const [openId, setOpenId] =
    28	    useState<'form' | 'history' | 'settings' | null>('form');
    29	
    30	  /* ---------- Sektionen definieren ---------- */
    31	  const SECTIONS: Section[] = [
    32	    {
    33	      id: 'form',
    34	      title: 'Formular',
    35	      icon: <ClipboardList className="h-4 w-4" />,
    36	      content: (
    37	        <form className="grid grid-cols-1 md:grid-cols-3 gap-4 py-4">
    38	          <input className="border rounded px-3 py-2" placeholder="Feld A" />
    39	          <input className="border rounded px-3 py-2" placeholder="Feld B" />
    40	          <input className="border rounded px-3 py-2" placeholder="Feld C" />
    41	          <textarea
    42	            className="border rounded px-3 py-2 col-span-full"
    43	            rows={4}
    44	            placeholder="Beschreibung"
    45	          />
    46	          <button className="col-span-full bg-blue-600 text-white py-2 rounded hover:bg-blue-700 transition">
    47	            Speichern
    48	          </button>
    49	        </form>
    50	      ),
    51	    },
    52	    {
    53	      id: 'history',
    54	      title: 'Chat-History',
    55	      icon: <MessageCircle className="h-4 w-4" />,
    56	      content: (
    57	        <div className="py-4 space-y-2 text-sm text-gray-600">
    58	          {/* Hier später echten Verlauf laden */}
    59	          <p>(Noch kein Verlauf geladen …)</p>
    60	        </div>
    61	      ),
    62	    },
    63	    {
    64	      id: 'settings',
    65	      title: 'Einstellungen',
    66	      icon: <Settings className="h-4 w-4" />,
    67	      content: (
    68	        <div className="py-4 space-y-3 text-sm">
    69	          <label className="flex items-center gap-2">
    70	            <input type="checkbox" className="accent-blue-600" />
    71	            Dark-Mode&nbsp;aktivieren
    72	          </label>
    73	          <label className="flex items-center gap-2">
    74	            <input type="checkbox" className="accent-blue-600" />
    75	            Benachrichtigungen
    76	          </label>
    77	        </div>
    78	      ),
    79	    },
    80	  ];
    81	
    82	  /* ---------- Render ---------- */
    83	  return (
    84	    <div className="w-full space-y-2 pr-1 overflow-y-auto">
    85	      {SECTIONS.map(sec => {
    86	        const open = openId === sec.id;
    87	        return (
    88	          <div key={sec.id} className="border rounded-lg bg-white">
    89	            {/* Header / Toggle */}
    90	            <button
    91	              onClick={() => setOpenId(open ? null : sec.id)}
    92	              className={`flex w-full items-center justify-between px-3 py-2 text-sm font-medium
    93	                ${open ? 'bg-blue-600 text-white' : 'bg-gray-50 hover:bg-gray-100 text-gray-700'}`}
    94	            >
    95	              <span className="flex items-center gap-2">
    96	                {sec.icon}
    97	                {sec.title}
    98	              </span>
    99	              <ChevronDown
   100	                className={`h-4 w-4 transform transition-transform ${open ? 'rotate-180' : ''}`}
   101	              />
   102	            </button>
   103	
   104	            {/* Panel */}
   105	            <div
   106	              className={`overflow-hidden transition-[max-height] duration-300 ease-in-out
   107	                ${open ? 'max-h-screen' : 'max-h-0'}`}
   108	            >
   109	              {open && <div className="px-3">{sec.content}</div>}
   110	            </div>
   111	          </div>
   112	        );
   113	      })}
   114	    </div>
   115	  );
   116	}


===== FILE: frontend/src/app/dashboard/components/Sidebar/SidebarForm.tsx =====
     1	"use client";
     2	
     3	import * as React from "react";
     4	import { useChatWs } from "@/lib/useChatWs";
     5	
     6	type Props = { embedded?: boolean };
     7	type FormState = {
     8	  wellen_mm?: number;
     9	  gehause_mm?: number;
    10	  breite_mm?: number;
    11	  medium?: string;
    12	  temp_max_c?: number;
    13	  druck_bar?: number;
    14	  drehzahl_u_min?: number;
    15	};
    16	
    17	function toNum(v: string): number | undefined {
    18	  if (v === "" || v == null) return undefined;
    19	  const n = Number(String(v).replace(",", "."));
    20	  return Number.isFinite(n) ? n : undefined;
    21	}
    22	
    23	function formatOneLine(f: FormState): string {
    24	  const parts: string[] = [];
    25	  if (f.wellen_mm) parts.push(`Welle ${f.wellen_mm}`);
    26	  if (f.gehause_mm) parts.push(`Gehäuse ${f.gehause_mm}`);
    27	  if (f.breite_mm) parts.push(`Breite ${f.breite_mm}`);
    28	  if (f.medium) parts.push(`Medium ${f.medium}`);
    29	  if (typeof f.temp_max_c !== "undefined") parts.push(`Tmax ${f.temp_max_c}`);
    30	  if (typeof f.druck_bar !== "undefined") parts.push(`Druck ${f.druck_bar} bar`);
    31	  if (typeof f.drehzahl_u_min !== "undefined") parts.push(`n ${f.drehzahl_u_min}`);
    32	  return parts.join(", ");
    33	}
    34	
    35	function filled(v: unknown) {
    36	  return !(v === undefined || v === null || v === "");
    37	}
    38	const baseInput =
    39	  "mt-1 w-full rounded px-3 py-2 text-sm transition border outline-none focus:ring-2 focus:ring-blue-200";
    40	const cls = (isFilled: boolean) =>
    41	  [
    42	    baseInput,
    43	    isFilled
    44	      ? "text-black font-semibold border-gray-900"
    45	      : "text-gray-700 border-gray-300 placeholder-gray-400",
    46	  ].join(" ");
    47	
    48	function FormInner({
    49	  form, setForm, missing, patch, submitAll, clearAll, containerRef
    50	}: {
    51	  form: FormState;
    52	  setForm: React.Dispatch<React.SetStateAction<FormState>>;
    53	  missing: string[];
    54	  patch: (k: keyof FormState, v: any) => void;
    55	  submitAll: () => void;
    56	  clearAll: () => void;
    57	  containerRef: React.RefObject<HTMLDivElement>;
    58	}) {
    59	  return (
    60	    <>
    61	      {missing.length > 0 && (
    62	        <div className="mb-3 rounded-md border border-amber-300 bg-amber-50 px-3 py-2 text-xs text-amber-800">
    63	          Fehlend: {missing.join(", ")}
    64	        </div>
    65	      )}
    66	
    67	      <form className="space-y-4" onSubmit={(e) => e.preventDefault()}>
    68	        <div className="grid grid-cols-1 md:grid-cols-3 gap-3">
    69	          <div>
    70	            <label className="block text-sm font-medium text-gray-700">Welle (mm)</label>
    71	            <input
    72	              type="number" inputMode="decimal" step="0.01" placeholder="z. B. 25"
    73	              className={cls(filled(form.wellen_mm))}
    74	              value={form.wellen_mm ?? ""}
    75	              onChange={(e) => setForm((f) => ({ ...f, wellen_mm: toNum(e.target.value) }))}
    76	              onBlur={(e) => patch("wellen_mm", toNum(e.target.value))}
    77	            />
    78	          </div>
    79	          <div>
    80	            <label className="block text-sm font-medium text-gray-700">Gehäuse (mm)</label>
    81	            <input
    82	              type="number" inputMode="decimal" step="0.01" placeholder="z. B. 47"
    83	              className={cls(filled(form.gehause_mm))}
    84	              value={form.gehause_mm ?? ""}
    85	              onChange={(e) => setForm((f) => ({ ...f, gehause_mm: toNum(e.target.value) }))}
    86	              onBlur={(e) => patch("gehause_mm", toNum(e.target.value))}
    87	            />
    88	          </div>
    89	          <div>
    90	            <label className="block text-sm font-medium text-gray-700">Breite (mm)</label>
    91	            <input
    92	              type="number" inputMode="decimal" step="0.01" placeholder="z. B. 7"
    93	              className={cls(filled(form.breite_mm))}
    94	              value={form.breite_mm ?? ""}
    95	              onChange={(e) => setForm((f) => ({ ...f, breite_mm: toNum(e.target.value) }))}
    96	              onBlur={(e) => patch("breite_mm", toNum(e.target.value))}
    97	            />
    98	          </div>
    99	        </div>
   100	
   101	        <div className="grid grid-cols-1 md:grid-cols-3 gap-3">
   102	          <div className="md:col-span-1">
   103	            <label className="block text-sm font-medium text-gray-700">Medium</label>
   104	            <input
   105	              type="text" placeholder="z. B. Hydrauliköl"
   106	              className={cls(filled(form.medium))}
   107	              value={form.medium ?? ""}
   108	              onChange={(e) => setForm((f) => ({ ...f, medium: e.target.value }))}
   109	              onBlur={(e) => patch("medium", e.target.value.trim() || undefined)}
   110	            />
   111	          </div>
   112	          <div>
   113	            <label className="block text-sm font-medium text-gray-700">Tmax (°C)</label>
   114	            <input
   115	              type="number" inputMode="decimal" step="1" placeholder="z. B. 80"
   116	              className={cls(filled(form.temp_max_c))}
   117	              value={form.temp_max_c ?? ""}
   118	              onChange={(e) => setForm((f) => ({ ...f, temp_max_c: toNum(e.target.value) }))}
   119	              onBlur={(e) => patch("temp_max_c", toNum(e.target.value))}
   120	            />
   121	          </div>
   122	          <div>
   123	            <label className="block text-sm font-medium text-gray-700">Druck (bar)</label>
   124	            <input
   125	              type="number" inputMode="decimal" step="0.1" placeholder="z. B. 2"
   126	              className={cls(filled(form.druck_bar))}
   127	              value={form.druck_bar ?? ""}
   128	              onChange={(e) => setForm((f) => ({ ...f, druck_bar: toNum(e.target.value) }))}
   129	              onBlur={(e) => patch("druck_bar", toNum(e.target.value))}
   130	            />
   131	          </div>
   132	        </div>
   133	
   134	        <div className="grid grid-cols-1 md:grid-cols-3 gap-3">
   135	          <div>
   136	            <label className="block text-sm font-medium text-gray-700">Drehzahl (U/min)</label>
   137	            <input
   138	              type="number" inputMode="numeric" step="1" placeholder="z. B. 1500"
   139	              className={cls(filled(form.drehzahl_u_min))}
   140	              value={form.drehzahl_u_min ?? ""}
   141	              onChange={(e) => setForm((f) => ({ ...f, drehzahl_u_min: toNum(e.target.value) }))}
   142	              onBlur={(e) => patch("drehzahl_u_min", toNum(e.target.value))}
   143	            />
   144	          </div>
   145	        </div>
   146	      </form>
   147	
   148	      <div className="border-t px-4 py-3 mt-4 flex items-center justify-between gap-2">
   149	        <div className="text-xs text-gray-500">
   150	          {missing.length > 0 ? "Bitte Felder ergänzen und übernehmen." : "\u00A0"}
   151	        </div>
   152	        <div className="flex gap-2">
   153	          <button type="button" className="rounded-md border px-3 py-1.5 text-sm hover:bg-gray-50" onClick={clearAll}>
   154	            Zurücksetzen
   155	          </button>
   156	          <button type="button" className="rounded-md bg-emerald-600 text-white px-3 py-1.5 text-sm hover:bg-emerald-700" onClick={submitAll}>
   157	            Übernehmen
   158	          </button>
   159	        </div>
   160	      </div>
   161	    </>
   162	  );
   163	}
   164	
   165	export default function SidebarForm({ embedded = false }: Props) {
   166	  const { send } = useChatWs();
   167	  const [open, setOpen] = React.useState(false);
   168	  const [missing, setMissing] = React.useState<string[]>([]);
   169	  const [form, setForm] = React.useState<FormState>({});
   170	  const containerRef = React.useRef<HTMLDivElement>(null);
   171	
   172	  const mergePrefill = React.useCallback((ua: any) => {
   173	    const pre = ua?.prefill ?? ua?.params ?? {};
   174	    const miss = Array.isArray(ua?.missing) ? ua.missing : undefined;
   175	    if (miss) setMissing(miss);
   176	    if (pre && typeof pre === "object") setForm((prev) => ({ ...prev, ...pre }));
   177	  }, []);
   178	
   179	  React.useEffect(() => {
   180	    const onUi = (ev: Event) => {
   181	      const ua: any = (ev as CustomEvent<any>).detail ?? (ev as any);
   182	      const action = ua?.ui_action ?? ua?.action;
   183	      if (action === "open_form" || ua?.prefill || ua?.params) {
   184	        mergePrefill(ua);
   185	        if (!embedded && action === "open_form") setOpen(true);
   186	        setTimeout(() => {
   187	          const root = containerRef.current;
   188	          const first = root?.querySelector<HTMLInputElement | HTMLTextAreaElement | HTMLSelectElement>(
   189	            "input, textarea, select"
   190	          );
   191	          first?.focus();
   192	        }, 0);
   193	      }
   194	    };
   195	    window.addEventListener("sealai:ui_action", onUi as EventListener);
   196	    window.addEventListener("sai:need-params", onUi as EventListener);
   197	    window.addEventListener("sealai:form:patch", onUi as EventListener);
   198	    return () => {
   199	      window.removeEventListener("sealai:ui_action", onUi as EventListener);
   200	      window.removeEventListener("sai:need-params", onUi as EventListener);
   201	      window.removeEventListener("sealai:form:patch", onUi as EventListener);
   202	    };
   203	  }, [embedded, mergePrefill]);
   204	
   205	  React.useEffect(() => {
   206	    const onKey = (e: KeyboardEvent) => { if (e.key === "Escape") setOpen(false); };
   207	    window.addEventListener("keydown", onKey);
   208	    return () => window.removeEventListener("keydown", onKey);
   209	  }, []);
   210	
   211	  const patch = React.useCallback((k: keyof FormState, v: any) => {
   212	    const next = { ...form, [k]: v };
   213	    setForm(next);
   214	    const payloadValue =
   215	      typeof v === "number" ? (Number.isFinite(v) ? v : undefined) : (v && String(v).trim()) || undefined;
   216	    if (typeof payloadValue !== "undefined") {
   217	      // Live-Patch ans Backend, aber KEINE Chat-Nachricht
   218	      send("📝 form patch", { params: { [k]: payloadValue } });
   219	    }
   220	  }, [form, send]);
   221	
   222	  const submitAll = () => {
   223	    const cleaned: Record<string, any> = {};
   224	    for (const [k, v] of Object.entries(form)) {
   225	      if (v === "" || v == null) continue;
   226	      cleaned[k] = v;
   227	    }
   228	    // 1) Backend-Submit
   229	    send("📝 form submit", { params: cleaned });
   230	
   231	    // 2) Chatnachricht nur hier erzeugen
   232	    const summary = formatOneLine(cleaned as FormState);
   233	    if (summary) {
   234	      window.dispatchEvent(
   235	        new CustomEvent("sealai:chat:add", {
   236	          detail: { text: summary, source: "sidebar_form", action: "submit", params: cleaned },
   237	        }),
   238	      );
   239	    }
   240	    if (!embedded) setOpen(false);
   241	  };
   242	
   243	  const clearAll = () => setForm({});
   244	
   245	  if (embedded) {
   246	    return (
   247	      <div className="p-2" ref={containerRef}>
   248	        <FormInner
   249	          form={form} setForm={setForm} missing={missing}
   250	          patch={patch} submitAll={submitAll} clearAll={clearAll} containerRef={containerRef}
   251	        />
   252	      </div>
   253	    );
   254	  }
   255	
   256	  return (
   257	    <div className="fixed inset-0 z-40 pointer-events-none" aria-hidden={!open}>
   258	      <div
   259	        className={[
   260	          "pointer-events-auto absolute right-0 top-0 h-full w-[360px] max-w-[90vw]",
   261	          "bg-white shadow-xl border-l border-gray-200",
   262	          "transition-transform duration-300 ease-out",
   263	          open ? "translate-x-0" : "translate-x-full",
   264	        ].join(" ")}
   265	        role="dialog"
   266	        aria-modal="false"
   267	      >
   268	        <div className="flex items-center justify-between px-4 py-3 border-b">
   269	          <div className="font-semibold">Beratungs-Formular</div>
   270	          <button type="button" className="rounded px-2 py-1 text-sm hover:bg-gray-100" onClick={() => setOpen(false)} aria-label="Schließen">✕</button>
   271	        </div>
   272	        <div className="p-4 overflow-y-auto h-[calc(100%-56px)]" ref={containerRef}>
   273	          <FormInner
   274	            form={form} setForm={setForm} missing={missing}
   275	            patch={patch} submitAll={submitAll} clearAll={clearAll} containerRef={containerRef}
   276	          />
   277	        </div>
   278	      </div>
   279	    </div>
   280	  );
   281	}


===== FILE: frontend/src/app/dashboard/components/Sidebar/SidebarLeft.tsx =====
     1	"use client";
     2	
     3	import SidebarForm from "./SidebarForm";
     4	
     5	type Props = { open: boolean; onOpenChange: (open: boolean) => void };
     6	
     7	export default function SidebarLeft({ open, onOpenChange }: Props) {
     8	  return (
     9	    <>
    10	      {/* Kein Backdrop mehr. Chat bleibt voll klickbar. */}
    11	      <aside
    12	        className={[
    13	          "fixed top-0 left-0 z-50 h-full",
    14	          "w-[86vw] max-w-[360px]",
    15	          "bg-white shadow-2xl rounded-r-2xl",
    16	          "transform transition-[transform,opacity,box-shadow] duration-300 ease-out will-change-transform",
    17	          open ? "translate-x-0 opacity-100" : "-translate-x-full opacity-0 pointer-events-none",
    18	        ].join(" ")}
    19	        role="dialog"
    20	        aria-modal="false"
    21	        aria-label="Beratungs-Formular"
    22	      >
    23	        <div className="p-4 border-b flex items-center justify-between">
    24	          <h2 className="font-semibold">Beratungs-Formular</h2>
    25	          <button
    26	            type="button"
    27	            className="text-xs px-2 py-1 border rounded hover:bg-gray-50"
    28	            onClick={() => onOpenChange(false)}
    29	          >
    30	            Schließen
    31	          </button>
    32	        </div>
    33	        <div className="p-4 h-[calc(100%-56px)] overflow-y-auto">
    34	          <SidebarForm embedded />
    35	        </div>
    36	      </aside>
    37	    </>
    38	  );
    39	}


===== FILE: frontend/src/app/dashboard/components/Sidebar/SidebarRight.tsx =====
     1	'use client';
     2	
     3	import { FC } from 'react';
     4	
     5	const SidebarRight: FC = () => {
     6	  return (
     7	    <div className="h-full w-full p-4">
     8	      <h2 className="text-lg font-semibold mb-4">Optionen</h2>
     9	      <ul className="space-y-2 text-sm">
    10	        <li><button className="hover:underline">🌙 Dark Mode</button></li>
    11	        <li><button className="hover:underline">⚙️ Einstellungen</button></li>
    12	        <li><button className="hover:underline">📤 Export</button></li>
    13	      </ul>
    14	    </div>
    15	  );
    16	};
    17	
    18	export default SidebarRight;


===== FILE: frontend/src/app/dashboard/components/Sidebar/Sidebar.tsx =====
     1	// frontend/src/app/dashboard/components/Sidebar/Sidebar.tsx
     2	
     3	"use client";
     4	interface SidebarProps {
     5	  open: boolean;
     6	  setOpen: (open: boolean) => void;
     7	  activeTab: string;
     8	  setActiveTab: (tab: string) => void;
     9	  tabs: { key: string; label: string }[];
    10	}
    11	
    12	export default function Sidebar({
    13	  open,
    14	  setOpen,
    15	  activeTab,
    16	  setActiveTab,
    17	  tabs,
    18	}: SidebarProps) {
    19	  return (
    20	    <aside
    21	      className={`
    22	        fixed top-0 left-0 h-full z-50 bg-white shadow-2xl border-r transition-all duration-300
    23	        ${open ? "w-[35vw] min-w-[320px]" : "w-0 min-w-0"}
    24	        flex flex-col overflow-x-hidden
    25	      `}
    26	      style={{ willChange: "width" }}
    27	    >
    28	      {/* Logo nur einmal ganz oben */}
    29	      {open && (
    30	        <>
    31	          <div className="flex items-center space-x-2 pl-6 pt-6">
    32	            <img src="/logo_sai.svg" alt="SealAI Logo" className="h-8 w-auto" />
    33	            <span className="text-2xl font-semibold text-gray-700">SealAI</span>
    34	          </div>
    35	
    36	          {/* Tabs */}
    37	          <div className="pt-8">
    38	            <div className="flex border-b border-gray-200">
    39	              {tabs.map((tab) => (
    40	                <button
    41	                  key={tab.key}
    42	                  className={`flex-1 py-2 text-center font-medium transition
    43	                    ${activeTab === tab.key
    44	                      ? "border-b-2 border-blue-600 text-blue-700 bg-blue-50"
    45	                      : "text-gray-500 hover:bg-gray-100"}`}
    46	                  onClick={() => setActiveTab(tab.key)}
    47	                >
    48	                  {tab.label}
    49	                </button>
    50	              ))}
    51	            </div>
    52	            {/* Tab-Inhalt */}
    53	            <div className="p-6">
    54	              {activeTab === "form" && <div>Formular kommt hier hin</div>}
    55	              {activeTab === "material" && <div>Materialauswahl kommt hier hin</div>}
    56	              {activeTab === "result" && <div>Ergebnisanzeige kommt hier hin</div>}
    57	            </div>
    58	          </div>
    59	
    60	          {/* Close-Button UNTER dem Logo */}
    61	          <button
    62	            className="ml-8 mt-6 p-3 bg-blue-600 text-white rounded-full shadow-lg hover:bg-blue-700 transition"
    63	            onClick={() => setOpen(false)}
    64	            title="Sidebar schließen"
    65	            style={{ minWidth: 48, minHeight: 48 }}
    66	          >
    67	            <span style={{ fontSize: 24 }}>&#10005;</span>
    68	          </button>
    69	        </>
    70	      )}
    71	    </aside>
    72	  );
    73	}


===== FILE: frontend/src/app/dashboard/components/Sidebar.tsx =====
     1	// frontend/app/dashboard/components/Sidebar.tsx
     2	import Link from "next/link";
     3	
     4	export default function Sidebar({ className = "" }: { className?: string }) {
     5	  return (
     6	    <aside className={className + " flex flex-col p-6"}>
     7	      <h2 className="text-2xl font-bold mb-8 dark:text-gray-100">SealAI</h2>
     8	      <nav className="flex-1 space-y-2">
     9	        <Link href="/dashboard">
    10	          <a className="block px-4 py-2 rounded hover:bg-gray-100 dark:hover:bg-gray-700">
    11	            Chat
    12	          </a>
    13	        </Link>
    14	        <Link href="/dashboard/history">
    15	          <a className="block px-4 py-2 rounded hover:bg-gray-100 dark:hover:bg-gray-700">
    16	            Verlauf
    17	          </a>
    18	        </Link>
    19	      </nav>
    20	    </aside>
    21	  );
    22	}


===== FILE: frontend/src/app/dashboard/DashboardShell.tsx =====
     1	"use client";
     2	
     3	import { useEffect, useState } from "react";
     4	import { useSession } from "next-auth/react";
     5	import SidebarLeft from "./components/Sidebar/SidebarLeft";
     6	import ChatScreen from "./ChatScreen";
     7	
     8	function LogoutButton() {
     9	  const { status } = useSession();
    10	  if (status !== "authenticated") return null;
    11	
    12	  const handleLogout = () => {
    13	    window.location.assign("/api/auth/sso-logout");
    14	  };
    15	
    16	  return (
    17	    <button
    18	      onClick={handleLogout}
    19	      className="inline-flex items-center gap-2 rounded-full border border-gray-200 px-3 py-1.5 text-sm text-gray-600 hover:bg-gray-50 hover:text-gray-900 transition"
    20	      aria-label="Abmelden"
    21	      title="Abmelden"
    22	    >
    23	      <span className="i-logout h-[14px] w-[14px] inline-block" />
    24	      Abmelden
    25	    </button>
    26	  );
    27	}
    28	
    29	export default function DashboardShell() {
    30	  const [drawerOpen, setDrawerOpen] = useState(false);
    31	
    32	  useEffect(() => {
    33	    const onNeed = () => setDrawerOpen(true);
    34	    const onUi = (ev: any) => {
    35	      const ua = ev?.detail ?? ev;
    36	      const action = (typeof ua === "string") ? ua : (ua?.ui_action ?? ua?.action);
    37	      if (action === "open_form") setDrawerOpen(true);
    38	    };
    39	    window.addEventListener("sai:need-params", onNeed as EventListener);
    40	    window.addEventListener("sealai:ui_action", onUi as EventListener);
    41	    return () => {
    42	      window.removeEventListener("sai:need-params", onNeed as EventListener);
    43	      window.removeEventListener("sealai:ui_action", onUi as EventListener);
    44	    };
    45	  }, []);
    46	
    47	  // Esc schließt Drawer
    48	  useEffect(() => {
    49	    if (!drawerOpen) return;
    50	    const onKey = (e: KeyboardEvent) => { if (e.key === "Escape") setDrawerOpen(false); };
    51	    window.addEventListener("keydown", onKey);
    52	    return () => window.removeEventListener("keydown", onKey);
    53	  }, [drawerOpen]);
    54	
    55	  return (
    56	    <div className="min-h-screen w-full bg-white">
    57	      <header className="sticky top-0 z-30 flex items-center justify-end px-4 py-3 bg-white/80 backdrop-blur border-b">
    58	        <LogoutButton />
    59	      </header>
    60	      <div className="flex min-h-[calc(100vh-56px)]">
    61	        <SidebarLeft open={drawerOpen} onOpenChange={setDrawerOpen} />
    62	        <main className="flex-1 min-w-0">
    63	          <ChatScreen />
    64	        </main>
    65	      </div>
    66	    </div>
    67	  );
    68	}


===== FILE: frontend/src/app/dashboard/Dashboard.tsx =====
     1	import Chat from "./components/Chat/ChatContainer";
     2	// import FormResultsCards from "./components/FormResultsCards";
     3	// import SidebarForm from "./components/Sidebar/SidebarForm";
     4	
     5	export default function Dashboard() {
     6	  return (
     7	    <>
     8	      <Chat />
     9	      {/* <FormResultsCards /> */}
    10	      {/* <SidebarForm /> */}
    11	    </>
    12	  );
    13	}


===== FILE: frontend/src/app/dashboard/layout.tsx =====
     1	import type { ReactNode } from "react";
     2	import DashboardShell from "./DashboardShell";
     3	
     4	export default function DashboardLayout({ children }: { children: ReactNode }) {
     5	  return <DashboardShell>{children}</DashboardShell>;
     6	}


===== FILE: frontend/src/app/dashboard/page.tsx =====
     1	'use client';
     2	
     3	import { useSession } from 'next-auth/react';
     4	import { useEffect } from 'react';
     5	import { signIn } from 'next-auth/react';
     6	import ChatScreen from './ChatScreen';
     7	
     8	export default function DashboardPage() {
     9	  const { status } = useSession();
    10	
    11	  useEffect(() => {
    12	    if (status === 'unauthenticated') {
    13	      // Startet Keycloak-SSO sofort im Client – kein SSR-Redirect!
    14	      signIn('keycloak', { callbackUrl: '/dashboard' });
    15	    }
    16	  }, [status]);
    17	
    18	  if (status === 'loading') {
    19	    return (
    20	      <div className="flex items-center justify-center min-h-screen text-lg text-gray-500">
    21	        Lade Authentifizierung ...
    22	      </div>
    23	    );
    24	  }
    25	  if (status === 'authenticated') {
    26	    return <ChatScreen />;
    27	  }
    28	  // Falls unauthenticated, wird sofort umgeleitet
    29	  return (
    30	    <div className="flex items-center justify-center min-h-screen text-lg text-gray-500">
    31	      Weiterleitung zum Login ...
    32	    </div>
    33	  );
    34	}


===== FILE: frontend/src/app/layout.tsx =====
     1	// src/app/layout.tsx
     2	import Providers from './providers'
     3	import type { ReactNode } from 'react'
     4	import '../styles/globals.css'
     5	import SiteBackground from '../components/SiteBackground'
     6	
     7	export default function RootLayout({ children }: { children: ReactNode }) {
     8	  return (
     9	    <html lang="de">
    10	      <body className="bg-black text-zinc-200 antialiased">
    11	        {/* Globaler Hintergrund für die komplette Seite */}
    12	        <SiteBackground />
    13	
    14	        <Providers>
    15	          {children}
    16	        </Providers>
    17	      </body>
    18	    </html>
    19	  )
    20	}


===== FILE: frontend/src/app/page.tsx =====
     1	// src/app/page.tsx — x.ai/Grok-Stil, Sektionen ohne harte Übergänge (transparent)
     2	"use client";
     3	
     4	import { signIn } from "next-auth/react";
     5	import HeroGrok from "../components/HeroGrok";
     6	
     7	function Header() {
     8	  return (
     9	    <header className="absolute top-0 left-0 right-0 z-50 bg-transparent">
    10	      <div className="mx-auto max-w-7xl px-6 py-4 flex items-center justify-between">
    11	        <a href="/" className="flex items-center gap-3">
    12	          <img src="/logo_sai.svg" alt="SealAI" className="h-6 w-auto" />
    13	          <span className="sr-only">SealAI</span>
    14	        </a>
    15	        <nav aria-label="Primary" className="hidden md:block">
    16	          <ul className="flex items-center gap-8 text-sm text-zinc-300">
    17	            <li><a href="#products" className="hover:text-white">Products</a></li>
    18	            <li><a href="#api" className="hover:text-white">API</a></li>
    19	            <li><a href="#company" className="hover:text-white">Company</a></li>
    20	            <li><a href="#careers" className="hover:text-white">Careers</a></li>
    21	            <li><a href="#news" className="hover:text-white">News</a></li>
    22	          </ul>
    23	        </nav>
    24	        <div className="flex items-center gap-3">
    25	          <a
    26	            href="/auth/signin"
    27	            onClick={(e) => { e.preventDefault(); signIn(undefined, { callbackUrl: "/dashboard" }); }}
    28	            className="inline-flex items-center rounded-xl border border-white/20 px-4 py-2 text-sm font-medium text-white hover:bg-white/10"
    29	          >
    30	            Try SealAI
    31	          </a>
    32	        </div>
    33	      </div>
    34	    </header>
    35	  );
    36	}
    37	
    38	export default function Landing() {
    39	  return (
    40	    // bg-transparent: globaler SiteBackground scheint durch (kein Übergang)
    41	    <main className="min-h-[100dvh] bg-transparent text-zinc-200">
    42	      <Header />
    43	
    44	      {/* Hero (Gradients + Spotlights, 100dvh) */}
    45	      <HeroGrok />
    46	
    47	      {/* Products — ohne border, transparenter Bereich */}
    48	      <section id="products" className="relative bg-transparent">
    49	        {/* ganz dezente weiche Trennung via Schattenverlauf (kein harter Strich) */}
    50	        <div className="pointer-events-none absolute inset-x-0 top-0 h-10 bg-gradient-to-b from-black/0 via-black/0 to-black/10" aria-hidden />
    51	
    52	        <div className="mx-auto max-w-7xl px-6 py-16 sm:py-20">
    53	          <h2 className="text-2xl font-medium text-white">Products</h2>
    54	          <div className="mt-8 grid grid-cols-1 gap-6 sm:grid-cols-2 lg:grid-cols-3">
    55	            <Card title="Advisor" desc="Fachberater für Dichtungstechnik mit Retrieval, Tools und Reports." cta="Use now" href="/auth/signin" />
    56	            <Card id="api" title="API" desc="Nutze SealAI programmatically. Secure auth, streaming, webhooks." cta="Build now" href="/api" secondary />
    57	            <Card title="Developer Docs" desc="Schnellstart, Beispiele und Best Practices für Integration." cta="Learn more" href="/docs" secondary />
    58	          </div>
    59	        </div>
    60	      </section>
    61	
    62	      {/* News — ohne border, transparent */}
    63	      <section id="news" className="relative bg-transparent">
    64	        <div className="mx-auto max-w-7xl px-6 py-16 sm:py-20">
    65	          <h2 className="text-2xl font-medium text-white">Latest news</h2>
    66	          <ul className="mt-6 space-y-6">
    67	            <li className="flex flex-col sm:flex-row sm:items-baseline gap-2">
    68	              <span className="text-sm text-zinc-400 w-32 shrink-0">July 2025</span>
    69	              <a href="#" className="text-zinc-100 hover:underline">
    70	                SealAI Advisor v0.9 – neue Material- und Profilagenten, schnellere Streams.
    71	              </a>
    72	            </li>
    73	            <li className="flex flex-col sm:flex-row sm:items-baseline gap-2">
    74	              <span className="text-sm text-zinc-400 w-32 shrink-0">June 2025</span>
    75	              <a href="#" className="text-zinc-100 hover:underline">
    76	                API Preview – Auth via Keycloak, LangGraph Streaming, Redis Checkpointer.
    77	              </a>
    78	            </li>
    79	          </ul>
    80	        </div>
    81	      </section>
    82	
    83	      {/* Footer — ohne border, transparent */}
    84	      <footer id="company" className="relative bg-transparent mt-8">
    85	        <div className="mx-auto max-w-7xl px-6 py-12 grid grid-cols-2 sm:grid-cols-3 lg:grid-cols-5 gap-8 text-sm">
    86	          <div className="col-span-2">
    87	            <img src="/logo_sai.svg" alt="SealAI" className="h-6 w-auto mb-4" />
    88	            <p className="text-zinc-400">© {new Date().getFullYear()} SealAI</p>
    89	          </div>
    90	          <div>
    91	            <p className="mb-3 text-zinc-300">Products</p>
    92	            <ul className="space-y-2 text-zinc-400">
    93	              <li><a href="#products" className="hover:text-white">Advisor</a></li>
    94	              <li><a href="#api" className="hover:text-white">API</a></li>
    95	            </ul>
    96	          </div>
    97	          <div>
    98	            <p className="mb-3 text-zinc-300">Company</p>
    99	            <ul className="space-y-2 text-zinc-400">
   100	              <li><a href="#company" className="hover:text-white">About</a></li>
   101	              <li><a href="#careers" className="hover:text-white">Careers</a></li>
   102	              <li><a href="/impressum" className="hover:text-white">Impressum</a></li>
   103	              <li><a href="/datenschutz" className="hover:text-white">Datenschutz</a></li>
   104	            </ul>
   105	          </div>
   106	          <div>
   107	            <p className="mb-3 text-zinc-300">Resources</p>
   108	            <ul className="space-y-2 text-zinc-400">
   109	              <li><a href="/status" className="hover:text-white">Status</a></li>
   110	              <li><a href="/docs" className="hover:text-white">Docs</a></li>
   111	            </ul>
   112	          </div>
   113	        </div>
   114	      </footer>
   115	    </main>
   116	  );
   117	}
   118	
   119	function Card({
   120	  title, desc, cta, href, secondary, id
   121	}: {
   122	  title: string; desc: string; cta: string; href: string; secondary?: boolean; id?: string
   123	}) {
   124	  return (
   125	    <a
   126	      id={id}
   127	      href={href}
   128	      className={[
   129	        "group block rounded-2xl border p-6 transition bg-white/[0.03]",
   130	        secondary ? "border-white/15 hover:bg-white/5" : "border-white/20 hover:bg-white/[0.06]",
   131	      ].join(" ")}
   132	    >
   133	      <div className="text-base font-medium text-white">{title}</div>
   134	      <p className="mt-2 text-sm text-zinc-400">{desc}</p>
   135	      <div className="mt-4 inline-flex items-center gap-2 text-sm font-semibold text-white">
   136	        {cta}
   137	        <svg className="size-4 opacity-70 group-hover:translate-x-0.5 transition" viewBox="0 0 24 24" fill="none" aria-hidden="true">
   138	          <path d="M5 12h14M13 5l7 7-7 7" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round" />
   139	        </svg>
   140	      </div>
   141	    </a>
   142	  );
   143	}


===== FILE: frontend/src/app/providers.tsx =====
     1	// src/app/providers.tsx
     2	'use client'
     3	
     4	import { SessionProvider } from 'next-auth/react'
     5	import type { ReactNode } from 'react'
     6	
     7	export default function Providers({ children }: { children: ReactNode }) {
     8	  return <SessionProvider>{children}</SessionProvider>
     9	}


===== FILE: frontend/src/app/register/page.js =====
     1	"use client";
     2	
     3	import { useState } from 'react';
     4	
     5	const RegisterPage = () => {
     6	  const [formData, setFormData] = useState({ email: '', password: '' });
     7	
     8	  const handleSubmit = async (e) => {
     9	    e.preventDefault();
    10	    // Registrierung-Logik hier
    11	  };
    12	
    13	  return (
    14	    <form onSubmit={handleSubmit}>
    15	      <input
    16	        type="email"
    17	        placeholder="Email"
    18	        value={formData.email}
    19	        onChange={(e) => setFormData({ ...formData, email: e.target.value })}
    20	      />
    21	      <input
    22	        type="password"
    23	        placeholder="Password"
    24	        value={formData.password}
    25	        onChange={(e) => setFormData({ ...formData, password: e.target.value })}
    26	      />
    27	      <button type="submit">Register</button>
    28	    </form>
    29	  );
    30	};
    31	
    32	export default RegisterPage;


===== FILE: frontend/src/components/Fog.tsx =====
     1	// src/components/Fog.tsx — bottom-only clouds, stronger & slower, no edge seam
     2	"use client";
     3	import React, { useEffect, useRef } from "react";
     4	
     5	export default function Fog() {
     6	  const canvasRef = useRef<HTMLCanvasElement | null>(null);
     7	  const rafRef = useRef<number | null>(null);
     8	
     9	  useEffect(() => {
    10	    const canvas = canvasRef.current!;
    11	    const ctx = canvas.getContext("2d", { alpha: true })!;
    12	    let running = true;
    13	
    14	    // Tunables (sichtbarer Nebel)
    15	    const BLUR_PX = 22;          // etwas weniger Weichzeichnung -> mehr Struktur
    16	    const PAD = BLUR_PX * 4;     // Offscreen-Puffer gegen Randartefakte
    17	    const TILE = 256;            // Größe der Rausch-Kachel
    18	
    19	    // Kleine wiederholbare Rauschkachel (gaussian-ish, leicht kontrastiert)
    20	    const noiseTile = document.createElement("canvas");
    21	    noiseTile.width = TILE; noiseTile.height = TILE;
    22	    {
    23	      const nctx = noiseTile.getContext("2d")!;
    24	      const img = nctx.createImageData(TILE, TILE);
    25	      const d = img.data;
    26	      for (let i = 0; i < d.length; i += 4) {
    27	        // Summe zweier Zufälle -> Glockenkurve; danach leicht kontrastverstärkt
    28	        let v = (Math.random() + Math.random()) * 127; // 0..254
    29	        // Simple contrast curve around mid gray
    30	        const c = 1.18; // Kontrastfaktor
    31	        v = (v - 127) * c + 127;
    32	        v = Math.max(0, Math.min(255, v));
    33	        d[i] = d[i + 1] = d[i + 2] = v; d[i + 3] = 255;
    34	      }
    35	      nctx.putImageData(img, 0, 0);
    36	    }
    37	
    38	    // Großes Offscreen-Canvas (mit Rand)
    39	    const frame = document.createElement("canvas");
    40	    const fctx = frame.getContext("2d")!;
    41	
    42	    function resize() {
    43	      const parent = canvas.parentElement!;
    44	      const dpr = Math.min(window.devicePixelRatio || 1, 2);
    45	      canvas.width = Math.floor(parent.clientWidth * dpr);
    46	      canvas.height = Math.floor(parent.clientHeight * dpr);
    47	      canvas.style.width = "100%";
    48	      canvas.style.height = "100%";
    49	      ctx.setTransform(dpr, 0, 0, dpr, 0, 0);
    50	
    51	      frame.width = parent.clientWidth + PAD * 2;
    52	      frame.height = parent.clientHeight + PAD * 2;
    53	    }
    54	
    55	    function draw(time: number) {
    56	      if (!running) return;
    57	      const t = time * 0.001;
    58	      const w = canvas.clientWidth;
    59	      const h = canvas.clientHeight;
    60	
    61	      // Offscreen mit zwei driftenden Schichten füllen
    62	      fctx.clearRect(0, 0, frame.width, frame.height);
    63	      const pat = fctx.createPattern(noiseTile, "repeat")!;
    64	
    65	      // Layer A (breit)
    66	      fctx.save();
    67	      fctx.globalAlpha = 0.95;
    68	      fctx.setTransform(1, 0, 0, 1, (-((t * 6) % TILE)) - PAD, Math.sin(t * 0.10) * 10 - PAD);
    69	      fctx.fillStyle = pat;
    70	      fctx.fillRect(0, 0, frame.width + PAD * 2, frame.height + PAD * 2);
    71	      fctx.restore();
    72	
    73	      // Layer B (gegengesetzt, etwas schneller)
    74	      fctx.save();
    75	      fctx.globalAlpha = 0.75;
    76	      fctx.setTransform(1, 0, 0, 1, (-((t * -9) % TILE)) - PAD, Math.cos(t * 0.08) * 12 - PAD);
    77	      fctx.fillStyle = pat;
    78	      fctx.fillRect(0, 0, frame.width + PAD * 2, frame.height + PAD * 2);
    79	      fctx.restore();
    80	
    81	      // Haupt-Render: weichzeichnen + kräftiger Alpha
    82	      ctx.clearRect(0, 0, w, h);
    83	      ctx.save();
    84	      ctx.filter = `blur(${BLUR_PX}px)`;
    85	      ctx.globalAlpha = 0.50; // vorher ~0.32
    86	      ctx.drawImage(frame, -PAD, -PAD, frame.width, frame.height, 0, 0, w, h);
    87	      ctx.restore();
    88	
    89	      // Zweite Tiefen-Schicht (minimal skaliert) für volumen
    90	      ctx.save();
    91	      ctx.filter = `blur(${Math.round(BLUR_PX * 1.2)}px)`;
    92	      ctx.globalAlpha = 0.25;
    93	      ctx.drawImage(frame, -PAD - 12, -PAD - 8, frame.width + 24, frame.height + 16, 0, 0, w, h);
    94	      ctx.restore();
    95	
    96	      // Blaue Bodentönung etwas kräftiger
    97	      const tint = ctx.createLinearGradient(0, h * 0.45, 0, h);
    98	      tint.addColorStop(0.0, "rgba(0,0,0,0)");
    99	      tint.addColorStop(1.0, "rgba(99,102,241,0.22)");
   100	      ctx.fillStyle = tint;
   101	      ctx.fillRect(0, 0, w, h);
   102	
   103	      // Maske: weiter oben sichtbar machen
   104	      const mask = ctx.createLinearGradient(0, 0, 0, h);
   105	      mask.addColorStop(0.00, "rgba(0,0,0,0)");
   106	      mask.addColorStop(0.45, "rgba(0,0,0,0.25)");
   107	      mask.addColorStop(0.65, "rgba(0,0,0,0.75)");
   108	      mask.addColorStop(1.00, "rgba(0,0,0,1)");
   109	      ctx.globalCompositeOperation = "destination-in";
   110	      ctx.fillStyle = mask;
   111	      ctx.fillRect(0, 0, w, h);
   112	      ctx.globalCompositeOperation = "source-over";
   113	
   114	      rafRef.current = requestAnimationFrame(draw);
   115	    }
   116	
   117	    resize();
   118	    const ro = new ResizeObserver(resize);
   119	    ro.observe(canvas.parentElement!);
   120	    rafRef.current = requestAnimationFrame(draw);
   121	
   122	    return () => {
   123	      running = false;
   124	      if (rafRef.current) cancelAnimationFrame(rafRef.current);
   125	      ro.disconnect();
   126	    };
   127	  }, []);
   128	
   129	  // etwas höhere Nebelhöhe, damit er deutlicher sichtbar ist
   130	  return (
   131	    <div className="pointer-events-none absolute inset-x-0 bottom-0 h-[48svh] md:h-[46svh] lg:h-[44svh]">
   132	      <canvas ref={canvasRef} className="w-full h-full" />
   133	    </div>
   134	  );
   135	}


===== FILE: frontend/src/components/HeroBackground.tsx =====
     1	"use client";
     2	import * as React from "react";
     3	import { Canvas, useFrame } from "@react-three/fiber";
     4	
     5	const vertexShader = `
     6	  varying vec2 vUv;
     7	  void main() {
     8	    vUv = uv;
     9	    gl_Position = projectionMatrix * modelViewMatrix * vec4(position,1.0);
    10	  }
    11	`;
    12	
    13	const fragmentShader = `
    14	  varying vec2 vUv;
    15	  uniform float uTime;
    16	  float rand(vec2 co){
    17	      return fract(sin(dot(co.xy,vec2(12.9898,78.233)))*43758.5453);
    18	  }
    19	  float noise(vec2 p){
    20	      vec2 i = floor(p);
    21	      vec2 f = fract(p);
    22	      float a = rand(i);
    23	      float b = rand(i + vec2(1.0, 0.0));
    24	      float c = rand(i + vec2(0.0, 1.0));
    25	      float d = rand(i + vec2(1.0, 1.0));
    26	      vec2 u = f * f * (3.0 - 2.0 * f);
    27	      return mix(a, b, u.x) +
    28	              (c - a)* u.y * (1.0 - u.x) +
    29	              (d - b) * u.x * u.y;
    30	  }
    31	  float fbm(vec2 p) {
    32	      float value = 0.0;
    33	      float amplitude = 0.5;
    34	      for (int i = 0; i < 6; i++) {
    35	          value += amplitude * noise(p);
    36	          p *= 2.0;
    37	          amplitude *= 0.5;
    38	      }
    39	      return value;
    40	  }
    41	
    42	  void main() {
    43	    vec2 uv = vUv * 2.0 - 1.0;
    44	    float t = uTime * 0.06;
    45	    float q = fbm(uv * 1.4 + t * 0.6);
    46	    float r = fbm(uv * 2.3 - t * 0.3 + q);
    47	    float mask = smoothstep(0.25, 0.7, r);
    48	    float beam = smoothstep(0.3, 1.0, uv.x + uv.y + 0.5) * 0.6;
    49	    float intensity = (r * 0.7 + q * 0.3) * (1.2 + beam);
    50	    vec3 color = mix(vec3(0.13,0.17,0.26), vec3(0.60,0.75,1.0), intensity);
    51	    color += beam * vec3(1.2,1.2,2.5);
    52	    float alpha = mask * 0.84;
    53	    gl_FragColor = vec4(color, alpha);
    54	  }
    55	`;
    56	
    57	export default function HeroBackground() {
    58	  const materialRef = React.useRef<any>(null);
    59	  useFrame(({ clock }) => {
    60	    if (materialRef.current) {
    61	      materialRef.current.uniforms.uTime.value = clock.getElapsedTime();
    62	    }
    63	  });
    64	
    65	  return (
    66	    <div className="absolute inset-0 w-full h-full z-0 pointer-events-none">
    67	      <Canvas
    68	        camera={{ position: [0, 0, 1], fov: 40 }}
    69	        style={{ width: "100%", height: "100%" }}
    70	        gl={{ alpha: true, antialias: true }}
    71	      >
    72	        <mesh scale={[3.6, 2.2, 1]}>
    73	          <planeGeometry args={[1, 1, 128, 128]} />
    74	          <shaderMaterial
    75	            ref={materialRef}
    76	            uniforms={{ uTime: { value: 0 } }}
    77	            vertexShader={vertexShader}
    78	            fragmentShader={fragmentShader}
    79	            transparent
    80	            depthWrite={false}
    81	          />
    82	        </mesh>
    83	      </Canvas>
    84	    </div>
    85	  );
    86	}


===== FILE: frontend/src/components/HeroGrok.tsx =====
     1	// src/components/HeroGrok.tsx — Hero (100dvh) mit Gradients & Spotlights, ohne Nebel
     2	"use client";
     3	
     4	import React from "react";
     5	import { signIn } from "next-auth/react";
     6	import Starfield from "./Starfield";
     7	
     8	export default function HeroGrok() {
     9	  return (
    10	    <section className="relative overflow-hidden h-[100dvh] min-h-[100dvh] flex">
    11	      {/* Sternenhimmel */}
    12	      <Starfield />
    13	
    14	      {/* Sehr dunkler, fast schwarzer Blauverlauf von oben */}
    15	      <div
    16	        className="pointer-events-none absolute inset-0 bg-gradient-to-b
    17	                   from-[#040815] via-[#0A1328]/80 to-transparent"
    18	        aria-hidden
    19	      />
    20	
    21	      {/* Spotlight rechts (volumetrisch, atmend) */}
    22	      <div
    23	        className="pointer-events-none absolute right-[-18%] top-1/2 -translate-y-1/2
    24	                   w-[70vw] h-[70vw]
    25	                   bg-[radial-gradient(closest-side,rgba(255,255,255,0.9),rgba(99,102,241,0.35),transparent_70%)]
    26	                   blur-3xl opacity-80 animate-glow-pulse"
    27	        aria-hidden
    28	      />
    29	
    30	      {/* Sekundäres leises Spotlight links */}
    31	      <div
    32	        className="pointer-events-none absolute left-[-25%] top-1/2 -translate-y-1/2
    33	                   w-[55vw] h-[55vw]
    34	                   bg-[radial-gradient(closest-side,rgba(37,99,235,0.35),rgba(59,130,246,0.18),transparent_70%)]
    35	                   blur-3xl opacity-35 animate-glow-pulse"
    36	        aria-hidden
    37	      />
    38	
    39	      {/* Horizontaler Light-Sweep rechts */}
    40	      <div
    41	        className="pointer-events-none absolute inset-y-0 right-[8%] w-[60vw]
    42	                   bg-[linear-gradient(90deg,transparent,rgba(180,200,255,0.25)_40%,transparent)]
    43	                   blur-2xl opacity-60 animate-glow-sweep"
    44	        aria-hidden
    45	      />
    46	
    47	      {/* Boden-Glow (Gradient statt Nebel; Video kommt später hier drüber) */}
    48	      <div
    49	        className="pointer-events-none absolute inset-x-0 bottom-0 h-[46svh]
    50	                   bg-gradient-to-t from-[#1b2142]/70 via-[#121936]/35 to-transparent"
    51	        aria-hidden
    52	      />
    53	
    54	      {/* Platzhalter-Layer für späteres Video (wabernder Nebel) */}
    55	      <div className="pointer-events-none absolute inset-0 z-[5]" aria-hidden />
    56	
    57	      {/* Inhalt zentriert */}
    58	      <div className="relative z-10 mx-auto max-w-7xl px-6 w-full h-full flex">
    59	        <div className="max-w-4xl m-auto text-center flex flex-col items-center justify-center gap-6">
    60	          <p className="text-xs uppercase tracking-widest text-zinc-400 mx-auto">SealAI</p>
    61	
    62	          <h1
    63	            className="text-[16vw] leading-none font-semibold text-white/90
    64	                       sm:text-[12vw] md:text-[10vw] lg:text-[9vw]
    65	                       [text-shadow:0_0_30px_rgba(120,140,255,0.18),0_0_10px_rgba(255,255,255,0.05)]"
    66	          >
    67	            SealAI
    68	          </h1>
    69	
    70	          <p className="max-w-2xl text-lg text-zinc-300">
    71	            Dein Assistent für Werkstoffauswahl, Profile und Konstruktion – mit Echtzeit-Recherche,
    72	            fundierter Beratung und Integration in deinen Workflow.
    73	          </p>
    74	
    75	          {/* Eingabe-Box */}
    76	          <div className="mt-2 max-w-2xl w-full mx-auto rounded-2xl border border-white/10 bg-black/50 backdrop-blur">
    77	            <div className="flex items-center">
    78	              <input
    79	                readOnly
    80	                value="What do you want to know?"
    81	                className="w-full bg-transparent px-5 py-4 text-sm text-zinc-400 outline-none"
    82	              />
    83	              <button
    84	                onClick={() => signIn(undefined, { callbackUrl: "/dashboard" })}
    85	                className="m-2 inline-flex items-center justify-center rounded-xl border border-white/15 px-3 py-2 text-sm font-medium text-white hover:bg-white/10"
    86	                aria-label="Try SealAI"
    87	              >
    88	                →
    89	              </button>
    90	            </div>
    91	          </div>
    92	        </div>
    93	      </div>
    94	
    95	      {/* Scroll-Hinweis */}
    96	      <div className="absolute bottom-6 left-1/2 -translate-x-1/2 text-white/40 text-xl animate-bounce">▾</div>
    97	    </section>
    98	  );
    99	}


===== FILE: frontend/src/components/organisms/Header.tsx =====
     1	'use client';
     2	
     3	import React from 'react';
     4	
     5	interface HeaderProps {
     6	  onToggle: () => void;
     7	  isSidebarOpen: boolean;
     8	}
     9	
    10	export default function Header({ onToggle, isSidebarOpen }: HeaderProps) {
    11	  return (
    12	    <div className="flex items-center justify-between h-16 px-4 border-b border-gray-200">
    13	      <button 
    14	        onClick={onToggle}
    15	        className="p-2 rounded hover:bg-gray-100"
    16	        aria-label="Toggle Sidebar"
    17	      >
    18	        {isSidebarOpen ? '←' : '☰'}
    19	      </button>
    20	      <span className="font-semibold text-lg">🦭 SealAI</span>
    21	      <div /> {/* Platzhalter für Rechtsshift */}
    22	    </div>
    23	  );
    24	}


===== FILE: frontend/src/components/organisms/Sidebar.tsx =====
     1	'use client';
     2	// frontend/src/components/organisms/Sidebar.tsx
     3	
     4	import Link from 'next/link';
     5	import { usePathname } from 'next/navigation';
     6	import { Home, MessageSquare } from 'lucide-react';
     7	
     8	export default function Sidebar() {
     9	  const path = usePathname();
    10	
    11	  const items = [
    12	    { label: 'Home', href: '/', icon: Home },
    13	    { label: 'Chat', href: '/dashboard', icon: MessageSquare },
    14	  ];
    15	
    16	  return (
    17	    <nav className="flex flex-col p-4 space-y-2">
    18	      {items.map(({ label, href, icon: Icon }) => {
    19	        const active = path === href;
    20	        return (
    21	          <Link
    22	            key={href}
    23	            href={href}
    24	            className={`flex items-center gap-2 px-3 py-2 rounded-md ${
    25	              active
    26	                ? 'bg-brand-600 text-white'
    27	                : 'hover:bg-gray-100 dark:hover:bg-gray-700'
    28	            }`}
    29	          >
    30	            <Icon className="w-5 h-5" />
    31	            <span className="font-medium">{label}</span>
    32	          </Link>
    33	        );
    34	      })}
    35	    </nav>
    36	  );
    37	}


===== FILE: frontend/src/components/SiteBackground.tsx =====
     1	// src/components/SiteBackground.tsx
     2	// Globaler Seitenhintergrund: sehr dunkles Blau + dezenter Starfield, fixiert
     3	"use client";
     4	
     5	import React from "react";
     6	import Starfield from "./Starfield";
     7	
     8	export default function SiteBackground() {
     9	  return (
    10	    <>
    11	      {/* Tiefschwarz als Fallback */}
    12	      <div className="pointer-events-none fixed inset-0 z-[-3] bg-black" />
    13	
    14	      {/* Dunkelblauer Verlauf wie im Hero */}
    15	      <div
    16	        className="pointer-events-none fixed inset-0 z-[-2]
    17	                   bg-gradient-to-b from-[#040815] via-[#0A1328] to-[#0B1020]"
    18	        aria-hidden
    19	      />
    20	
    21	      {/* Dezenter Sternenhimmel über gesamte Seite */}
    22	      <div className="pointer-events-none fixed inset-0 z-[-1] opacity-35">
    23	        <Starfield />
    24	      </div>
    25	    </>
    26	  );
    27	}


===== FILE: frontend/src/components/Starfield.tsx =====
     1	// src/components/Starfield.tsx — subtiler Sternenhimmel (twinkle)
     2	"use client";
     3	import React, { useEffect, useRef } from "react";
     4	
     5	type Star = { x: number; y: number; r: number; phase: number; speed: number };
     6	
     7	export default function Starfield({ density = 240 }: { density?: number }) {
     8	  const ref = useRef<HTMLCanvasElement | null>(null);
     9	  const raf = useRef<number | null>(null);
    10	
    11	  useEffect(() => {
    12	    const c = ref.current!;
    13	    const ctx = c.getContext("2d")!;
    14	    const stars: Star[] = [];
    15	    let running = true;
    16	
    17	    function resize() {
    18	      const parent = c.parentElement!;
    19	      const dpr = Math.min(window.devicePixelRatio || 1, 2);
    20	      c.width = Math.floor(parent.clientWidth * dpr);
    21	      c.height = Math.floor(parent.clientHeight * dpr);
    22	      c.style.width = "100%";
    23	      c.style.height = "100%";
    24	      ctx.setTransform(dpr, 0, 0, dpr, 0, 0);
    25	
    26	      stars.length = 0;
    27	      const count = Math.floor((parent.clientWidth * parent.clientHeight) / 18000);
    28	      for (let i = 0; i < Math.min(density, Math.max(80, count)); i++) {
    29	        stars.push({
    30	          x: Math.random() * parent.clientWidth,
    31	          y: Math.random() * parent.clientHeight,
    32	          r: Math.random() * 1.2 + 0.2,
    33	          phase: Math.random() * Math.PI * 2,
    34	          speed: 0.6 + Math.random() * 0.6,
    35	        });
    36	      }
    37	    }
    38	
    39	    function draw(t: number) {
    40	      if (!running) return;
    41	      ctx.clearRect(0, 0, c.width, c.height);
    42	      ctx.fillStyle = "#fff";
    43	      for (const s of stars) {
    44	        const a = 0.08 + Math.abs(Math.sin(s.phase + t * 0.001 * s.speed)) * 0.18;
    45	        ctx.globalAlpha = a;
    46	        ctx.beginPath();
    47	        ctx.arc(s.x, s.y, s.r, 0, Math.PI * 2);
    48	        ctx.fill();
    49	      }
    50	      ctx.globalAlpha = 1;
    51	      raf.current = requestAnimationFrame(draw);
    52	    }
    53	
    54	    resize();
    55	    const ro = new ResizeObserver(resize);
    56	    ro.observe(c.parentElement!);
    57	    raf.current = requestAnimationFrame(draw);
    58	
    59	    return () => { running = false; if (raf.current) cancelAnimationFrame(raf.current); ro.disconnect(); };
    60	  }, [density]);
    61	
    62	  return <canvas ref={ref} className="absolute inset-0 w-full h-full" aria-hidden />;
    63	}


===== FILE: frontend/src/components/ui/card.tsx =====
     1	// 📄 frontend/components/ui/card.tsx
     2	
     3	import React from "react";
     4	
     5	export function Card({ children }: { children: React.ReactNode }) {
     6	  return (
     7	    <div
     8	      style={{
     9	        border: "1px solid #ccc",
    10	        borderRadius: "0.5rem",
    11	        padding: "1rem",
    12	        backgroundColor: "#fff",
    13	        boxShadow: "0 2px 6px rgba(0,0,0,0.05)"
    14	      }}
    15	    >
    16	      {children}
    17	    </div>
    18	  );
    19	}


===== FILE: frontend/src/lib/logout.ts =====
     1	"use client"
     2	
     3	import { signOut } from "next-auth/react"
     4	
     5	const CLIENT_ID =
     6	  process.env.NEXT_PUBLIC_KEYCLOAK_CLIENT_ID ||
     7	  process.env.KEYCLOAK_CLIENT_ID ||
     8	  "nextauth"
     9	
    10	const ISSUER =
    11	  process.env.NEXT_PUBLIC_KEYCLOAK_ISSUER ||
    12	  process.env.KEYCLOAK_ISSUER ||
    13	  "https://auth.sealai.net/realms/sealAI"
    14	
    15	export const logout = async (idToken?: string, redirectTo = "/") => {
    16	  const origin = typeof window !== "undefined" ? window.location.origin : ""
    17	  const safePath = redirectTo.startsWith("/") ? redirectTo : "/"
    18	  const postLogout = `${origin}${safePath}`
    19	
    20	  // 1) lokale NextAuth-Session beenden
    21	  await signOut({ redirect: false })
    22	
    23	  // 2) RP-initiated logout URL bauen
    24	  const base = `${ISSUER}/protocol/openid-connect/logout`
    25	  const params = new URLSearchParams({
    26	    client_id: CLIENT_ID,
    27	    post_logout_redirect_uri: postLogout,
    28	  })
    29	
    30	  // Nur ein id_token_hint mitsenden, wenn wirklich vorhanden
    31	  if (idToken && idToken.split(".").length === 3) {
    32	    params.set("id_token_hint", idToken)
    33	  }
    34	
    35	  // 3) Browser zu Keycloak umleiten
    36	  window.location.href = `${base}?${params.toString()}`
    37	}
    38	
    39	// Alias für alte Importe
    40	export const handleLogout = logout


===== FILE: frontend/src/lib/useAccessToken.ts =====
     1	"use client";
     2	
     3	import { useSession } from "next-auth/react";
     4	import * as React from "react";
     5	
     6	/**
     7	 * Liefert das Access Token und den Auth-Status (Client-seitig gecacht).
     8	 */
     9	export function useAccessToken() {
    10	  const { data, status } = useSession();
    11	  const [token, setToken] = React.useState<string | undefined>(undefined);
    12	
    13	  React.useEffect(() => {
    14	    if (status === "authenticated") {
    15	      const t =
    16	        (data as any)?.accessToken ??
    17	        (data as any)?.user?.accessToken ??
    18	        (data as any)?.user?.token ??
    19	        (data as any)?.access_token;
    20	      setToken(typeof t === "string" ? t : undefined);
    21	    } else if (status === "unauthenticated") {
    22	      setToken(undefined);
    23	    }
    24	  }, [status, data]);
    25	
    26	  return {
    27	    token,
    28	    loading: status === "loading",
    29	    authenticated: status === "authenticated",
    30	  };
    31	}
    32	
    33	/**
    34	 * Holt *immer* die frischeste Session-Ansicht vom Server und extrahiert das Access-Token.
    35	 * Nutzt NextAuth `/api/auth/session`. Keine Exceptions nach außen – bei Fehler `undefined`.
    36	 */
    37	export async function fetchFreshAccessToken(): Promise<string | undefined> {
    38	  try {
    39	    const res = await fetch("/api/auth/session", { cache: "no-store" });
    40	    if (!res.ok) return undefined;
    41	    const json = await res.json();
    42	    const t =
    43	      json?.accessToken ??
    44	      json?.user?.accessToken ??
    45	      json?.user?.token ??
    46	      json?.access_token;
    47	    return typeof t === "string" ? t : undefined;
    48	  } catch {
    49	    return undefined;
    50	  }
    51	}


===== FILE: frontend/src/lib/useChatSse.ts =====
     1	"use client";
     2	
     3	import * as React from "react";
     4	import { useSession } from "next-auth/react";
     5	
     6	type State = {
     7	  streaming: boolean;
     8	  text: string;
     9	  error: string | null;
    10	};
    11	
    12	export function useChatSse(endpoint: string = "/api/langgraph/chat") {
    13	  const { status } = useSession();
    14	  const [state, setState] = React.useState<State>({ streaming: false, text: "", error: null });
    15	  const controllerRef = React.useRef<AbortController | null>(null);
    16	
    17	  const send = React.useCallback(async (input: string, bodyExtra?: Record<string, unknown>) => {
    18	    if (status !== "authenticated") {
    19	      setState((s) => ({ ...s, error: "unauthenticated" }));
    20	      return;
    21	    }
    22	    const trimmed = input.trim();
    23	    if (!trimmed) return;
    24	
    25	    controllerRef.current?.abort();
    26	    controllerRef.current = new AbortController();
    27	
    28	    setState({ streaming: true, text: "", error: null });
    29	
    30	    const resp = await fetch(endpoint, {
    31	      method: "POST",
    32	      headers: {
    33	        "Content-Type": "application/json",
    34	        Accept: "text/event-stream",
    35	      },
    36	      body: JSON.stringify({ input: trimmed, stream: true, ...(bodyExtra || {}) }),
    37	      signal: controllerRef.current.signal,
    38	    }).catch((e) => {
    39	      setState({ streaming: false, text: "", error: String(e?.message || "network_error") });
    40	      return null as any;
    41	    });
    42	
    43	    if (!resp || !resp.ok || !resp.body) {
    44	      if (resp) setState({ streaming: false, text: "", error: `http_${resp.status}` });
    45	      return;
    46	    }
    47	
    48	    const reader = resp.body.getReader();
    49	    const decoder = new TextDecoder();
    50	    let buf = "";
    51	
    52	    try {
    53	      for (;;) {
    54	        const { value, done } = await reader.read();
    55	        if (done) break;
    56	        buf += decoder.decode(value, { stream: true });
    57	
    58	        const frames = buf.split("\n\n");
    59	        buf = frames.pop() ?? "";
    60	        for (const frame of frames) {
    61	          const dataLine = frame.split("\n").find((l) => l.startsWith("data: "));
    62	          if (!dataLine) continue;
    63	          try {
    64	            const payload = JSON.parse(dataLine.slice(6));
    65	            if (typeof payload?.delta === "string" && payload.delta.length) {
    66	              setState((s) => ({ ...s, text: s.text + payload.delta }));
    67	            } else if (payload?.final?.text) {
    68	              setState((s) => ({ ...s, text: payload.final.text }));
    69	            } else if (payload?.error) {
    70	              setState((s) => ({ ...s, error: String(payload.error) }));
    71	            }
    72	          } catch {
    73	            // ignore malformed frames
    74	          }
    75	        }
    76	      }
    77	    } catch (e: any) {
    78	      if (e?.name !== "AbortError") {
    79	        setState((s) => ({ ...s, error: String(e?.message || "stream_error") }));
    80	      }
    81	    } finally {
    82	      try { await reader.cancel(); } catch {}
    83	      setState((s) => ({ ...s, streaming: false }));
    84	    }
    85	  }, [status, endpoint]);
    86	
    87	  const cancel = React.useCallback(() => {
    88	    controllerRef.current?.abort();
    89	    setState((s) => ({ ...s, streaming: false }));
    90	  }, []);
    91	
    92	  const reset = React.useCallback(() => {
    93	    controllerRef.current?.abort();
    94	    setState({ streaming: false, text: "", error: null });
    95	  }, []);
    96	
    97	  return { ...state, send, cancel, reset };
    98	}


===== FILE: frontend/src/lib/useChatWs.ts =====
     1	"use client";
     2	
     3	import * as React from "react";
     4	import ChatWsClient from "./ws";
     5	import { useAccessToken, fetchFreshAccessToken } from "./useAccessToken";
     6	
     7	export type UseChatWsArgs = { chatId?: string; endpoint?: string };
     8	
     9	type UseChatWsState = {
    10	  connected: boolean;
    11	  streaming: boolean;
    12	  threadId?: string;
    13	  agent?: string;
    14	  text: string;             // transient Stream-Puffer (wird nach DONE geleert)
    15	  lastError?: string;
    16	  lastUiAction?: any;       // zuletzt empfangenes UI-Event
    17	};
    18	
    19	export function useChatWs({ chatId = "default", endpoint }: UseChatWsArgs = {}) {
    20	  const { token } = useAccessToken();
    21	  const [state, setState] = React.useState<UseChatWsState>({
    22	    connected: false,
    23	    streaming: false,
    24	    text: "",
    25	  });
    26	
    27	  const clientRef = React.useRef<ChatWsClient>();
    28	  const awaitingSendRef = React.useRef(false);
    29	
    30	  const getToken = React.useCallback(async () => {
    31	    const fresh = await fetchFreshAccessToken().catch(() => undefined);
    32	    return fresh ?? token;
    33	  }, [token]);
    34	
    35	  React.useEffect(() => {
    36	    let mounted = true;
    37	    if (!token) return;
    38	
    39	    const client = new ChatWsClient({
    40	      url: endpoint ?? "/api/v1/ai/ws",
    41	      getToken,
    42	      onOpen: () => mounted && setState((s) => ({ ...s, connected: true })),
    43	      onClose: () =>
    44	        mounted && setState((s) => ({ ...s, connected: false, streaming: false })),
    45	      onError: (ev: Event) =>
    46	        mounted &&
    47	        setState((s) => ({
    48	          ...s,
    49	          lastError: String((ev as any)?.message ?? "WebSocket error"),
    50	        })),
    51	      onStreamStart: ({ threadId, agent }) => {
    52	        if (!mounted || !awaitingSendRef.current) return;
    53	        setState((s) => ({
    54	          ...s,
    55	          streaming: true,
    56	          threadId,
    57	          agent,
    58	          text: "",
    59	        }));
    60	      },
    61	      onStreamDelta: ({ delta }) =>
    62	        mounted &&
    63	        setState((s) => ({
    64	          ...s,
    65	          text: (s.text ?? "") + String(delta),
    66	        })),
    67	      onStreamDone: () => {
    68	        if (!mounted) return;
    69	        awaitingSendRef.current = false;
    70	        setState((s) => ({ ...s, streaming: false, text: "" }));
    71	      },
    72	      onUiAction: (ua) => {
    73	        if (!mounted) return;
    74	        setState((s) => ({ ...s, lastUiAction: ua }));
    75	        // globales Event, damit Shell den linken Drawer öffnen kann
    76	        window.dispatchEvent(new CustomEvent("sealai:ui_action", { detail: ua }));
    77	      },
    78	    });
    79	
    80	    clientRef.current = client;
    81	    client.connect().catch((e: any) => {
    82	      if (!mounted) return;
    83	      setState((s) => ({ ...s, lastError: String(e?.message ?? e) }));
    84	    });
    85	
    86	    return () => {
    87	      mounted = false;
    88	      client.close();
    89	      clientRef.current = undefined;
    90	    };
    91	  }, [token, endpoint, getToken]);
    92	
    93	  const send = React.useCallback(
    94	    (input: string, extra?: Record<string, unknown>) => {
    95	      const trimmed = input?.trim();
    96	      if (!trimmed) return;
    97	      awaitingSendRef.current = true;
    98	      clientRef.current?.request(trimmed, chatId, extra);
    99	    },
   100	    [chatId],
   101	  );
   102	
   103	  const cancel = React.useCallback(() => {
   104	    clientRef.current?.cancel(state.threadId);
   105	    awaitingSendRef.current = false;
   106	    setState((s) => ({ ...s, streaming: false, text: "" }));
   107	  }, [state.threadId]);
   108	
   109	  const reset = React.useCallback(() => {
   110	    awaitingSendRef.current = false;
   111	    setState({ connected: !!state.connected, streaming: false, text: "" });
   112	  }, [state.connected]);
   113	
   114	  return { ...state, send, cancel, reset };
   115	}


===== FILE: frontend/src/lib/utils.ts =====
     1	// 📁 frontend/app/lib/utils.ts
     2	export function cn(...classes: (string | false | null | undefined)[]) {
     3	  return classes.filter(Boolean).join(" ");
     4	}


===== FILE: frontend/src/lib/ws.ts =====
     1	// WebSocket-Client mit Token-Refresh, Reconnect, Heartbeat und Stream-Events.
     2	
     3	export type StreamStartPayload = { threadId: string; agent?: string };
     4	export type StreamDeltaPayload = { delta: string; done?: boolean };
     5	export type StreamDonePayload = { threadId?: string };
     6	
     7	export type ChatWsEvents = {
     8	  onOpen?: () => void;
     9	  onClose?: (ev: CloseEvent) => void;
    10	  onError?: (ev: Event) => void;
    11	  onMessage?: (msg: unknown) => void;
    12	  onStreamStart?: (p: StreamStartPayload) => void;
    13	  onStreamDelta?: (p: StreamDeltaPayload) => void;
    14	  onStreamDone?: (p: StreamDonePayload) => void;
    15	  onUiAction?: (ui: any) => void; // ← hinzugefügt
    16	};
    17	
    18	export type WSOptions = {
    19	  token?: string; // Fallback-Token (besser: getToken)
    20	  url?: string; // ws[s]://… oder Pfad (/api/v1/ai/ws)
    21	  protocols?: string | string[];
    22	  heartbeatMs?: number;
    23	  maxBackoffMs?: number;
    24	  getToken?: () => Promise<string | undefined>;
    25	};
    26	
    27	function wsOrigin(): { proto: "ws:" | "wss:"; host: string } {
    28	  const { protocol, host } = window.location;
    29	  return { proto: protocol === "https:" ? "wss:" : "ws:", host };
    30	}
    31	
    32	function withToken(urlOrPath: string, token: string | undefined): string {
    33	  const { proto, host } = wsOrigin();
    34	  const isAbs = urlOrPath.startsWith("ws://") || urlOrPath.startsWith("wss://");
    35	  const base = isAbs ? urlOrPath : `${proto}//${host}${urlOrPath.startsWith("/") ? "" : "/"}${urlOrPath}`;
    36	  if (!token) return base;
    37	  const sep = base.includes("?") ? "&" : "?";
    38	  return `${base}${sep}token=${encodeURIComponent(token)}`;
    39	}
    40	
    41	function safeParse(s: string): unknown {
    42	  try {
    43	    return JSON.parse(s);
    44	  } catch {
    45	    return s;
    46	  }
    47	}
    48	
    49	class ChatWsClient {
    50	  private ws?: WebSocket;
    51	  private hb?: number;
    52	  private backoff = 1000;
    53	  private closed = false;
    54	  private openPromise?: Promise<void>;
    55	  private started = false;
    56	  private lastThreadId?: string;
    57	  private firedNeedParams = false;
    58	
    59	  private readonly opts: Required<Pick<WSOptions, "heartbeatMs" | "maxBackoffMs">> & Omit<WSOptions, "heartbeatMs" | "maxBackoffMs">;
    60	  private readonly ev: ChatWsEvents;
    61	  private readonly subs = new Set<(msg: unknown) => void>();
    62	
    63	  constructor(options: WSOptions & ChatWsEvents) {
    64	    this.opts = {
    65	      url: options.url ?? "/api/v1/ai/ws",
    66	      protocols: options.protocols ?? ["json"],
    67	      heartbeatMs: options.heartbeatMs ?? 15000,
    68	      maxBackoffMs: options.maxBackoffMs ?? 30000,
    69	      token: options.token,
    70	      getToken: options.getToken,
    71	    };
    72	    this.ev = {
    73	      onOpen: options.onOpen,
    74	      onClose: options.onClose,
    75	      onError: options.onError,
    76	      onMessage: options.onMessage,
    77	      onStreamStart: options.onStreamStart,
    78	      onStreamDelta: options.onStreamDelta,
    79	      onStreamDone: options.onStreamDone,
    80	      onUiAction: options.onUiAction, // ← hinzugefügt
    81	    };
    82	  }
    83	
    84	  async connect(): Promise<void> {
    85	    if (this.openPromise) return this.openPromise;
    86	    this.closed = false;
    87	
    88	    this.openPromise = new Promise<void>(async (resolve, reject) => {
    89	      let token: string | undefined = undefined;
    90	      try {
    91	        token = (await this.opts.getToken?.()) ?? this.opts.token;
    92	      } catch {}
    93	
    94	      const url = withToken(this.opts.url!, token);
    95	      try {
    96	        this.ws = new WebSocket(url, this.opts.protocols as string[]);
    97	      } catch (e) {
    98	        reject(e);
    99	        return;
   100	      }
   101	
   102	      const ws = this.ws;
   103	
   104	      ws.onopen = () => {
   105	        this.backoff = 1000;
   106	        this.started = false;
   107	        this.firedNeedParams = false;
   108	        this.startHeartbeat();
   109	        this.ev.onOpen?.();
   110	        resolve();
   111	      };
   112	
   113	      ws.onmessage = (ev) => {
   114	        const data = typeof ev.data === "string" ? safeParse(ev.data) : ev.data;
   115	        this.ev.onMessage?.(data);
   116	        for (const cb of this.subs) cb(data);
   117	        this.routeStreamEvents(data);
   118	      };
   119	
   120	      ws.onclose = (ev) => {
   121	        this.stopHeartbeat();
   122	        this.ev.onClose?.(ev);
   123	        if (!this.closed) this.scheduleReconnect();
   124	      };
   125	
   126	      ws.onerror = (ev) => {
   127	        this.ev.onError?.(ev);
   128	      };
   129	    });
   130	
   131	    return this.openPromise;
   132	  }
   133	
   134	  subscribe(handler: (msg: unknown) => void): () => void {
   135	    this.subs.add(handler);
   136	    return () => this.subs.delete(handler);
   137	  }
   138	
   139	  private sendInternal(payload: unknown): void {
   140	    const s = JSON.stringify(payload);
   141	    if (this.ws && this.ws.readyState === WebSocket.OPEN) this.ws.send(s);
   142	  }
   143	
   144	  send(payload: unknown): void {
   145	    this.sendInternal(payload);
   146	  }
   147	
   148	  request(input: string, chatId = "default", extra?: Record<string, unknown>): void {
   149	    this.firedNeedParams = false;
   150	    this.sendInternal({ chat_id: chatId || "default", input, ...(extra || {}) });
   151	  }
   152	
   153	  cancel(threadId?: string): void {
   154	    const tid = threadId ?? this.lastThreadId ?? "default";
   155	    this.sendInternal({ type: "cancel", thread_id: tid });
   156	  }
   157	
   158	  close(): void {
   159	    this.closed = true;
   160	    this.stopHeartbeat();
   161	    try {
   162	      this.ws?.close();
   163	    } catch {}
   164	    this.ws = undefined;
   165	    this.openPromise = undefined;
   166	  }
   167	
   168	  private startHeartbeat() {
   169	    this.stopHeartbeat();
   170	    this.hb = window.setInterval(() => {
   171	      if (!this.ws || this.ws.readyState !== WebSocket.OPEN) return;
   172	      try {
   173	        this.ws.send(JSON.stringify({ type: "ping", ts: Date.now() }));
   174	      } catch {}
   175	    }, this.opts.heartbeatMs);
   176	  }
   177	
   178	  private stopHeartbeat() {
   179	    if (this.hb) {
   180	      window.clearInterval(this.hb);
   181	      this.hb = undefined;
   182	    }
   183	  }
   184	
   185	  private scheduleReconnect() {
   186	    const delay = Math.min(this.backoff, this.opts.maxBackoffMs);
   187	    this.backoff = Math.min(this.backoff * 2, this.opts.maxBackoffMs);
   188	    window.setTimeout(() => {
   189	      if (this.closed) return;
   190	      this.openPromise = undefined; // neues connect() → neuer Token
   191	      this.connect().catch(() => this.scheduleReconnect());
   192	    }, delay);
   193	  }
   194	
   195	  private routeStreamEvents(raw: unknown) {
   196	    const d = raw as any;
   197	
   198	    // Backend: {"phase":"starting", thread_id, ...}
   199	    if (d?.phase === "starting") {
   200	      this.started = true;
   201	      const tid = d.thread_id ?? "default";
   202	      this.lastThreadId = tid;
   203	      this.ev.onStreamStart?.({ threadId: tid, agent: d?.agent });
   204	    }
   205	
   206	    // Debug-Events: {"event":"dbg", "meta":{"langgraph_node":"ask_missing"}, ...}
   207	    if (d?.event === "dbg") {
   208	      const node = (d?.meta?.langgraph_node || d?.meta?.run_name || d?.name || "").toString().toLowerCase();
   209	      if (!this.firedNeedParams && node === "ask_missing") {
   210	        this.firedNeedParams = true;
   211	        window.dispatchEvent(new CustomEvent("sai:need-params", { detail: { node } }));
   212	        // echtes UI-Open-Event
   213	        window.dispatchEvent(new CustomEvent("sealai:ui_action", { detail: { ui_action: "open_form" } }));
   214	      }
   215	    }
   216	
   217	    // UI-Events: {"event":"ui_action", ...} oder Backward-Compat {"ui_event": {...}}: {"event":"ui_action", ...} oder Backward-Compat {"ui_event": {...}}
   218	      if (d?.event === "ui_action" || d?.ui_event || typeof d?.ui_action !== "undefined") {
   219	        const ua = typeof d?.ui_action === "string"
   220	          ? { ui_action: d.ui_action }
   221	          : (d?.ui_event && typeof d.ui_event === "object" ? d.ui_event : d);
   222	        this.ev.onUiAction?.(ua);
   223	        window.dispatchEvent(new CustomEvent("sealai:ui_action", { detail: ua }));
   224	      }
   225	
   226	    // Token-Stream
   227	    if (typeof d?.delta !== "undefined") {
   228	      if (
   229	        !this.firedNeedParams &&
   230	        typeof d.delta === "string" &&
   231	        /mir fehlen noch folgende angaben|kannst du mir diese bitte nennen|präzise.*empfehlung.*brauche.*noch kurz|pack die werte gern.*eine zeile/i.test(d.delta)
   232	      ) {
   233	        this.firedNeedParams = true;
   234	        window.dispatchEvent(new CustomEvent("sai:need-params", { detail: { hint: "text" } }));
   235	        window.dispatchEvent(new CustomEvent("sealai:ui_action", { detail: { ui_action: "open_form" } }));
   236	      }
   237	      this.ev.onStreamDelta?.({ delta: String(d.delta), done: false });
   238	    }
   239	
   240	    // Optional final text
   241	    if (d?.final?.text && !d?.delta) {
   242	      if (
   243	        !this.firedNeedParams &&
   244	        typeof d.final.text === "string" &&
   245	        /mir fehlen noch folgende angaben|kannst du mir diese bitte nennen|präzise.*empfehlung.*brauche.*noch kurz|pack die werte gern.*eine zeile/i.test(d.final.text)
   246	      ) {
   247	        this.firedNeedParams = true;
   248	        window.dispatchEvent(new CustomEvent("sai:need-params", { detail: { hint: "final" } }));
   249	        window.dispatchEvent(new CustomEvent("sealai:ui_action", { detail: { ui_action: "open_form" } }));
   250	      }
   251	      this.ev.onStreamDelta?.({ delta: String(d.final.text), done: false });
   252	    }
   253	
   254	    // Done
   255	    if (d?.event === "done" || d?.done === true) {
   256	      this.ev.onStreamDone?.({ threadId: d.thread_id });
   257	    }
   258	
   259	    // LCEL / frames
   260	    if (d?.message) {
   261	      if (!this.started) {
   262	        this.started = true;
   263	        const tid = d?.meta?.thread_id ?? "default";
   264	        this.lastThreadId = tid;
   265	        this.ev.onStreamStart?.({ threadId: tid, agent: d?.message?.name });
   266	      }
   267	      const content = d?.message?.data?.content ?? d?.message?.content;
   268	      if (typeof content === "string") this.ev.onStreamDelta?.({ delta: content, done: false });
   269	    }
   270	  }
   271	}
   272	
   273	export default ChatWsClient;


===== FILE: frontend/src/middleware.ts =====
     1	// frontend/src/middleware.ts
     2	import { NextRequest, NextResponse } from "next/server";
     3	import { getToken } from "next-auth/jwt";
     4	
     5	const PUBLIC_FILE = /\.(.*)$/;
     6	
     7	// NextAuth v4/v5 mögliche Session-Cookies
     8	const SESSION_COOKIES = [
     9	  "__Secure-authjs.session-token",
    10	  "authjs.session-token",
    11	  "__Secure-next-auth.session-token",
    12	  "next-auth.session-token",
    13	];
    14	
    15	function hasSessionCookie(req: NextRequest) {
    16	  return SESSION_COOKIES.some((n) => !!req.cookies.get(n)?.value);
    17	}
    18	
    19	export async function middleware(req: NextRequest) {
    20	  const { pathname } = req.nextUrl;
    21	
    22	  // nie abfangen
    23	  if (
    24	    pathname.startsWith("/api/auth") ||
    25	    pathname.startsWith("/_next") ||
    26	    pathname.startsWith("/static") ||
    27	    pathname === "/favicon.ico" ||
    28	    PUBLIC_FILE.test(pathname)
    29	  ) {
    30	    return NextResponse.next();
    31	  }
    32	
    33	  // schneller Cookie-Check
    34	  const hasCookie = hasSessionCookie(req);
    35	
    36	  // zusätzlich JWT prüfen (falls SECRET passt)
    37	  let hasJwt = false;
    38	  try {
    39	    const tok = await getToken({ req });
    40	    hasJwt = !!tok;
    41	  } catch {
    42	    // ignore
    43	  }
    44	
    45	  if (hasCookie || hasJwt) return NextResponse.next();
    46	
    47	  // nicht eingeloggt -> Keycloak-Login ERZWINGEN (prompt=login)
    48	  const url = req.nextUrl.clone();
    49	  const base = process.env.NEXTAUTH_URL ?? `${url.protocol}//${url.host}`;
    50	  const redirect = new URL("/api/auth/signin/keycloak", base);
    51	  redirect.searchParams.set("callbackUrl", url.pathname + url.search);
    52	  redirect.searchParams.set("prompt", "login"); // wichtig: kein stilles SSO
    53	
    54	  return NextResponse.redirect(redirect);
    55	}
    56	
    57	export const config = {
    58	  matcher: ["/dashboard/:path*"],
    59	};


===== FILE: frontend/src/styles/chat-markdown.css =====
     1	/* ===== Grok-like Markdown UX for SealAI ===== */
     2	:root{
     3	  --md-font-family: Inter, system-ui, -apple-system, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", "Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol", sans-serif;
     4	  --md-font-size: 15.5px;
     5	  --md-line-height: 1.65;
     6	  --md-tight-line: 1.45;
     7	
     8	  --md-fg: #111827;        /* text */
     9	  --md-fg-soft: #4B5563;   /* muted */
    10	  --md-fg-strong: #0F172A;
    11	
    12	  --md-hr: #E5E7EB;
    13	  --md-border: #E5E7EB;
    14	
    15	  --code-bg: #0F172A;      /* Grok: dark code blocks */
    16	  --code-fg: #F9FAFB;
    17	  --code-inline-bg: #F3F4F6;
    18	  --code-inline-fg: #374151;
    19	
    20	  --space-xxs: 0.25rem;
    21	  --space-xs:  0.375rem;
    22	  --space-sm:  0.5rem;
    23	  --space-md:  0.75rem;   /* primary paragraph gap ~12px */
    24	  --space-lg:  1rem;
    25	  --space-xl:  1.25rem;
    26	}
    27	
    28	/* Root container used by MarkdownMessage */
    29	.chat-markdown,
    30	.markdown-content,
    31	.markdown-body{
    32	  font-family: var(--md-font-family);
    33	  font-size: var(--md-font-size);
    34	  line-height: var(--md-line-height);
    35	  color: var(--md-fg);
    36	  -webkit-font-smoothing: antialiased;
    37	  -moz-osx-font-smoothing: grayscale;
    38	}
    39	
    40	/* Paragraphs: compact like Grok */
    41	.chat-markdown p,
    42	.markdown-content p,
    43	.markdown-body p{
    44	  margin: var(--space-md) 0;
    45	}
    46	
    47	/* Headings: clear, not oversized */
    48	.chat-markdown h1, .markdown-content h1, .markdown-body h1{
    49	  font-size: 1.35rem;
    50	  line-height: var(--md-tight-line);
    51	  font-weight: 600;
    52	  letter-spacing: -0.01em;
    53	  margin: var(--space-lg) 0 var(--space-sm);
    54	  color: var(--md-fg);
    55	}
    56	.chat-markdown h2, .markdown-content h2, .markdown-body h2{
    57	  font-size: 1.18rem;
    58	  line-height: var(--md-tight-line);
    59	  font-weight: 600;
    60	  margin: var(--space-lg) 0 var(--space-sm);
    61	}
    62	.chat-markdown h3, .markdown-content h3, .markdown-body h3{
    63	  font-size: 1.05rem;
    64	  line-height: var(--md-tight-line);
    65	  font-weight: 600;
    66	  margin: var(--space-md) 0 var(--space-xs);
    67	}
    68	.chat-markdown h4, .markdown-content h4, .markdown-body h4{
    69	  font-size: .98rem;
    70	  line-height: var(--md-tight-line);
    71	  font-weight: 600;
    72	  margin: var(--space-sm) 0 var(--space-xxs);
    73	}
    74	
    75	/* Links */
    76	.chat-markdown a,
    77	.markdown-content a,
    78	.markdown-body a{
    79	  color: #2563EB;
    80	  text-decoration: none;
    81	}
    82	.chat-markdown a:hover,
    83	.markdown-content a:hover,
    84	.markdown-body a:hover{
    85	  text-decoration: underline;
    86	}
    87	
    88	/* Blockquote: subtle */
    89	.chat-markdown blockquote,
    90	.markdown-content blockquote,
    91	.markdown-body blockquote{
    92	  margin: var(--space-md) 0;
    93	  padding: var(--space-xs) var(--space-md);
    94	  border-left: 3px solid var(--md-border);
    95	  color: var(--md-fg-soft);
    96	  background: #FAFAFA;
    97	  border-radius: 0 8px 8px 0;
    98	}
    99	
   100	/* Strong */
   101	.chat-markdown strong,
   102	.markdown-content strong,
   103	.markdown-body strong{
   104	  font-weight: 600;
   105	  color: var(--md-fg-strong);
   106	}
   107	
   108	/* Lists: top-level compact bullets, nested level as indented "–" dashes */
   109	.chat-markdown ul,
   110	.markdown-content ul,
   111	.markdown-body ul,
   112	.chat-markdown ol,
   113	.markdown-content ol,
   114	.markdown-body ol{
   115	  margin: var(--space-sm) 0;
   116	  padding-left: 1.15rem;
   117	}
   118	
   119	.chat-markdown li,
   120	.markdown-content li,
   121	.markdown-body li{
   122	  margin: 0.18rem 0;
   123	}
   124	
   125	/* Nested UL one level deeper → dash style with extra indent */
   126	.chat-markdown ul ul,
   127	.markdown-content ul ul,
   128	.markdown-body ul ul{
   129	  list-style: none;
   130	  padding-left: 1.25rem;
   131	  margin-top: 0.15rem;
   132	}
   133	.chat-markdown ul ul > li,
   134	.markdown-content ul ul > li,
   135	.markdown-body ul ul > li{
   136	  position: relative;
   137	  padding-left: 0.8rem;
   138	}
   139	.chat-markdown ul ul > li::before,
   140	.markdown-content ul ul > li::before,
   141	.markdown-body ul ul > li::before{
   142	  content: "–";
   143	  position: absolute;
   144	  left: 0;
   145	  top: 0;
   146	  color: #374151;
   147	}
   148	
   149	/* Horizontal rule */
   150	.chat-markdown hr,
   151	.markdown-content hr,
   152	.markdown-body hr{
   153	  border: none;
   154	  border-top: 1px solid var(--md-hr);
   155	  margin: var(--space-lg) 0;
   156	}
   157	
   158	/* Images */
   159	.chat-markdown img,
   160	.markdown-content img,
   161	.markdown-body img{
   162	  max-width: 100%;
   163	  height: auto;
   164	  border-radius: 8px;
   165	  margin: var(--space-sm) 0;
   166	}
   167	
   168	/* Inline code */
   169	.chat-markdown code,
   170	.markdown-content code,
   171	.markdown-body code{
   172	  font-size: 0.92em;
   173	  padding: 0.15em 0.35em;
   174	  border-radius: 0.3em;
   175	  background: var(--code-inline-bg);
   176	  color: var(--code-inline-fg);
   177	}
   178	
   179	/* Code blocks */
   180	.chat-markdown pre,
   181	.markdown-content pre,
   182	.markdown-body pre{
   183	  background: var(--code-bg);
   184	  color: var(--code-fg);
   185	  border-radius: 10px;
   186	  padding: 0.9em 1em;
   187	  margin: var(--space-sm) 0;
   188	  overflow-x: auto;
   189	  font-size: 0.92rem;
   190	  line-height: 1.5;
   191	}
   192	
   193	/* Tables */
   194	.chat-markdown table,
   195	.markdown-content table,
   196	.markdown-body table{
   197	  border-collapse: collapse;
   198	  margin: var(--space-sm) 0;
   199	  width: 100%;
   200	}
   201	.chat-markdown th, .chat-markdown td,
   202	.markdown-content th, .markdown-content td,
   203	.markdown-body th, .markdown-body td{
   204	  border: 1px solid var(--md-border);
   205	  padding: 0.45rem 0.6rem;
   206	}
   207	.chat-markdown th,
   208	.markdown-content th,
   209	.markdown-body th{
   210	  background: #F8FAFC;
   211	  font-weight: 600;
   212	}
   213	
   214	/* Trim first/last margins inside message bubbles */
   215	.chat-markdown > :first-child,
   216	.markdown-content > :first-child,
   217	.markdown-body > :first-child{ margin-top: 0 !important; }
   218	.chat-markdown > :last-child,
   219	.markdown-content > :last-child,
   220	.markdown-body > :last-child{ margin-bottom: 0 !important; }
   221	
   222	/* ===== Components used by MarkdownMessage (cm-*) ===== */
   223	.cm-p,
   224	.cm-h1, .cm-h2, .cm-h3, .cm-h4,
   225	.cm-li, .cm-ul, .cm-ol,
   226	.cm-quote, .cm-a, .cm-th, .cm-td, .cm-table{
   227	  color: var(--md-fg);
   228	  font-size: var(--md-font-size);
   229	  line-height: var(--md-line-height);
   230	}
   231	
   232	.cm-h1 { font-size: 1.35rem; font-weight: 600; margin: var(--space-lg) 0 var(--space-sm); }
   233	.cm-h2 { font-size: 1.18rem; font-weight: 600; margin: var(--space-lg) 0 var(--space-sm); }
   234	.cm-h3 { font-size: 1.05rem; font-weight: 600; margin: var(--space-md) 0 var(--space-xs); }
   235	.cm-h4 { font-size: .98rem;  font-weight: 600; margin: var(--space-sm) 0 var(--space-xxs); }
   236	
   237	.cm-quote{
   238	  border-left: 3px solid var(--md-border);
   239	  background: #FAFAFA;
   240	  padding: var(--space-xs) var(--space-md);
   241	  border-radius: 0 8px 8px 0;
   242	  color: var(--md-fg-soft);
   243	}
   244	
   245	.cm-a{ color:#2563EB; text-decoration:none; }
   246	.cm-a:hover{ text-decoration:underline; }
   247	
   248	.cm-th, .cm-td{
   249	  border: 1px solid var(--md-border);
   250	  padding: 0.45rem 0.6rem;
   251	}
   252	.cm-th{ background:#F8FAFC; font-weight:600; }
   253	


===== FILE: frontend/src/styles/globals.css =====
     1	/* Inter – leicht, kompakt */
     2	@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');
     3	
     4	@tailwind base;
     5	@tailwind components;
     6	@tailwind utilities;
     7	
     8	/* Basistypo: minimal kleiner für “Grok-Look” */
     9	html {
    10	  font-family: 'Inter', system-ui, -apple-system, Segoe UI, Roboto, 'Helvetica Neue', Arial, 'Noto Sans', 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', sans-serif;
    11	  font-size: 15px;                 /* ↓ von 16px */
    12	  -webkit-font-smoothing: antialiased;
    13	  -moz-osx-font-smoothing: grayscale;
    14	  text-rendering: optimizeLegibility;
    15	  font-feature-settings: "liga","calt","ss01","cv05";
    16	  font-variation-settings: normal;
    17	}
    18	
    19	body {
    20	  @apply bg-white text-gray-800;
    21	}
    22	
    23	/* Scrollbar dezent */
    24	.scrollbar-thin { scrollbar-width: thin; }
    25	.scrollbar-thumb-blue-200::-webkit-scrollbar-thumb { background-color: #bfdbfe; }
    26	.scrollbar-thumb-blue-200::-webkit-scrollbar { width: 8px; }
    27	
    28	.chat-scroll-container { scrollbar-gutter: stable both-edges; }
    29	
    30	/* Optional generelles Ausblenden der Scrollbar im Chatbereich */
    31	::-webkit-scrollbar { width: 0px !important; background: transparent !important; }
    32	.chat-scroll-hide { scrollbar-width: none !important; -ms-overflow-style: none !important; }
    33	.chat-scroll-hide::-webkit-scrollbar { display: none !important; }
    34	
    35	/* Markdown-Reset: kompakte Abstände (falls andere Render-Pfade) */
    36	.markdown-body, .markdown-content { font-family: inherit; }
    37	
    38	.markdown-body p,
    39	.markdown-body ul,
    40	.markdown-body ol,
    41	.markdown-body li,
    42	.markdown-body blockquote,
    43	.markdown-content p,
    44	.markdown-content ul,
    45	.markdown-content ol,
    46	.markdown-content li,
    47	.markdown-content blockquote {
    48	  margin: 0 !important;
    49	  padding: 0 !important;
    50	  line-height: 1.45;
    51	}
    52	
    53	.markdown-body ul,
    54	.markdown-body ol,
    55	.markdown-content ul,
    56	.markdown-content ol {
    57	  padding-left: 1.25rem !important;
    58	}
    59	
    60	.markdown-body li,
    61	.markdown-content li {
    62	  display: list-item !important;
    63	  margin: 0.18rem 0 !important;
    64	}
    65	
    66	/* Optional kompakter Code außerhalb von cm-Styles */
    67	.markdown-body code, .markdown-content code {
    68	  font-size: 90%;
    69	  padding: 0.15em 0.35em;
    70	  border-radius: 0.3em;
    71	  background: #f3f4f6;
    72	  color: #374151;
    73	}
    74	
    75	.markdown-body pre, .markdown-content pre {
    76	  background: #111827;
    77	  color: #f3f4f6;
    78	  border-radius: 8px;
    79	  padding: 0.9em;
    80	  margin: 0.45em 0;
    81	  font-size: 92%;
    82	  overflow-x: auto;
    83	}
    84	
    85	/* Kleinere, angenehm dichte Tabellen global */
    86	.markdown-body table, .markdown-content table {
    87	  border-collapse: collapse;
    88	  margin: 0.45em 0 !important;
    89	}
    90	.markdown-body th, .markdown-content th {
    91	  background: #f4f4f4;
    92	  font-weight: 600;
    93	}


===== FILE: frontend/src/types/chat.ts =====
     1	export type Message = {
     2	  role: "user" | "assistant" | "system";
     3	  content: string;
     4	};


===== FILE: frontend/src/types/markdown-to-jsx.d.ts =====
     1	declare module 'markdown-to-jsx';


===== FILE: frontend/src/types/next-auth.d.ts =====
     1	import 'next-auth';
     2	
     3	declare module 'next-auth' {
     4	  interface Session {
     5	    accessToken?: string;
     6	    idToken?: string;
     7	  }
     8	}
     9	
    10	export {};

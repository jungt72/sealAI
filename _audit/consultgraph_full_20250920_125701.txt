# ConsultGraph Snapshot
# Repo: /root/sealai
# Zeitpunkt: 2025-09-20T12:57:02+00:00
# Dateien: 67

################################################################################
# FILE: backend/app/api/v1/endpoints/ai.py
################################################################################
     1	# backend/app/api/v1/endpoints/ai.py
     2	from __future__ import annotations
     3	
     4	import os
     5	import re
     6	from typing import Optional
     7	
     8	from fastapi import APIRouter, HTTPException, Request
     9	from pydantic import BaseModel, Field
    10	from redis import Redis
    11	
    12	# Nur die Consult-Funktion nutzen; Checkpointer holen wir aus app.state
    13	from app.services.langgraph.graph.consult.io import invoke_consult as _invoke_consult
    14	
    15	# ─────────────────────────────────────────────────────────────
    16	# ENV / Redis STM (Short-Term Memory)
    17	# ─────────────────────────────────────────────────────────────
    18	REDIS_URL = os.getenv("REDIS_URL", "redis://redis:6379/0")
    19	STM_PREFIX = os.getenv("STM_PREFIX", "chat:stm")
    20	STM_TTL_SEC = int(os.getenv("STM_TTL_SEC", "604800"))  # 7 Tage
    21	
    22	def _stm_key(thread_id: str) -> str:
    23	    return f"{STM_PREFIX}:{thread_id}"
    24	
    25	def _get_redis() -> Redis:
    26	    return Redis.from_url(REDIS_URL, decode_responses=True)
    27	
    28	def _set_stm(thread_id: str, key: str, value: str) -> None:
    29	    r = _get_redis()
    30	    skey = _stm_key(thread_id)
    31	    r.hset(skey, key, value)
    32	    r.expire(skey, STM_TTL_SEC)
    33	
    34	def _get_stm(thread_id: str, key: str) -> Optional[str]:
    35	    r = _get_redis()
    36	    skey = _stm_key(thread_id)
    37	    v = r.hget(skey, key)
    38	    return v if (isinstance(v, str) and v.strip()) else None
    39	
    40	# ─────────────────────────────────────────────────────────────
    41	# Intent: “merke dir … / remember …” (optional)
    42	# ─────────────────────────────────────────────────────────────
    43	RE_REMEMBER_NUM  = re.compile(r"\b(merke\s*dir|merk\s*dir|remember)\b[^0-9\-+]*?(-?\d+(?:[.,]\d+)?)", re.I)
    44	RE_REMEMBER_FREE = re.compile(r"\b(merke\s*dir|merk\s*dir|remember)\b[:\s]+(.+)$", re.I)
    45	RE_ASK_NUMBER    = re.compile(r"\b(welche\s+zahl\s+meinte\s+ich|what\s+number\s+did\s+i\s+mean)\b", re.I)
    46	RE_ASK_FREE      = re.compile(r"\b(woran\s+erinn?erst\s+du\s+dich|what\s+did\s+you\s+remember)\b", re.I)
    47	
    48	def _normalize_num_str(s: str) -> str:
    49	    return (s or "").replace(",", ".")
    50	
    51	def _maybe_handle_memory_intent(text: str, thread_id: str) -> Optional[str]:
    52	    t = (text or "").strip()
    53	    if not t:
    54	        return None
    55	
    56	    m = RE_REMEMBER_NUM.search(t)
    57	    if m:
    58	        raw = m.group(2)
    59	        norm = _normalize_num_str(raw)
    60	        _set_stm(thread_id, "last_number", norm)
    61	        return f"Alles klar – ich habe mir **{raw}** gemerkt."
    62	
    63	    m2 = RE_REMEMBER_FREE.search(t)
    64	    if m2 and not m:
    65	        val = (m2.group(2) or "").strip()
    66	        if val:
    67	            _set_stm(thread_id, "last_note", val)
    68	            return "Notiert. 👍"
    69	
    70	    if RE_ASK_NUMBER.search(t):
    71	        v = _get_stm(thread_id, "last_number")
    72	        return f"Du meintest **{v}**." if v else "Ich habe dazu noch keine Zahl gespeichert."
    73	
    74	    if RE_ASK_FREE.search(t):
    75	        v = _get_stm(thread_id, "last_note")
    76	        return f"Ich habe mir gemerkt: “{v}”." if v else "Ich habe dazu noch nichts gespeichert."
    77	
    78	    return None
    79	
    80	# ─────────────────────────────────────────────────────────────
    81	# API
    82	# ─────────────────────────────────────────────────────────────
    83	router = APIRouter()  # KEIN prefix hier – der übergeordnete Router hängt '/ai' an.
    84	
    85	class ChatRequest(BaseModel):
    86	    chat_id: str = Field(default="default", description="Konversations-ID")
    87	    input_text: str = Field(..., description="Nutzertext")
    88	
    89	class ChatResponse(BaseModel):
    90	    text: str
    91	
    92	@router.post("/beratung", response_model=ChatResponse)
    93	async def beratung(request: Request, payload: ChatRequest) -> ChatResponse:
    94	    """
    95	    Einstieg in den Consult-Flow. Nutzt (falls vorhanden) den Checkpointer aus app.state.
    96	    Zusätzlich: einfache STM-Merkfunktion (merke dir … / welche Zahl …?).
    97	    """
    98	    user_text = (payload.input_text or "").strip()
    99	    if not user_text:
   100	        raise HTTPException(status_code=400, detail="input_text empty")
   101	
   102	    thread_id = f"api:{payload.chat_id}"
   103	
   104	    # 1) Memory-Intents kurz-circuited beantworten
   105	    mem = _maybe_handle_memory_intent(user_text, thread_id)
   106	    if mem:
   107	        return ChatResponse(text=mem)
   108	
   109	    # 2) Consult-Flow aufrufen (mit optionalem Checkpointer)
   110	    checkpointer = getattr(request.app.state, "swarm_checkpointer", None)
   111	    out = _invoke_consult(user_text, thread_id=thread_id, checkpointer=checkpointer)
   112	    return ChatResponse(text=out)

################################################################################
# FILE: backend/app/api/v1/endpoints/chat_ws.py
################################################################################
     1	# backend/app/api/v1/endpoints/chat_ws.py
     2	from __future__ import annotations
     3	
     4	import os
     5	import re
     6	import json
     7	import asyncio
     8	from typing import Any, Dict, Iterable, List, Optional, Tuple
     9	import redis
    10	
    11	from fastapi import APIRouter, WebSocket, WebSocketDisconnect
    12	from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
    13	from langchain_core.messages.ai import AIMessageChunk
    14	
    15	# ursprünglicher Guard, kann 403 werfen
    16	from app.api.v1.dependencies.auth import guard_websocket
    17	from app.services.langgraph.llm_factory import get_llm as make_llm
    18	from app.services.langgraph.redis_lifespan import get_redis_checkpointer
    19	from app.services.langgraph.prompt_registry import get_agent_prompt
    20	
    21	from app.services.langgraph.graph.consult.memory_utils import (
    22	    read_history as stm_read_history,
    23	    write_message as stm_write_message,
    24	)
    25	
    26	from app.services.langgraph.tools import long_term_memory as ltm
    27	
    28	router = APIRouter()
    29	
    30	# --- Tunables / Env ---
    31	COALESCE_MIN_CHARS      = int(os.getenv("WS_COALESCE_MIN_CHARS", "24"))
    32	COALESCE_MAX_LAT_MS     = float(os.getenv("WS_COALESCE_MAX_LAT_MS", "60"))
    33	IDLE_TIMEOUT_SEC        = int(os.getenv("WS_IDLE_TIMEOUT_SEC", "20"))
    34	FIRST_TOKEN_TIMEOUT_MS  = int(os.getenv("WS_FIRST_TOKEN_TIMEOUT_MS", "5000"))
    35	WS_INPUT_MAX_CHARS      = int(os.getenv("WS_INPUT_MAX_CHARS", "4000"))
    36	WS_RATE_LIMIT_PER_MIN   = int(os.getenv("WS_RATE_LIMIT_PER_MIN", "30"))
    37	MICRO_CHUNK_CHARS       = int(os.getenv("WS_MICRO_CHUNK_CHARS", "0"))
    38	EMIT_FINAL_TEXT         = os.getenv("WS_EMIT_FINAL_TEXT", "0") == "1"
    39	DEBUG_EVENTS            = os.getenv("WS_DEBUG_EVENTS", "0") == "1"
    40	WS_EVENT_TIMEOUT_SEC    = int(os.getenv("WS_EVENT_TIMEOUT_SEC", "60"))
    41	FORCE_SYNC_FALLBACK     = os.getenv("WS_FORCE_SYNC", "0") == "1"
    42	
    43	FLUSH_ENDINGS: Tuple[str, ...] = (". ", "? ", "! ", "\n\n", ":", ";", "…", ", ", ") ", "] ", " }")
    44	
    45	def _env_stream_nodes() -> set[str]:
    46	    raw = os.getenv("WS_STREAM_NODES", "*").strip()
    47	    if not raw or raw in {"*", "all"}:
    48	        return {"*"}
    49	    return {x.strip().lower() for x in raw.split(",") if x.strip()}
    50	
    51	STREAM_NODES = _env_stream_nodes()
    52	
    53	def _get_rl_redis(app) -> Optional[redis.Redis]:
    54	    client = getattr(app.state, "redis_rl", None)
    55	    if client is not None:
    56	        return client
    57	    url = os.getenv("REDIS_URL")
    58	    if not url:
    59	        return None
    60	    try:
    61	        client = redis.Redis.from_url(url, decode_responses=True)
    62	        app.state.redis_rl = client
    63	        return client
    64	    except Exception:
    65	        return None
    66	
    67	def _piece_from_llm_chunk(chunk: Any) -> Optional[str]:
    68	    if isinstance(chunk, AIMessageChunk):
    69	        return chunk.content or ""
    70	    txt = getattr(chunk, "content", None)
    71	    if isinstance(txt, str) and txt:
    72	        return txt
    73	    ak = getattr(chunk, "additional_kwargs", None)
    74	    if isinstance(ak, dict):
    75	        for k in ("delta", "content", "text", "token"):
    76	            v = ak.get(k)
    77	            if isinstance(v, str) and v:
    78	                return v
    79	    if isinstance(chunk, dict):
    80	        for k in ("delta", "content", "text", "token"):
    81	            v = chunk.get(k)
    82	            if isinstance(v, str) and v:
    83	                return v
    84	    return None
    85	
    86	def _iter_text_from_chunk(chunk) -> Iterable[str]:
    87	    if isinstance(chunk, dict):
    88	        c = chunk.get("content")
    89	        if isinstance(c, str) and c:
    90	            yield c; return
    91	        d = chunk.get("delta")
    92	        if isinstance(d, str) and d:
    93	            yield d; return
    94	    content = getattr(chunk, "content", None)
    95	    if isinstance(content, str) and content:
    96	        yield content; return
    97	    if isinstance(content, list):
    98	        for part in content:
    99	            if isinstance(part, str):
   100	                yield part
   101	            elif isinstance(part, dict) and isinstance(part.get("text"), str):
   102	                yield part["text"]
   103	    ak = getattr(chunk, "additional_kwargs", None)
   104	    if isinstance(ak, dict):
   105	        for k in ("delta", "content", "text", "token"):
   106	            v = ak.get(k)
   107	            if isinstance(v, str) and v:
   108	                yield v
   109	
   110	_BOUNDARY_RX = re.compile(r"[ \n\t.,;:!?…)\]}]")
   111	
   112	def _micro_chunks(s: str) -> Iterable[str]:
   113	    n = MICRO_CHUNK_CHARS
   114	    if n <= 0 or len(s) <= n:
   115	        yield s; return
   116	    i = 0; L = len(s)
   117	    while i < L:
   118	        j = min(i + n, L); k = j
   119	        if j < L:
   120	            m = _BOUNDARY_RX.search(s, j, min(L, j + 40))
   121	            if m: k = m.end()
   122	        yield s[i:k]; i = k
   123	
   124	def _is_relevant_node(ev: Dict) -> bool:
   125	    if "*" in STREAM_NODES or "all" in STREAM_NODES:
   126	        return True
   127	    meta = ev.get("metadata") or {}; run  = ev.get("run") or {}
   128	    node = str(meta.get("langgraph_node") or "").lower()
   129	    run_name = str(run.get("name") or meta.get("run_name") or "").lower()
   130	    return (node in STREAM_NODES) or (run_name in STREAM_NODES)
   131	
   132	def _extract_texts(obj: Any) -> List[str]:
   133	    out: List[str] = []
   134	    if isinstance(obj, str) and obj.strip():
   135	        out.append(obj.strip()); return out
   136	    if isinstance(obj, dict):
   137	        for k in ("response", "final_text", "text", "answer"):
   138	            v = obj.get(k)
   139	            if isinstance(v, str) and v.strip():
   140	                out.append(v.strip())
   141	        msgs = obj.get("messages")
   142	        if isinstance(msgs, list):
   143	            for m in msgs:
   144	                if isinstance(m, AIMessage):
   145	                    c = getattr(m, "content", "")
   146	                    if isinstance(c, str) and c.strip():
   147	                        out.append(c.strip())
   148	                elif isinstance(m, dict):
   149	                    c = m.get("content")
   150	                    if isinstance(c, str) and c.strip():
   151	                        out.append(c.strip())
   152	        for k in ("output", "state", "final_state", "result"):
   153	            sub = obj.get(k)
   154	            out.extend(_extract_texts(sub))
   155	    elif isinstance(obj, list):
   156	        for it in obj:
   157	            out.extend(_extract_texts(it))
   158	    return out
   159	
   160	def _last_ai_text_from_result_like(obj: Dict[str, Any]) -> str:
   161	    texts = _extract_texts(obj)
   162	    return texts[-1].strip() if texts else ""
   163	
   164	REMEMBER_RX = re.compile(r"^\s*(?:!remember|remember|merke(?:\s*dir)?|speicher(?:e)?)\s*[:\-]?\s*(.+)$", re.I)
   165	GRAPH_BUILDER = os.getenv("GRAPH_BUILDER", "supervisor").lower()
   166	
   167	def _ensure_graph(app) -> None:
   168	    if getattr(app.state, "graph_async", None) is not None or getattr(app.state, "graph_sync", None) is not None:
   169	        return
   170	    if GRAPH_BUILDER == "supervisor":
   171	        from app.services.langgraph.supervisor_graph import build_supervisor_graph as build_graph
   172	    else:
   173	        from app.services.langgraph.graph.consult.build import build_consult_graph as build_graph
   174	    saver = None
   175	    try:
   176	        saver = get_redis_checkpointer(app)
   177	    except Exception:
   178	        saver = None
   179	    g = build_graph()
   180	    try:
   181	        compiled = g.compile(checkpointer=saver) if saver else g.compile()
   182	    except Exception:
   183	        compiled = g.compile()
   184	    app.state.graph_async = compiled
   185	    app.state.graph_sync  = compiled
   186	
   187	def _choose_subprotocol(ws: WebSocket) -> Optional[str]:
   188	    raw = ws.headers.get("sec-websocket-protocol")
   189	    if not raw:
   190	        return None
   191	    return raw.split(",")[0].strip() or None
   192	
   193	async def _send_json_safe(ws: WebSocket, payload: Dict) -> bool:
   194	    try:
   195	        await ws.send_json(payload); return True
   196	    except WebSocketDisconnect:
   197	        return False
   198	    except Exception:
   199	        return False
   200	
   201	def _get_bearer_from_headers(ws: WebSocket) -> Optional[str]:
   202	    auth = ws.headers.get("authorization") or ws.headers.get("Authorization")
   203	    if not auth:
   204	        return None
   205	    parts = auth.split()
   206	    if len(parts) == 2 and parts[0].lower() == "bearer":
   207	        return parts[1]
   208	    return None
   209	
   210	# ------------------- Streaming helpers -------------------
   211	
   212	async def _send_typing_stub(ws: WebSocket, thread_id: str):
   213	    await _send_json_safe(ws, {"event": "token", "delta": "…denke nach", "thread_id": thread_id})
   214	
   215	async def _stream_llm_direct(ws: WebSocket, llm, *, user_input: str, thread_id: str):
   216	    def cancelled() -> bool:
   217	        flags = getattr(ws.app.state, "ws_cancel_flags", {})
   218	        return bool(flags.get(thread_id))
   219	
   220	    history = stm_read_history(thread_id, limit=80)
   221	    if cancelled():
   222	        return
   223	
   224	    loop = asyncio.get_event_loop()
   225	    buf: List[str] = []; accum: List[str] = []; last_flush = [loop.time()]
   226	
   227	    async def flush():
   228	        if not buf or cancelled():
   229	            return
   230	        chunk = "".join(buf); buf.clear(); last_flush[0] = loop.time()
   231	        accum.append(chunk)
   232	        await _send_json_safe(ws, {"event": "token", "delta": chunk, "thread_id": thread_id})
   233	
   234	    sys_msg = SystemMessage(content=get_agent_prompt("supervisor"))
   235	    await _send_typing_stub(ws, thread_id)
   236	
   237	    agen = llm.astream([sys_msg] + history + [HumanMessage(content=user_input)])
   238	    try:
   239	        first = await asyncio.wait_for(agen.__anext__(), timeout=FIRST_TOKEN_TIMEOUT_MS / 1000.0)
   240	    except asyncio.TimeoutError:
   241	        try:
   242	            if not cancelled():
   243	                resp = await llm.ainvoke([sys_msg] + history + [HumanMessage(content=user_input)])
   244	                text = getattr(resp, "content", "") or ""
   245	            else:
   246	                text = ""
   247	        except Exception:
   248	            text = ""
   249	        try: await agen.aclose()
   250	        except Exception: pass
   251	        if text and not cancelled():
   252	            await _send_json_safe(ws, {"event": "token", "delta": text, "thread_id": thread_id})
   253	            try: stm_write_message(thread_id=thread_id, role="assistant", content=text)
   254	            except Exception: pass
   255	        if EMIT_FINAL_TEXT and not cancelled():
   256	            await _send_json_safe(ws, {"event": "final", "text": text, "thread_id": thread_id})
   257	        await _send_json_safe(ws, {"event": "done", "thread_id": thread_id})
   258	        return
   259	    except Exception:
   260	        try: await agen.aclose()
   261	        except Exception: pass
   262	        return
   263	
   264	    if cancelled():
   265	        try: await agen.aclose()
   266	        except Exception: pass
   267	        return
   268	
   269	    txt = (_piece_from_llm_chunk(first) or "")
   270	    if txt and not cancelled():
   271	        for seg in _micro_chunks(txt):
   272	            buf.append(seg); await flush()
   273	
   274	    try:
   275	        async for chunk in agen:
   276	            if cancelled(): break
   277	            for piece in _iter_text_from_chunk(chunk):
   278	                if not piece or cancelled(): continue
   279	                for seg in _micro_chunks(piece):
   280	                    buf.append(seg)
   281	                    enough  = sum(len(x) for x in buf) >= COALESCE_MIN_CHARS
   282	                    natural = any("".join(buf).endswith(e) for e in FLUSH_ENDINGS)
   283	                    too_old = (loop.time() - last_flush[0]) * 1000.0 >= COALESCE_MAX_LAT_MS
   284	                    if enough or natural or too_old:
   285	                        await flush()
   286	        await flush()
   287	    finally:
   288	        try: await agen.aclose()
   289	        except Exception: pass
   290	
   291	    if cancelled():
   292	        return
   293	
   294	    final_text = ("".join(accum)).strip()
   295	    if final_text:
   296	        try: stm_write_message(thread_id=thread_id, role="assistant", content=final_text)
   297	        except Exception: pass
   298	    if EMIT_FINAL_TEXT:
   299	        await _send_json_safe(ws, {"event": "final", "text": final_text, "thread_id": thread_id})
   300	    await _send_json_safe(ws, {"event": "done", "thread_id": thread_id})
   301	
   302	async def _stream_supervised(ws: WebSocket, *, app, user_input: str, thread_id: str, params_patch: Optional[Dict]=None):
   303	    def cancelled() -> bool:
   304	        flags = getattr(ws.app.state, "ws_cancel_flags", {})
   305	        return bool(flags.get(thread_id))
   306	
   307	    if cancelled():
   308	        return
   309	
   310	    try:
   311	        _ensure_graph(app)
   312	    except Exception as e:
   313	        if EMIT_FINAL_TEXT and not cancelled():
   314	            await _send_json_safe(ws, {"event": "final", "text": "", "thread_id": thread_id, "error": f"graph_build_failed: {e!r}"})
   315	        await _send_json_safe(ws, {"event": "done", "thread_id": thread_id})
   316	        return
   317	
   318	    g_async = getattr(app.state, "graph_async", None)
   319	    g_sync  = getattr(app.state, "graph_sync", None)
   320	
   321	    history = stm_read_history(thread_id, limit=80)
   322	    sys_msg = SystemMessage(content=get_agent_prompt("supervisor"))
   323	    initial: Dict[str, Any] = {
   324	        "messages": [sys_msg] + history + [HumanMessage(content=user_input)],
   325	        "chat_id": thread_id,
   326	        "input": user_input
   327	    }
   328	    if isinstance(params_patch, dict) and params_patch:
   329	        initial["params"] = params_patch
   330	
   331	    cfg = {"configurable": {"thread_id": thread_id, "checkpoint_ns": getattr(app.state, "checkpoint_ns", None)}}
   332	
   333	    loop = asyncio.get_event_loop()
   334	    buf: List[str] = []; last_flush = [loop.time()]; streamed_any = False
   335	    final_tail: str = ""; accum: List[str] = []
   336	
   337	    async def flush():
   338	        nonlocal streamed_any
   339	        if not buf or cancelled(): return
   340	        chunk = "".join(buf); buf.clear(); last_flush[0] = loop.time()
   341	        streamed_any = True; accum.append(chunk)
   342	        if not await _send_json_safe(ws, {"event": "token", "delta": chunk, "thread_id": thread_id}):
   343	            raise WebSocketDisconnect()
   344	
   345	    def _emit_ui_event_if_any(ev_data: Any) -> bool:
   346	        if not isinstance(ev_data, dict):
   347	            return False
   348	        ui_ev = ev_data.get("ui_event")
   349	        if isinstance(ui_ev, dict):
   350	            payload = {**ui_ev, "event": "ui_action", "thread_id": thread_id}
   351	            return asyncio.create_task(_send_json_safe(ws, payload)) is not None
   352	        for key in ("output", "state", "final_state", "result"):
   353	            sub = ev_data.get(key)
   354	            if isinstance(sub, dict) and isinstance(sub.get("ui_event"), dict):
   355	                u = {**sub["ui_event"], "event": "ui_action", "thread_id": thread_id}
   356	                return asyncio.create_task(_send_json_safe(ws, u)) is not None
   357	        return False
   358	
   359	    def _try_stream_text_from_node(data: Any) -> None:
   360	        texts = _extract_texts(data)
   361	        if not texts: return
   362	        joined = "\n".join([t for t in texts if isinstance(t, str)])
   363	        for seg in _micro_chunks(joined):
   364	            buf.append(seg)
   365	
   366	    await _send_typing_stub(ws, thread_id)
   367	
   368	    async def _run_stream(version: str):
   369	        nonlocal final_tail
   370	        async for ev in g_async.astream_events(initial, config=cfg, version=version):  # type: ignore
   371	            if cancelled(): return
   372	            ev_name = ev.get("event") if isinstance(ev, dict) else None
   373	            data = ev.get("data") if isinstance(ev, dict) else None
   374	
   375	            if ev_name in ("on_chat_model_stream", "on_llm_stream") and _is_relevant_node(ev):
   376	                chunk = (data or {}).get("chunk") if isinstance(data, dict) else None
   377	                if chunk:
   378	                    for piece in _iter_text_from_chunk(chunk):
   379	                        if not piece or cancelled(): continue
   380	                        for seg in _micro_chunks(piece):
   381	                            buf.append(seg)
   382	                            enough  = sum(len(x) for x in buf) >= COALESCE_MIN_CHARS
   383	                            natural = any("".join(buf).endswith(e) for e in FLUSH_ENDINGS)
   384	                            too_old = (loop.time() - last_flush[0]) * 1000.0 >= COALESCE_MAX_LAT_MS
   385	                            if enough or natural or too_old:
   386	                                await flush()
   387	
   388	            if ev_name in ("on_node_end",):
   389	                if isinstance(data, dict):
   390	                    _try_stream_text_from_node(data.get("output") or data)
   391	                await flush()
   392	                _emit_ui_event_if_any(data)
   393	
   394	            if ev_name in ("on_chain_end", "on_graph_end"):
   395	                if isinstance(data, dict):
   396	                    _emit_ui_event_if_any(data)
   397	                    final_tail = _last_ai_text_from_result_like(data) or final_tail
   398	
   399	        await flush()
   400	
   401	    timed_out = False
   402	    if FORCE_SYNC_FALLBACK:
   403	        timed_out = True
   404	    elif g_async is not None and not cancelled():
   405	        try:
   406	            await asyncio.wait_for(_run_stream("v2"), timeout=WS_EVENT_TIMEOUT_SEC)
   407	        except asyncio.TimeoutError:
   408	            timed_out = True
   409	        except Exception:
   410	            try:
   411	                await asyncio.wait_for(_run_stream("v1"), timeout=WS_EVENT_TIMEOUT_SEC)
   412	            except asyncio.TimeoutError:
   413	                timed_out = True
   414	            except Exception:
   415	                pass
   416	
   417	    if cancelled():
   418	        return
   419	
   420	    assistant_text: str = ""
   421	    if final_tail:
   422	        if not streamed_any:
   423	            accum.append(final_tail)
   424	        assistant_text = final_tail
   425	    elif (not streamed_any) or timed_out:
   426	        try:
   427	            result = None
   428	            if g_sync is not None:
   429	                def _run_sync(): return g_sync.invoke(initial, config=cfg)
   430	                result = await asyncio.get_event_loop().run_in_executor(None, _run_sync)
   431	            elif g_async is not None:
   432	                result = await g_async.ainvoke(initial, config=cfg)  # type: ignore
   433	            final_text = _last_ai_text_from_result_like(result or {}) or ""
   434	            assistant_text = final_text
   435	            if final_text:
   436	                accum.append(final_text)
   437	        except Exception:
   438	            assistant_text = ""
   439	        if not assistant_text:
   440	            try:
   441	                llm = getattr(app.state, "llm", make_llm(streaming=False))
   442	                resp = await llm.ainvoke([SystemMessage(content=get_agent_prompt("supervisor"))] + history + [HumanMessage(content=user_input)])
   443	                assistant_text = (getattr(resp, "content", "") or "").strip()
   444	            except Exception:
   445	                assistant_text = ""
   446	    else:
   447	        assistant_text = "".join(accum)
   448	
   449	    if cancelled():
   450	        return
   451	
   452	    final_text = (assistant_text or "".join(accum)).strip()
   453	
   454	    already = "".join(accum).strip()
   455	    if final_text and (not already or already != final_text):
   456	        if not await _send_json_safe(ws, {"event": "token", "delta": final_text, "thread_id": thread_id}):
   457	            return
   458	
   459	    try:
   460	        if final_text:
   461	            stm_write_message(thread_id=thread_id, role="assistant", content=final_text)
   462	    except Exception:
   463	        pass
   464	
   465	    if EMIT_FINAL_TEXT:
   466	        await _send_json_safe(ws, {"event": "final", "text": final_text, "thread_id": thread_id})
   467	    await _send_json_safe(ws, {"event": "done", "thread_id": thread_id})
   468	
   469	# ------------------- WebSocket endpoint -------------------
   470	
   471	@router.websocket("/ai/ws")
   472	async def ws_chat(ws: WebSocket):
   473	    """
   474	    Fehler­tolerante Auth:
   475	    - Versuche guard_websocket(ws).
   476	    - Bei Fehler: keine 403, sondern anonymer Fallback.
   477	    - Optionales Bearer-Token wird aus Header gelesen und in scope gelegt.
   478	    """
   479	    user_payload: Dict[str, Any] = {}
   480	    try:
   481	        user_payload = await guard_websocket(ws)  # kann 403 werfen
   482	    except Exception:
   483	        token = _get_bearer_from_headers(ws)
   484	        user_payload = {"sub": "anonymous"} if not token else {"sub": "bearer"}
   485	    # User-Kontext in scope ablegen (für spätere Nutzung)
   486	    try:
   487	        ws.scope["user"] = user_payload
   488	    except Exception:
   489	        pass
   490	
   491	    await ws.accept(subprotocol=_choose_subprotocol(ws))
   492	
   493	    app = ws.app
   494	    if not getattr(app.state, "llm", None):
   495	        app.state.llm = make_llm(streaming=True)
   496	    try:
   497	        ltm.prewarm_ltm()
   498	    except Exception:
   499	        pass
   500	    if not hasattr(app.state, "ws_cancel_flags"):
   501	        app.state.ws_cancel_flags = {}
   502	
   503	    try:
   504	        while True:
   505	            raw = await ws.receive_text()
   506	
   507	            if isinstance(raw, str) and WS_INPUT_MAX_CHARS > 0 and len(raw) > (WS_INPUT_MAX_CHARS * 2):
   508	                await _send_json_safe(ws, {
   509	                    "event": "error",
   510	                    "code": "input_oversize",
   511	                    "message": f"payload too large (>{WS_INPUT_MAX_CHARS*2} chars)",
   512	                })
   513	                await _send_json_safe(ws, {"event": "done", "thread_id": "ws"})
   514	                continue
   515	
   516	            try:
   517	                data = json.loads(raw)
   518	            except Exception:
   519	                await _send_json_safe(ws, {"event": "error", "message": "invalid_json"}); continue
   520	
   521	            typ = (data.get("type") or "").strip().lower()
   522	            if typ == "ping":
   523	                await _send_json_safe(ws, {"event": "pong", "ts": data.get("ts")}); continue
   524	            if typ == "cancel":
   525	                tid = (data.get("thread_id") or f"api:{(data.get('chat_id') or 'default').strip()}").strip()
   526	                app.state.ws_cancel_flags[tid] = True
   527	                await _send_json_safe(ws, {"event": "done", "thread_id": tid}); continue
   528	
   529	            chat_id    = (data.get("chat_id") or "").strip() or "default"
   530	            thread_id  = f"api:{chat_id}"
   531	            payload    = ws.scope.get("user") or {}
   532	            user_id    = str(payload.get("sub") or payload.get("email") or chat_id)
   533	
   534	            rl = _get_rl_redis(app)
   535	            if rl and WS_RATE_LIMIT_PER_MIN > 0:
   536	                key = f"ws:ratelimit:{user_id}:{chat_id}"
   537	                try:
   538	                    cur = rl.incr(key)
   539	                    if cur == 1:
   540	                        rl.expire(key, 60)
   541	                    if cur > WS_RATE_LIMIT_PER_MIN:
   542	                        await _send_json_safe(ws, {
   543	                            "event": "error",
   544	                            "code": "rate_limited",
   545	                            "message": "Too many requests, slow down.",
   546	                            "retry_after_sec": int(rl.ttl(key) or 60)
   547	                        })
   548	                        await _send_json_safe(ws, {"event": "done", "thread_id": thread_id})
   549	                        continue
   550	                except Exception:
   551	                    pass
   552	
   553	            params_patch = data.get("params") or data.get("params_patch")
   554	            if not isinstance(params_patch, dict):
   555	                params_patch = None
   556	
   557	            user_input = (data.get("input") or data.get("text") or data.get("query") or "").strip()
   558	
   559	            if user_input and WS_INPUT_MAX_CHARS > 0 and len(user_input) > WS_INPUT_MAX_CHARS:
   560	                await _send_json_safe(ws, {
   561	                    "event": "error",
   562	                    "code": "input_too_long",
   563	                    "message": f"input exceeds {WS_INPUT_MAX_CHARS} chars"
   564	                })
   565	                await _send_json_safe(ws, {"event": "done", "thread_id": thread_id})
   566	                continue
   567	
   568	            if not user_input and not params_patch:
   569	                await _send_json_safe(ws, {"event": "error", "message": "missing input", "thread_id": thread_id}); continue
   570	
   571	            try: app.state.ws_cancel_flags.pop(thread_id, None)
   572	            except Exception: pass
   573	
   574	            if user_input:
   575	                try: stm_write_message(thread_id=thread_id, role="user", content=user_input)
   576	                except Exception: pass
   577	
   578	            m = REMEMBER_RX.match(user_input or "")
   579	            if m:
   580	                note = m.group(1).strip(); ok = False
   581	                try:
   582	                    _ = ltm.upsert_memory(user=thread_id, chat_id=thread_id, text=note, kind="note"); ok = True
   583	                except Exception: ok = False
   584	                msg = "✅ Gespeichert." if ok else "⚠️ Konnte nicht speichern."
   585	                await _send_json_safe(ws, {"event": "token", "delta": msg, "thread_id": thread_id})
   586	                if EMIT_FINAL_TEXT:
   587	                    await _send_json_safe(ws, {"event": "final", "text": msg, "thread_id": thread_id})
   588	                await _send_json_safe(ws, {"event": "done", "thread_id": thread_id})
   589	                try: stm_write_message(thread_id=thread_id, role="assistant", content=msg)
   590	                except Exception: pass
   591	                continue
   592	
   593	            await _send_json_safe(ws, {"phase": "starting", "thread_id": thread_id, "route_guess": "supervisor", "reason": "stable_default"})
   594	            await _send_typing_stub(ws, thread_id)
   595	
   596	            await _stream_supervised(ws, app=app, user_input=(user_input or "form patch"), thread_id=thread_id, params_patch=params_patch)
   597	
   598	            try: app.state.ws_cancel_flags.pop(thread_id, None)
   599	            except Exception: pass
   600	
   601	    except WebSocketDisconnect:
   602	        return
   603	    except Exception as e:
   604	        if EMIT_FINAL_TEXT:
   605	            await _send_json_safe(ws, {"event": "final", "text": "", "error": f"ws_internal_error: {e!r}"})
   606	        await _send_json_safe(ws, {"event": "done", "thread_id": "ws"})

################################################################################
# FILE: backend/app/api/v1/endpoints/consult_invoke.py
################################################################################
     1	from __future__ import annotations
     2	
     3	from fastapi import APIRouter, Request, HTTPException, status
     4	from pydantic import BaseModel, Field
     5	from typing import Optional
     6	
     7	from app.services.langgraph.graph.consult.io import invoke_consult
     8	from app.services.langgraph.redis_lifespan import get_redis_checkpointer
     9	
    10	router = APIRouter(prefix="/test", tags=["test"])  # wird unter /api/v1 gemountet
    11	
    12	class ConsultInvokeIn(BaseModel):
    13	    text: str = Field(..., description="Nutzereingabe")
    14	    chat_id: str = Field(..., description="Thread/Chat ID")
    15	
    16	class ConsultInvokeOut(BaseModel):
    17	    text: str
    18	
    19	@router.post("/consult/invoke", response_model=ConsultInvokeOut)
    20	async def consult_invoke_endpoint(payload: ConsultInvokeIn, request: Request):
    21	    text = (payload.text or "").strip()
    22	    if not text:
    23	        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="text empty")
    24	
    25	    chat_id = f"api:{payload.chat_id.strip() or 'test'}"
    26	    try:
    27	        saver = None
    28	        try:
    29	            saver = get_redis_checkpointer(request.app)
    30	        except Exception:
    31	            saver = None
    32	        out = invoke_consult(text, thread_id=chat_id, checkpointer=saver)
    33	        return {"text": out}
    34	    except Exception as e:
    35	        raise HTTPException(status_code=500, detail=f"invoke_failed: {e}")

################################################################################
# FILE: backend/app/api/v1/endpoints/langgraph_sse.py
################################################################################
     1	from __future__ import annotations
     2	
     3	import asyncio
     4	import json
     5	import os
     6	from typing import Any, AsyncGenerator, Dict, Iterable, Optional
     7	
     8	from fastapi import APIRouter, Request
     9	from fastapi.responses import StreamingResponse
    10	from langchain_core.messages import HumanMessage, SystemMessage
    11	
    12	from app.services.langgraph.llm_factory import get_llm as make_llm
    13	from app.services.langgraph.redis_lifespan import get_redis_checkpointer
    14	from app.services.langgraph.prompt_registry import get_agent_prompt
    15	from app.services.langgraph.graph.consult.memory_utils import (
    16	    read_history as stm_read_history,
    17	    write_message as stm_write_message,
    18	)
    19	
    20	router = APIRouter()
    21	
    22	# Tunables
    23	SSE_MIN_CHARS      = int(os.getenv("SSE_COALESCE_MIN_CHARS", "24"))
    24	SSE_MAX_LAT_MS     = float(os.getenv("SSE_COALESCE_MAX_LAT_MS", "60"))
    25	SSE_EVENT_TIMEOUT  = int(os.getenv("SSE_EVENT_TIMEOUT_SEC", "60"))
    26	SSE_INPUT_MAX      = int(os.getenv("SSE_INPUT_MAX_CHARS", "4000"))
    27	GRAPH_BUILDER      = os.getenv("GRAPH_BUILDER", "supervisor").lower()
    28	
    29	# Helpers
    30	def _iter_text_from_chunk(chunk: Any) -> Iterable[str]:
    31	    if isinstance(chunk, dict):
    32	        for k in ("content", "delta", "text", "token"):
    33	            v = chunk.get(k)
    34	            if isinstance(v, str) and v:
    35	                yield v
    36	    c = getattr(chunk, "content", None)
    37	    if isinstance(c, str) and c:
    38	        yield c
    39	    if isinstance(c, list):
    40	        for part in c:
    41	            if isinstance(part, str) and part:
    42	                yield part
    43	
    44	def _last_ai_text_from_result_like(obj: Dict[str, Any]) -> str:
    45	    def _collect(x: Any, out: list[str]):
    46	        if isinstance(x, str) and x.strip():
    47	            out.append(x.strip()); return
    48	        if isinstance(x, dict):
    49	            for k in ("response", "final_text", "text", "answer"):
    50	                v = x.get(k)
    51	                if isinstance(v, str) and v.strip():
    52	                    out.append(v.strip())
    53	            msgs = x.get("messages")
    54	            if isinstance(msgs, list):
    55	                for m in msgs:
    56	                    if isinstance(m, dict):
    57	                        c = m.get("content")
    58	                        if isinstance(c, str) and c.strip():
    59	                            out.append(c.strip())
    60	            for k in ("output", "state", "final_state", "result"):
    61	                _collect(x.get(k), out)
    62	        elif isinstance(x, list):
    63	            for it in x:
    64	                _collect(it, out)
    65	    tmp: list[str] = []
    66	    _collect(obj, tmp)
    67	    return tmp[-1].strip() if tmp else ""
    68	
    69	def _build_graph(app):
    70	    # Gleiche Logik wie WS: Supervisor bevorzugen, sonst Consult
    71	    if GRAPH_BUILDER == "supervisor":
    72	        from app.services.langgraph.supervisor_graph import build_supervisor_graph as build_graph
    73	    else:
    74	        from app.services.langgraph.graph.consult.build import build_consult_graph as build_graph
    75	    saver = None
    76	    try:
    77	        saver = get_redis_checkpointer(app)
    78	    except Exception:
    79	        saver = None
    80	    g = build_graph()
    81	    try:
    82	        return g.compile(checkpointer=saver) if saver else g.compile()
    83	    except Exception:
    84	        return g.compile()
    85	
    86	def _sse(event: str, data: Any) -> bytes:
    87	    if not isinstance(data, str):
    88	        data = json.dumps(data, ensure_ascii=False)
    89	    return f"event: {event}\ndata: {data}\n\n".encode("utf-8")
    90	
    91	@router.post("/chat/stream")
    92	async def chat_stream(request: Request) -> StreamingResponse:
    93	    """
    94	    Input (JSON):
    95	      { "chat_id": "default", "input_text": "..." }
    96	    Output: text/event-stream mit Events: token, final, done
    97	    """
    98	    body = await request.json()
    99	    chat_id   = (body.get("chat_id") or "default").strip() or "default"
   100	    user_text = (body.get("input_text") or "").strip()
   101	    if not user_text:
   102	        async def bad() -> AsyncGenerator[bytes, None]:
   103	            yield _sse("error", "input_text empty"); yield _sse("done", "")
   104	        return StreamingResponse(bad(), media_type="text/event-stream")
   105	    if SSE_INPUT_MAX > 0 and len(user_text) > SSE_INPUT_MAX:
   106	        async def too_long() -> AsyncGenerator[bytes, None]:
   107	            yield _sse("error", f"input exceeds {SSE_INPUT_MAX} chars"); yield _sse("done", "")
   108	        return StreamingResponse(too_long(), media_type="text/event-stream")
   109	
   110	    async def gen() -> AsyncGenerator[bytes, None]:
   111	        app = request.app
   112	        # LLM fallback für Notfälle bereitstellen
   113	        if not getattr(app.state, "llm", None):
   114	            app.state.llm = make_llm(streaming=True)
   115	
   116	        # History
   117	        thread_id = f"api:{chat_id}"
   118	        history = stm_read_history(thread_id, limit=80)
   119	        sys_msg = SystemMessage(content=get_agent_prompt("supervisor"))
   120	
   121	        # Graph vorbereiten (wie WS)
   122	        if not getattr(app.state, "graph_async", None):
   123	            app.state.graph_async = _build_graph(app)
   124	            app.state.graph_sync  = app.state.graph_async
   125	
   126	        g_async = app.state.graph_async
   127	        initial = {
   128	            "messages": [sys_msg] + history + [HumanMessage(content=user_text)],
   129	            "chat_id": thread_id,
   130	            "input": user_text,
   131	        }
   132	        cfg = {"configurable": {"thread_id": thread_id, "checkpoint_ns": getattr(app.state, "checkpoint_ns", None)}}
   133	
   134	        loop = asyncio.get_event_loop()
   135	        buf: list[str] = []
   136	        last_flush = [loop.time()]
   137	        accum: list[str] = []
   138	        final_tail = ""
   139	
   140	        async def flush():
   141	            if not buf:
   142	                return
   143	            chunk = "".join(buf); buf.clear(); last_flush[0] = loop.time()
   144	            accum.append(chunk)
   145	            yield_bytes = _sse("token", chunk)
   146	            # yield innerhalb Hilfsfunktion geht nicht; zurückgeben
   147	            return yield_bytes
   148	
   149	        # Sofortiges Lebenszeichen
   150	        yield _sse("token", "…")
   151	        # Stream
   152	        async def run_stream(version: str):
   153	            nonlocal final_tail
   154	            async for ev in g_async.astream_events(initial, config=cfg, version=version):  # type: ignore
   155	                ev_name = ev.get("event"); data = ev.get("data")
   156	                if ev_name in ("on_chat_model_stream", "on_llm_stream"):
   157	                    chunk = (data or {}).get("chunk") if isinstance(data, dict) else None
   158	                    if chunk:
   159	                        for piece in _iter_text_from_chunk(chunk):
   160	                            if not piece:
   161	                                continue
   162	                            buf.append(piece)
   163	                            enough  = sum(len(x) for x in buf) >= SSE_MIN_CHARS
   164	                            too_old = (loop.time() - last_flush[0]) * 1000.0 >= SSE_MAX_LAT_MS
   165	                            if enough or too_old:
   166	                                y = await flush()
   167	                                if y: yield y
   168	                if ev_name in ("on_node_end",):
   169	                    y = await flush()
   170	                    if y: yield y
   171	                if ev_name in ("on_chain_end", "on_graph_end"):
   172	                    if isinstance(data, dict):
   173	                        final_tail = _last_ai_text_from_result_like(data) or final_tail
   174	            y = await flush()
   175	            if y: yield y
   176	
   177	        timed_out = False
   178	        try:
   179	            async for y in asyncio.wait_for(run_stream("v2").__aiter__(), timeout=SSE_EVENT_TIMEOUT):  # type: ignore
   180	                if y:
   181	                    yield y
   182	        except asyncio.TimeoutError:
   183	            timed_out = True
   184	        except Exception:
   185	            # v1 Fallback
   186	            try:
   187	                async for y in asyncio.wait_for(run_stream("v1").__aiter__(), timeout=SSE_EVENT_TIMEOUT):  # type: ignore
   188	                    if y:
   189	                        yield y
   190	            except Exception:
   191	                pass
   192	
   193	        final_text = final_tail or "".join(accum).strip()
   194	        if not final_text:
   195	            # Notfall: einmalig synchron antworten
   196	            try:
   197	                resp = await app.state.llm.ainvoke([sys_msg] + history + [HumanMessage(content=user_text)])
   198	                final_text = (getattr(resp, "content", "") or "").strip()
   199	            except Exception:
   200	                final_text = ""
   201	        if final_text:
   202	            try: stm_write_message(thread_id=thread_id, role="assistant", content=final_text)
   203	            except Exception: pass
   204	            yield _sse("token", final_text)
   205	        yield _sse("final", final_text)
   206	        yield _sse("done", "")
   207	
   208	    return StreamingResponse(gen(), media_type="text/event-stream")

################################################################################
# FILE: backend/app/api/v1/endpoints/system.py
################################################################################
     1	from __future__ import annotations
     2	
     3	from typing import Optional, Dict, Any
     4	from pydantic import BaseModel, Field
     5	from fastapi import APIRouter, Request
     6	import json
     7	
     8	from app.services.langgraph.graph.consult.io import invoke_consult
     9	from app.services.langgraph.redis_lifespan import get_redis_checkpointer
    10	
    11	router = APIRouter()  # Prefix und Tags werden im übergeordneten Router gesetzt
    12	
    13	class _ConsultInvokeIn(BaseModel):
    14	    text: str = Field(..., min_length=1)
    15	    chat_id: Optional[str] = None
    16	
    17	@router.post("/test/consult/invoke", tags=["test"])
    18	async def test_consult_invoke(body: _ConsultInvokeIn, request: Request) -> Dict[str, Any]:
    19	    thread_id = f"api:{body.chat_id or 'test'}"
    20	    try:
    21	        saver = get_redis_checkpointer(request.app)
    22	    except Exception:
    23	        saver = None
    24	
    25	    out = invoke_consult(body.text, thread_id=thread_id, checkpointer=saver)
    26	
    27	    try:
    28	        parsed = json.loads(out)
    29	        payload = {"json": parsed} if isinstance(parsed, (dict, list)) else {"text": out}
    30	    except Exception:
    31	        payload = {"text": out}
    32	
    33	    return {"final": payload}

################################################################################
# FILE: backend/app/main.py
################################################################################
     1	from __future__ import annotations
     2	
     3	import logging
     4	import os
     5	from fastapi import FastAPI
     6	from fastapi.middleware.cors import CORSMiddleware
     7	from fastapi.responses import PlainTextResponse
     8	
     9	from app.api.v1.api import api_router
    10	from app.services.langgraph.graph.consult.build import build_consult_graph
    11	
    12	# Bevorzugte LLM-Factory (nutzt das zentrale LLM für WS/SSE)
    13	try:
    14	    from app.services.langgraph.llm_factory import get_llm as _make_llm  # hat meist streaming=True
    15	except Exception:  # Fallback nur, falls Modul nicht vorhanden
    16	    _make_llm = None  # type: ignore
    17	
    18	# Zweite Option: LLM-Factory aus der Consult-Config
    19	try:
    20	    from app.services.langgraph.graph.consult.config import create_llm as _create_llm_cfg
    21	except Exception:
    22	    _create_llm_cfg = None  # type: ignore
    23	
    24	# RAG-Orchestrator für Warmup
    25	try:
    26	    from app.services.rag import rag_orchestrator as ro  # enthält prewarm(), hybrid_retrieve, …
    27	except Exception:
    28	    ro = None  # type: ignore
    29	
    30	# ---- Access-Log-Filter: /health stummschalten ----
    31	class _HealthSilencer(logging.Filter):
    32	    def filter(self, record: logging.LogRecord) -> bool:
    33	        try:
    34	            msg = record.getMessage()
    35	        except Exception:
    36	            return True
    37	        return "/health" not in msg
    38	
    39	logging.getLogger("uvicorn.access").addFilter(_HealthSilencer())
    40	# ---------------------------------------------------
    41	
    42	log = logging.getLogger("uvicorn.error")
    43	
    44	
    45	def _init_llm():
    46	    """
    47	    Initialisiert ein Chat LLM für Streaming-Endpoints.
    48	    Robust gegen unterschiedliche Factory-Signaturen/Module.
    49	    """
    50	    # 1) Primär: zentrale LLM-Factory
    51	    if _make_llm:
    52	        try:
    53	            return _make_llm(streaming=True)  # neue Signatur
    54	        except TypeError:
    55	            # ältere Signatur ohne streaming-Param
    56	            return _make_llm()
    57	
    58	    # 2) Fallback: Consult-Config Factory
    59	    if _create_llm_cfg:
    60	        try:
    61	            return _create_llm_cfg(streaming=True)
    62	        except TypeError:
    63	            return _create_llm_cfg()
    64	
    65	    return None
    66	
    67	
    68	def create_app() -> FastAPI:
    69	    app = FastAPI(title="SealAI Backend", version=os.getenv("APP_VERSION", "dev"))
    70	
    71	    app.add_middleware(
    72	        CORSMiddleware,
    73	        allow_origins=os.getenv("CORS_ALLOW_ORIGINS", "*").split(","),
    74	        allow_credentials=True,
    75	        allow_methods=["*"],
    76	        allow_headers=["*"],
    77	    )
    78	
    79	    # Health für LB/Compose
    80	    @app.get("/health")
    81	    async def _health() -> PlainTextResponse:
    82	        return PlainTextResponse("ok")
    83	
    84	    # API v1
    85	    app.include_router(api_router, prefix="/api/v1")
    86	
    87	    @app.on_event("startup")
    88	    async def _startup():
    89	        # 1) LLM für Streaming-Endpoints initialisieren
    90	        try:
    91	            app.state.llm = _init_llm()
    92	            if app.state.llm is None:
    93	                raise RuntimeError("No LLM factory available")
    94	            log.info("LLM initialized for streaming endpoints.")
    95	        except Exception as e:
    96	            app.state.llm = None
    97	            log.warning("LLM init failed: %s", e)
    98	
    99	        # 2) RAG Warmup (Embedding, Reranker, Redis, Qdrant) – verhindert langen ersten Request
   100	        try:
   101	            if ro and hasattr(ro, "prewarm"):
   102	                ro.prewarm()
   103	                log.info("RAG prewarm completed.")
   104	            else:
   105	                log.info("RAG prewarm skipped (no ro.prewarm available).")
   106	        except Exception as e:
   107	            log.warning("RAG prewarm failed: %s", e)
   108	
   109	        # 3) Sync-Fallback-Graph (ohne Checkpointer) vorbereiten
   110	        try:
   111	            app.state.graph_sync = build_consult_graph().compile()
   112	            log.info("Consult graph compiled for sync fallback.")
   113	        except Exception as e:
   114	            app.state.graph_sync = None
   115	            log.warning("Graph compile failed: %s", e)
   116	
   117	        log.info("Startup: no prebuilt async graph (lazy build in chat_ws).")
   118	
   119	    return app
   120	
   121	
   122	app = create_app()

################################################################################
# FILE: backend/app/services/langgraph/agents/__init__.py
################################################################################

################################################################################
# FILE: backend/app/services/langgraph/agents/material_agent.py
################################################################################
     1	import os
     2	from jinja2 import Environment, FileSystemLoader
     3	from langchain_openai import ChatOpenAI
     4	from langchain_core.messages import SystemMessage
     5	
     6	PROMPT_FILE = os.path.join(os.path.dirname(__file__), "..", "prompts", "material_agent.jinja2")
     7	
     8	def get_prompt(context=None):
     9	    env = Environment(loader=FileSystemLoader(os.path.dirname(PROMPT_FILE)))
    10	    template = env.get_template("material_agent.jinja2")
    11	    return template.render(context=context)
    12	
    13	class MaterialAgent:
    14	    name = "material_agent"
    15	
    16	    def __init__(self, context=None):
    17	        self.system_prompt = get_prompt(context)
    18	        self.llm = ChatOpenAI(
    19	            model=os.getenv("OPENAI_MODEL", "gpt-4.1-mini"),
    20	            api_key=os.getenv("OPENAI_API_KEY"),
    21	            base_url=os.getenv("OPENAI_BASE_URL") or None,
    22	            temperature=float(os.getenv("OPENAI_TEMPERATURE", "0")),
    23	            streaming=True,
    24	        )
    25	
    26	    def invoke(self, state):
    27	        messages = [SystemMessage(content=self.system_prompt)] + state["messages"]
    28	        return {"messages": self.llm.invoke(messages)}
    29	
    30	def get_material_agent(context=None):
    31	    return MaterialAgent(context)

################################################################################
# FILE: backend/app/services/langgraph/agents/profile_agent.py
################################################################################
     1	# backend/app/services/langgraph/agents/profile_agent.py
     2	from __future__ import annotations
     3	
     4	from typing import Any, Dict, List
     5	
     6	from langchain_core.messages import SystemMessage, AIMessage, HumanMessage
     7	from app.services.langgraph.llm_factory import get_llm
     8	from app.services.langgraph.prompting import render_template
     9	
    10	def _last_user_text(messages: List[Any]) -> str:
    11	    for m in reversed(messages or []):
    12	        c = getattr(m, "content", None)
    13	        if isinstance(c, str) and c.strip():
    14	            return c.strip()
    15	    return ""
    16	
    17	class ProfileAgent:
    18	    name = "profile_agent"
    19	
    20	    def __init__(self) -> None:
    21	        self.llm = get_llm()
    22	
    23	    def invoke(self, state: Dict[str, Any]) -> Dict[str, Any]:
    24	        msgs = state.get("messages") or []
    25	        query = _last_user_text(msgs)
    26	        system_prompt = render_template("profile_agent.jinja2", query=query)
    27	        response = self.llm.invoke([SystemMessage(content=system_prompt)] + msgs)
    28	        ai = response if isinstance(response, AIMessage) else AIMessage(content=getattr(response, "content", "") or "")
    29	        return {"messages": [ai]}
    30	
    31	def get_profile_agent() -> ProfileAgent:
    32	    return ProfileAgent()

################################################################################
# FILE: backend/app/services/langgraph/domains/base.py
################################################################################
     1	# -*- coding: utf-8 -*-
     2	from __future__ import annotations
     3	import os
     4	import yaml
     5	from dataclasses import dataclass
     6	from typing import Dict, Any, Tuple, List, Optional, Callable
     7	
     8	
     9	@dataclass
    10	class DomainSpec:
    11	    id: str
    12	    name: str
    13	    base_dir: str            # Ordner der Domain (für Prompts/Schema)
    14	    schema_file: str         # relativer Pfad
    15	    calculator: Callable[[dict], Dict[str, Any]]  # compute(params) -> {'calculated': ..., 'flags': ...}
    16	    ask_order: List[str]     # Reihenfolge der Nachfragen (falls fehlt)
    17	
    18	    def template_dir(self) -> str:
    19	        return os.path.join(self.base_dir, "prompts")
    20	
    21	    def schema_path(self) -> str:
    22	        return os.path.join(self.base_dir, self.schema_file)
    23	
    24	_REGISTRY: Dict[str, DomainSpec] = {}
    25	
    26	def register_domain(spec: DomainSpec) -> None:
    27	    _REGISTRY[spec.id] = spec
    28	    # Domain-Prompts dem Jinja-Loader bekannt machen
    29	
    30	def get_domain(domain_id: str) -> Optional[DomainSpec]:
    31	    return _REGISTRY.get(domain_id)
    32	
    33	def list_domains() -> List[str]:
    34	    return list(_REGISTRY.keys())
    35	
    36	# -------- YAML Schema Laden & Validieren (leichtgewichtig) ----------
    37	def load_schema(spec: DomainSpec) -> Dict[str, Any]:
    38	    with open(spec.schema_path(), "r", encoding="utf-8") as f:
    39	        return yaml.safe_load(f) or {}
    40	
    41	def validate_params(spec: DomainSpec, params: Dict[str, Any]) -> Tuple[List[str], List[str]]:
    42	    """
    43	    Gibt (errors, warnings) zurück.
    44	    YAML-Schema Felder:
    45	      fields:
    46	        <name>:
    47	          required: bool
    48	          type: str ('int'|'float'|'str'|'enum')
    49	          min: float
    50	          max: float
    51	          enum: [..]
    52	          ask_if: optional (Dependency-Hinweis, nur Info)
    53	    """
    54	    schema = load_schema(spec)
    55	    fields = schema.get("fields", {})
    56	    errors: List[str] = []
    57	    warnings: List[str] = []
    58	
    59	    def _typename(x):
    60	        if isinstance(x, bool):   # bool ist auch int in Python
    61	            return "bool"
    62	        if isinstance(x, int):
    63	            return "int"
    64	        if isinstance(x, float):
    65	            return "float"
    66	        if isinstance(x, str):
    67	            return "str"
    68	        return type(x).__name__
    69	
    70	    for key, rule in fields.items():
    71	        req = bool(rule.get("required", False))
    72	        if req and (key not in params or params.get(key) in (None, "")):
    73	            errors.append(f"Pflichtfeld fehlt: {key}")
    74	            continue
    75	        if key not in params or params.get(key) in (None, ""):
    76	            continue
    77	
    78	        val = params.get(key)
    79	        typ = rule.get("type")
    80	        if typ == "enum":
    81	            allowed = rule.get("enum", [])
    82	            if val not in allowed:
    83	                errors.append(f"{key}: ungültiger Wert '{val}', erlaubt: {allowed}")
    84	        elif typ == "int":
    85	            if not isinstance(val, int):
    86	                # ints können als float ankommen (LLM) – tolerant casten
    87	                try:
    88	                    params[key] = int(float(val))
    89	                except Exception:
    90	                    errors.append(f"{key}: erwartet int, erhalten {_typename(val)}")
    91	            else:
    92	                # ok
    93	                pass
    94	        elif typ == "float":
    95	            if isinstance(val, (int, float)):
    96	                params[key] = float(val)
    97	            else:
    98	                try:
    99	                    params[key] = float(str(val).replace(",", "."))
   100	                except Exception:
   101	                    errors.append(f"{key}: erwartet float, erhalten {_typename(val)}")
   102	        elif typ == "str":
   103	            if not isinstance(val, str):
   104	                params[key] = str(val)
   105	
   106	        # Ranges
   107	        if isinstance(params.get(key), (int, float)):
   108	            v = float(params[key])
   109	            if "min" in rule and v < float(rule["min"]):
   110	                errors.append(f"{key}: {v} < min {rule['min']}")
   111	            if "max" in rule and v > float(rule["max"]):
   112	                warnings.append(f"{key}: {v} > empfohlene Obergrenze {rule['max']}")
   113	
   114	    return errors, warnings

################################################################################
# FILE: backend/app/services/langgraph/domains/__init__.py
################################################################################
     1	# -*- coding: utf-8 -*-
     2	# Stellt sicher, dass Domains beim Import registriert werden.
     3	from .rwdr import register as register_rwdr
     4	from .hydraulics_rod import register as register_hydraulics_rod
     5	
     6	def register_all_domains() -> None:
     7	    register_rwdr()
     8	    register_hydraulics_rod()

################################################################################
# FILE: backend/app/services/langgraph/examples/__init__.py
################################################################################

################################################################################
# FILE: backend/app/services/langgraph/graph/consult/build.py
################################################################################
     1	# backend/app/services/langgraph/graph/consult/build.py
     2	from __future__ import annotations
     3	
     4	import logging
     5	from typing import Any, Dict, List
     6	from langgraph.graph import StateGraph, END
     7	
     8	from .state import ConsultState
     9	from .utils import normalize_messages
    10	from .domain_router import detect_domain
    11	from .domain_runtime import compute_domain
    12	
    13	from .nodes.intake import intake_node
    14	from .nodes.ask_missing import ask_missing_node
    15	from .nodes.validate import validate_node
    16	from .nodes.recommend import recommend_node
    17	from .nodes.explain import explain_node
    18	from .nodes.calc_agent import calc_agent_node
    19	from .nodes.rag import run_rag_node
    20	from .nodes.validate_answer import validate_answer
    21	
    22	# NEU
    23	from .nodes.smalltalk import smalltalk_node
    24	from .nodes.lite_router import lite_router_node
    25	
    26	from .heuristic_extract import pre_extract_params
    27	from .extract import extract_params_with_llm
    28	from .config import create_llm
    29	
    30	log = logging.getLogger("uvicorn.error")
    31	
    32	
    33	def _join_user_text(msgs: List) -> str:
    34	    out: List[str] = []
    35	    for m in msgs:
    36	        role = (getattr(m, "type", "") or getattr(m, "role", "")).lower()
    37	        content = getattr(m, "content", "")
    38	        if isinstance(m, dict):
    39	            role = (m.get("type") or m.get("role") or "").lower()
    40	            content = m.get("content")
    41	        if role in ("human", "user") and isinstance(content, str) and content.strip():
    42	            out.append(content.strip())
    43	    return "\n".join(out)
    44	
    45	
    46	def _merge_seed_first(seed: Dict[str, Any], llm_out: Dict[str, Any]) -> Dict[str, Any]:
    47	    out = dict(llm_out or {})
    48	    for k, v in (seed or {}).items():
    49	        if v not in (None, "", []):
    50	            out[k] = v
    51	    return out
    52	
    53	
    54	def _compact_param_summary(domain: str, params: Dict[str, Any]) -> str:
    55	    p = params or {}
    56	    parts: List[str] = []
    57	
    58	    if domain == "rwdr":
    59	        parts.append("RWDR")
    60	        if p.get("abmessung"):
    61	            parts.append(str(p["abmessung"]))
    62	        elif p.get("wellen_mm") and p.get("gehause_mm") and p.get("breite_mm"):
    63	            parts.append(f'{p["wellen_mm"]}x{p["gehause_mm"]}x{p["breite_mm"]}')
    64	    elif domain == "hydraulics_rod":
    65	        parts.append("Hydraulik Stangendichtung")
    66	
    67	    if p.get("medium"):
    68	        parts.append(str(p["medium"]))
    69	    if p.get("temp_max_c") or p.get("tmax_c"):
    70	        parts.append(f'Tmax {int(p.get("temp_max_c") or p.get("tmax_c"))} °C')
    71	    if p.get("druck_bar"):
    72	        parts.append(f'Druck {p["druck_bar"]} bar')
    73	    if p.get("drehzahl_u_min"):
    74	        parts.append(f'{int(p["drehzahl_u_min"])} U/min')
    75	    if p.get("relativgeschwindigkeit_ms"):
    76	        parts.append(f'v≈{float(p["relativgeschwindigkeit_ms"]):.2f} m/s')
    77	
    78	    bl = p.get("material_blacklist") or p.get("vermeide_materialien")
    79	    wl = p.get("material_whitelist") or p.get("bevorzugte_materialien")
    80	    if bl:
    81	        parts.append(f'Vermeide: {bl}')
    82	    if wl:
    83	        parts.append(f'Bevorzugt: {wl}')
    84	
    85	    return ", ".join(parts)
    86	
    87	
    88	def _extract_node(state: Dict[str, Any]) -> Dict[str, Any]:
    89	    msgs = normalize_messages(state.get("messages", []))
    90	    params = dict(state.get("params") or {})
    91	    user_text = _join_user_text(msgs)
    92	
    93	    heur = pre_extract_params(user_text)
    94	    seed = {**params, **{k: v for k, v in heur.items() if v not in (None, "", [])}}
    95	
    96	    llm_params = extract_params_with_llm(user_text)
    97	    final_params = _merge_seed_first(seed, llm_params)
    98	    return {**state, "params": final_params, "phase": "extract"}
    99	
   100	
   101	def _domain_router_node(state: Dict[str, Any]) -> Dict[str, Any]:
   102	    msgs = normalize_messages(state.get("messages", []))
   103	    params = dict(state.get("params") or {})
   104	    try:
   105	        domain = detect_domain(None, msgs, params) or "rwdr"
   106	        domain = domain.strip().lower()
   107	    except Exception:
   108	        domain = "rwdr"
   109	    return {**state, "domain": domain, "phase": "domain_router"}
   110	
   111	
   112	def _compute_node(state: Dict[str, Any]) -> Dict[str, Any]:
   113	    domain = (state.get("domain") or "rwdr").strip().lower()
   114	    params = dict(state.get("params") or {})
   115	    derived = compute_domain(domain, params) or {}
   116	
   117	    alias_map = {
   118	        "tmax_c": params.get("temp_max_c"),
   119	        "temp_c": params.get("temp_max_c"),
   120	        "druck": params.get("druck_bar"),
   121	        "pressure_bar": params.get("druck_bar"),
   122	        "n_u_min": params.get("drehzahl_u_min"),
   123	        "rpm": params.get("drehzahl_u_min"),
   124	        "v_ms": params.get("relativgeschwindigkeit_ms"),
   125	    }
   126	    for k, v in alias_map.items():
   127	        if k not in params and v not in (None, "", []):
   128	            params[k] = v
   129	
   130	    return {**state, "params": params, "derived": derived, "phase": "compute"}
   131	
   132	
   133	def _prepare_query_node(state: Dict[str, Any]) -> Dict[str, Any]:
   134	    if (state.get("query") or "").strip():
   135	        return {**state, "phase": "prepare_query"}
   136	
   137	    msgs = normalize_messages(state.get("messages", []))
   138	    params = dict(state.get("params") or {})
   139	    domain = (state.get("domain") or "rwdr").strip().lower()
   140	
   141	    user_text = _join_user_text(msgs)
   142	    param_str = _compact_param_summary(domain, params)
   143	    prefix = "RWDR" if domain == "rwdr" else "Hydraulik"
   144	    query = ", ".join([s for s in [prefix, user_text, param_str] if s])
   145	
   146	    new_state = dict(state)
   147	    new_state["query"] = query
   148	    return {**new_state, "phase": "prepare_query"}
   149	
   150	
   151	def _respond_node(state: Dict[str, Any]) -> Dict[str, Any]:
   152	    return {**state, "phase": "respond"}
   153	
   154	
   155	# ---- Conditional helpers ----
   156	def _route_key(state: Dict[str, Any]) -> str:
   157	    return (state.get("route") or "default").strip().lower() or "default"
   158	
   159	
   160	def _ask_or_ok(state: Dict[str, Any]) -> str:
   161	    p = state.get("params") or {}
   162	
   163	    def has(v: Any) -> bool:
   164	        if v is None: return False
   165	        if isinstance(v, (list, dict)) and not v: return False
   166	        if isinstance(v, str) and not v.strip(): return False
   167	        return True
   168	
   169	    base_ok = has(p.get("temp_max_c")) and has(p.get("druck_bar"))
   170	    rel_ok  = has(p.get("relativgeschwindigkeit_ms")) or (has(p.get("wellen_mm")) and has(p.get("drehzahl_u_min")))
   171	
   172	    msgs = normalize_messages(state.get("messages", []))
   173	    user = (_join_user_text(msgs) or "").lower()
   174	    info_triggers = (
   175	        "was weißt du", "was weisst du", "what do you know", "info",
   176	        "rag", "rag:", "kyrolon", "ptfe", "datenblatt", "sds"
   177	    )
   178	    if not (base_ok and rel_ok) and any(t in user for t in info_triggers):
   179	        return "info"
   180	
   181	    return "ok" if (base_ok and rel_ok) else "ask"
   182	
   183	
   184	def _after_rag(state: Dict[str, Any]) -> str:
   185	    p = state.get("params") or {}
   186	
   187	    def has(v: Any) -> bool:
   188	        if v is None: return False
   189	        if isinstance(v, (list, dict)) and not v: return False
   190	        if isinstance(v, str) and not v.strip(): return False
   191	        return True
   192	
   193	    base_ok = has(p.get("temp_max_c")) and has(p.get("druck_bar"))
   194	    rel_ok  = has(p.get("relativgeschwindigkeit_ms")) or (has(p.get("wellen_mm")) and has(p.get("drehzahl_u_min")))
   195	    docs    = state.get("retrieved_docs") or state.get("docs") or []
   196	    ctx_ok  = bool(docs) or bool(state.get("context"))
   197	
   198	    return "recommend" if (base_ok and rel_ok and ctx_ok) else "explain"
   199	
   200	
   201	def build_graph() -> StateGraph:
   202	    log.info("[ConsultGraph] Initialisierung…")
   203	    g = StateGraph(ConsultState)
   204	
   205	    # --- Nodes ---
   206	    g.add_node("lite_router", lite_router_node)  # NEU
   207	    g.add_node("smalltalk", smalltalk_node)      # NEU
   208	
   209	    g.add_node("intake", intake_node)
   210	    g.add_node("extract", _extract_node)
   211	    g.add_node("domain_router", _domain_router_node)
   212	    g.add_node("compute", _compute_node)
   213	    g.add_node("calc_agent", calc_agent_node)
   214	    g.add_node("ask_missing", ask_missing_node)
   215	    g.add_node("validate", validate_node)
   216	    g.add_node("prepare_query", _prepare_query_node)
   217	    g.add_node("rag", run_rag_node)
   218	    g.add_node("recommend", recommend_node)
   219	    g.add_node("validate_answer", validate_answer)
   220	    g.add_node("explain", explain_node)
   221	    g.add_node("respond", _respond_node)
   222	
   223	    # --- Entry & Routing ---
   224	    g.set_entry_point("lite_router")
   225	    g.add_conditional_edges("lite_router", _route_key, {
   226	        "smalltalk": "smalltalk",
   227	        "info": "prepare_query",
   228	        "default": "intake",
   229	    })
   230	
   231	    # --- Main flow ---
   232	    g.add_edge("intake", "extract")
   233	    g.add_edge("extract", "domain_router")
   234	    g.add_edge("domain_router", "compute")
   235	    g.add_edge("compute", "calc_agent")
   236	    g.add_edge("calc_agent", "ask_missing")
   237	
   238	    g.add_conditional_edges("ask_missing", _ask_or_ok, {
   239	        "ask":  "respond",
   240	        "ok":   "validate",
   241	        "info": "prepare_query",
   242	    })
   243	
   244	    g.add_edge("validate", "prepare_query")
   245	    g.add_edge("prepare_query", "rag")
   246	    g.add_conditional_edges("rag", _after_rag, {
   247	        "recommend": "recommend",
   248	        "explain":   "explain",
   249	    })
   250	    g.add_edge("recommend", "validate_answer")
   251	    g.add_edge("validate_answer", "explain")
   252	    g.add_edge("explain", "respond")
   253	    g.add_edge("respond", END)
   254	
   255	    log.info("[ConsultGraph] erfolgreich erstellt.")
   256	    return g
   257	
   258	
   259	def build_consult_graph() -> StateGraph:
   260	    return build_graph()

################################################################################
# FILE: backend/app/services/langgraph/graph/consult/config.py
################################################################################
     1	# backend/app/services/langgraph/graph/consult/config.py
     2	from __future__ import annotations
     3	
     4	import os
     5	from typing import List, Optional
     6	from langchain_openai import ChatOpenAI
     7	
     8	
     9	# --- Domänen-Schalter ---------------------------------------------------------
    10	# Kommagetrennte Liste via ENV z. B.: "rwdr,hydraulics_rod"
    11	def _env_domains() -> List[str]:
    12	    raw = (os.getenv("CONSULT_ENABLED_DOMAINS") or "").strip()
    13	    if not raw:
    14	        return ["rwdr", "hydraulics_rod"]
    15	    return [x.strip().lower() for x in raw.split(",") if x.strip()]
    16	
    17	
    18	ENABLED_DOMAINS: List[str] = _env_domains()
    19	
    20	
    21	# --- LLM-Fabrik ---------------------------------------------------------------
    22	def _model_name() -> str:
    23	    # Fällt auf GPT-5 mini zurück, wie gewünscht
    24	    return (os.getenv("LLM_MODEL_DEFAULT") or "gpt-5-mini").strip()
    25	
    26	
    27	def _base_url() -> Optional[str]:
    28	    # kompatibel zu llm_factory: neues Feld heißt base_url (nicht api_base)
    29	    base = (os.getenv("OPENAI_API_BASE") or "").strip()
    30	    return base or None
    31	
    32	
    33	def create_llm(*, streaming: bool = True) -> ChatOpenAI:
    34	    """
    35	    Einheitliche LLM-Erzeugung für den Consult-Graph.
    36	    Nutzt GPT-5-mini (Default) und übernimmt OPENAI_API_BASE, falls gesetzt,
    37	    via base_url (kein api_base!).
    38	    """
    39	    kwargs = {
    40	        "model": _model_name(),
    41	        "streaming": streaming,
    42	        "temperature": float(os.getenv("LLM_TEMPERATURE", "0.3")),
    43	        "max_retries": int(os.getenv("LLM_MAX_RETRIES", "2")),
    44	    }
    45	    base = _base_url()
    46	    if base:
    47	        kwargs["base_url"] = base
    48	    return ChatOpenAI(**kwargs)

################################################################################
# FILE: backend/app/services/langgraph/graph/consult/domain_router.py
################################################################################
     1	# backend/app/services/langgraph/graph/consult/domain_router.py
     2	from __future__ import annotations
     3	import json
     4	from typing import List
     5	from langchain_openai import ChatOpenAI
     6	from app.services.langgraph.llm_router import get_router_llm, get_router_fallback_llm
     7	from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage
     8	from app.services.langgraph.prompting import render_template, messages_for_template, strip_json_fence
     9	from .config import ENABLED_DOMAINS
    10	
    11	def detect_domain(llm: ChatOpenAI, msgs: List[AnyMessage], params: dict) -> str:
    12	    router = llm or get_router_llm()
    13	    prompt = render_template(
    14	        "domain_router.jinja2",
    15	        messages=messages_for_template(msgs),
    16	        params_json=json.dumps(params, ensure_ascii=False),
    17	        enabled_domains=ENABLED_DOMAINS,
    18	    )
    19	    # 1st pass
    20	    resp = router.invoke([HumanMessage(content=prompt)])
    21	    domain, conf = None, 0.0
    22	    try:
    23	        data = json.loads(strip_json_fence(resp.content or ""))
    24	        domain = str((data.get("domain") or "")).strip().lower()
    25	        conf = float(data.get("confidence") or 0.0)
    26	    except Exception:
    27	        domain, conf = None, 0.0
    28	
    29	    # Fallback, wenn unsicher
    30	    if (domain not in ENABLED_DOMAINS) or (conf < 0.70):
    31	        fb = get_router_fallback_llm()
    32	        try:
    33	            resp2 = fb.invoke([HumanMessage(content=prompt)])
    34	            data2 = json.loads(strip_json_fence(resp2.content or ""))
    35	            d2 = str((data2.get("domain") or "")).strip().lower()
    36	            c2 = float(data2.get("confidence") or 0.0)
    37	            if (d2 in ENABLED_DOMAINS) and (c2 >= conf):
    38	                domain, conf = d2, c2
    39	        except Exception:
    40	            pass
    41	
    42	    # Heuristische Fallbacks – nur Nutzertext
    43	    if (domain not in ENABLED_DOMAINS) or (conf < 0.40):
    44	        utter = ""
    45	        for m in reversed(msgs or []):
    46	            if hasattr(m, "content") and getattr(m, "content"):
    47	                if isinstance(m, HumanMessage):
    48	                    utter = (m.content or "").lower().strip()
    49	                    break
    50	        if "wellendichtring" in utter or "rwdr" in utter:
    51	            domain = "rwdr"
    52	        elif "stangendichtung" in utter or "kolbenstange" in utter or "hydraulik" in utter:
    53	            domain = "hydraulics_rod"
    54	        elif (params.get("bauform") or "").upper().startswith("BA"):
    55	            domain = "rwdr"
    56	        elif ENABLED_DOMAINS:
    57	            domain = ENABLED_DOMAINS[0]
    58	        else:
    59	            domain = "rwdr"
    60	    return domain

################################################################################
# FILE: backend/app/services/langgraph/graph/consult/domain_runtime.py
################################################################################
     1	# backend/app/services/langgraph/graph/consult/domain_runtime.py
     2	from __future__ import annotations
     3	import importlib
     4	import logging
     5	from typing import Any, Dict, List
     6	from .state import Parameters, Derived
     7	
     8	log = logging.getLogger(__name__)
     9	
    10	def compute_domain(domain: str, params: Parameters) -> Derived:
    11	    try:
    12	        mod = importlib.import_module(f"app.services.langgraph.domains.{domain}.calculator")
    13	        compute = getattr(mod, "compute")
    14	        out = compute(params)  # type: ignore
    15	        return {
    16	            "calculated": dict(out.get("calculated", {})),
    17	            "flags": dict(out.get("flags", {})),
    18	            "warnings": list(out.get("warnings", [])),
    19	            "requirements": list(out.get("requirements", [])),
    20	        }
    21	    except Exception as e:
    22	        log.warning("Domain compute failed (%s): %s", domain, e)
    23	        return {"calculated": {}, "flags": {}, "warnings": [], "requirements": []}
    24	
    25	def missing_by_domain(domain: str, p: Parameters) -> List[str]:
    26	    # ✅ Hydraulik-Stange nutzt stange_mm / nut_d_mm / nut_b_mm
    27	    if domain == "hydraulics_rod":
    28	        req = [
    29	            "falltyp",
    30	            "stange_mm",
    31	            "nut_d_mm",
    32	            "nut_b_mm",
    33	            "medium",
    34	            "temp_max_c",
    35	            "druck_bar",
    36	            "geschwindigkeit_m_s",
    37	        ]
    38	    else:
    39	        req = [
    40	            "falltyp",
    41	            "wellen_mm",
    42	            "gehause_mm",
    43	            "breite_mm",
    44	            "medium",
    45	            "temp_max_c",
    46	            "druck_bar",
    47	            "drehzahl_u_min",
    48	        ]
    49	
    50	    def _is_missing(key: str, val: Any) -> bool:
    51	        if val is None or val == "" or val == "unknown":
    52	            return True
    53	        if key == "druck_bar":
    54	            try: float(val); return False
    55	            except Exception: return True
    56	        if key in ("wellen_mm", "gehause_mm", "breite_mm", "drehzahl_u_min", "geschwindigkeit_m_s",
    57	                   "stange_mm", "nut_d_mm", "nut_b_mm"):
    58	            try: return float(val) <= 0
    59	            except Exception: return True
    60	        if key == "temp_max_c":
    61	            try: float(val); return False
    62	            except Exception: return True
    63	        return False
    64	
    65	    return [k for k in req if _is_missing(k, p.get(k))]
    66	
    67	def anomaly_messages(domain: str, params: Parameters, derived: Derived) -> List[str]:
    68	    msgs: List[str] = []
    69	    flags = (derived.get("flags") or {})
    70	    if flags.get("requires_pressure_stage") and not flags.get("pressure_stage_ack"):
    71	        msgs.append("Ein Überdruck >2 bar ist für Standard-Radialdichtringe kritisch. Dürfen Druckstufenlösungen geprüft werden?")
    72	    if flags.get("speed_high"):
    73	        msgs.append("Die Drehzahl/Umfangsgeschwindigkeit ist hoch – ist sie dauerhaft oder nur kurzzeitig (Spitzen)?")
    74	    if flags.get("temp_very_high"):
    75	        msgs.append("Die Temperatur ist sehr hoch. Handelt es sich um Dauer- oder Spitzentemperaturen?")
    76	    if domain == "hydraulics_rod" and flags.get("extrusion_risk") and not flags.get("extrusion_risk_ack"):
    77	        msgs.append("Bei dem Druck besteht Extrusionsrisiko. Darf eine Stütz-/Back-up-Ring-Lösung geprüft werden?")
    78	    return msgs

################################################################################
# FILE: backend/app/services/langgraph/graph/consult/extract.py
################################################################################
     1	# backend/app/services/langgraph/graph/consult/extract.py
     2	from __future__ import annotations
     3	
     4	import json
     5	import logging
     6	import re
     7	from typing import Any, Dict, List, Optional
     8	
     9	from langchain_core.messages import SystemMessage, HumanMessage
    10	from app.services.langgraph.llm_factory import get_llm
    11	from app.services.langgraph.prompting import render_template
    12	
    13	log = logging.getLogger(__name__)
    14	
    15	# ============================================================
    16	# einfache Heuristik als Fallback
    17	# ============================================================
    18	
    19	_NUMBER = r"[-+]?\d+(?:[.,]\d+)?"
    20	
    21	def _to_float(x: Any) -> Optional[float]:
    22	    try:
    23	        if isinstance(x, (int, float)):
    24	            return float(x)
    25	        s = str(x).strip().replace(",", ".")
    26	        return float(s)
    27	    except Exception:
    28	        return None
    29	
    30	def heuristic_extract(user_input: str) -> Dict[str, Any]:
    31	    txt = (user_input or "").lower()
    32	    out: Dict[str, Any] = {"source": "heuristic"}
    33	
    34	    if any(w in txt for w in ["rwdr", "wellendichtring", "radialwellendichtring"]):
    35	        out["domain"] = "rwdr"; out["falltyp"] = "rwdr"
    36	    elif any(w in txt for w in ["stangendichtung", "kolbenstange", "hydraulik"]):
    37	        out["domain"] = "hydraulics_rod"; out["falltyp"] = "hydraulics_rod"
    38	
    39	    m = re.search(rf"(?P<d>{_NUMBER})\s*[x×]\s*(?P<D>{_NUMBER})\s*[x×]\s*(?P<b>{_NUMBER})\s*mm", txt)
    40	    if m:
    41	        out["wellen_mm"]  = _to_float(m.group("d"))
    42	        out["gehause_mm"] = _to_float(m.group("D"))
    43	        out["breite_mm"]  = _to_float(m.group("b"))
    44	    else:
    45	        md = re.search(rf"(?:welle|d)\s*[:=]?\s*({_NUMBER})\s*mm", txt)
    46	        mD = re.search(rf"(?:gehäuse|gehause|D)\s*[:=]?\s*({_NUMBER})\s*mm", txt)
    47	        mb = re.search(rf"(?:breite|b)\s*[:=]?\s*({_NUMBER})\s*mm", txt)
    48	        if md: out["wellen_mm"]  = _to_float(md.group(1))
    49	        if mD: out["gehause_mm"] = _to_float(mD.group(1))
    50	        if mb: out["breite_mm"]  = _to_float(mb.group(1))
    51	
    52	    tmax = re.search(rf"(?:tmax|temp(?:eratur)?(?:\s*max)?)\s*[:=]?\s*({_NUMBER})\s*°?\s*c", txt)
    53	    if not tmax:
    54	        tmax = re.search(rf"({_NUMBER})\s*°?\s*c", txt)
    55	    if tmax:
    56	        out["temp_max_c"] = _to_float(tmax.group(1))
    57	
    58	    p = re.search(rf"(?:p(?:_?max)?|druck)\s*[:=]?\s*({_NUMBER})\s*bar", txt)
    59	    if p:
    60	        out["druck_bar"] = _to_float(p.group(1))
    61	
    62	    rpm = re.search(rf"(?:n|drehzahl|rpm)\s*[:=]?\s*({_NUMBER})\s*(?:u/?min|rpm)", txt)
    63	    if rpm:
    64	        out["drehzahl_u_min"] = _to_float(rpm.group(1))
    65	
    66	    v = re.search(rf"(?:v|geschwindigkeit)\s*[:=]?\s*({_NUMBER})\s*m/?s", txt)
    67	    if v:
    68	        out["geschwindigkeit_m_s"] = _to_float(v.group(1))
    69	
    70	    med = re.search(r"(?:medium|medien|stoff)\s*[:=]\s*([a-z0-9\-_/.,\s]+)", txt)
    71	    if med:
    72	        out["medium"] = med.group(1).strip()
    73	    else:
    74	        for k in ["öl", "oel", "diesel", "benzin", "kraftstoff", "wasser", "dampf", "säure", "saeure", "lauge"]:
    75	            if k in txt:
    76	                out["medium"] = k; break
    77	
    78	    return out
    79	
    80	# ============================================================
    81	# robustes JSON aus LLM
    82	# ============================================================
    83	
    84	_JSON_RX = re.compile(r"\{[\s\S]*\}")
    85	
    86	def _safe_json(text: str) -> Optional[Dict[str, Any]]:
    87	    if not isinstance(text, str):
    88	        return None
    89	    try:
    90	        return json.loads(text)
    91	    except Exception:
    92	        pass
    93	    m = _JSON_RX.search(text)
    94	    if not m:
    95	        return None
    96	    try:
    97	        return json.loads(m.group(0))
    98	    except Exception:
    99	        return None
   100	
   101	# ============================================================
   102	# Öffentliche API (SIGNATUR passt zu build.py)
   103	# ============================================================
   104	
   105	def extract_params_with_llm(user_input: str, *, rag_context: str | None = None) -> Dict[str, Any]:
   106	    """
   107	    Extrahiert Pflicht-/Kernparameter aus der Nutzeranfrage.
   108	    - FIX: korrektes Rendering von Jinja (keine zusätzlichen Positional-Args)
   109	    - Normales Chat-Completion + robustes JSON-Parsing
   110	    - Fallback: lokale Heuristik
   111	    """
   112	    try:
   113	        sys_prompt = render_template(
   114	            "consult_extract_params.jinja2",
   115	            messages=[{"type": "user", "content": (user_input or "").strip()}],
   116	            params_json="{}",
   117	        )
   118	    except Exception as e:
   119	        log.warning("[extract_params_with_llm] template_render_failed: %r", e)
   120	        return heuristic_extract(user_input)
   121	
   122	    messages: List[Any] = [
   123	        SystemMessage(content=sys_prompt),
   124	        HumanMessage(content=user_input or ""),
   125	    ]
   126	
   127	    llm = get_llm(streaming=False)
   128	
   129	    try:
   130	        resp = llm.invoke(messages)
   131	        text = getattr(resp, "content", "") or ""
   132	    except Exception as e:
   133	        log.warning("[extract_params_with_llm] llm_invoke_failed_plain: %r", e)
   134	        return heuristic_extract(user_input)
   135	
   136	    data = _safe_json(text)
   137	    if not isinstance(data, dict):
   138	        log.info("[extract_params_with_llm] no_json_in_response – using heuristic")
   139	        return heuristic_extract(user_input)
   140	
   141	    normalized: Dict[str, Any] = {}
   142	
   143	    def _pick(name: str, *aliases: str, cast=None):
   144	        for k in (name, *aliases):
   145	            if k in data and data[k] is not None:
   146	                v = data[k]
   147	                if cast:
   148	                    try:
   149	                        v = cast(v)
   150	                    except Exception:
   151	                        pass
   152	                normalized[name] = v
   153	                return
   154	
   155	    _pick("falltyp")
   156	    _pick("domain")
   157	    _pick("wellen_mm", "stange_mm", cast=_to_float)
   158	    _pick("gehause_mm", "nut_d_mm", cast=_to_float)
   159	    _pick("breite_mm", "nut_b_mm", cast=_to_float)
   160	    _pick("temp_max_c", cast=_to_float)
   161	    _pick("drehzahl_u_min", cast=_to_float)
   162	    _pick("druck_bar", cast=_to_float)
   163	    _pick("geschwindigkeit_m_s", cast=_to_float)
   164	    _pick("medium")
   165	
   166	    for k, v in data.items():
   167	        if k not in normalized:
   168	            normalized[k] = v
   169	
   170	    normalized.setdefault("source", "llm_json")
   171	    return normalized
   172	
   173	def extract_params(user_input: str, *, rag_context: str | None = None) -> Dict[str, Any]:
   174	    return extract_params_with_llm(user_input, rag_context=rag_context)

################################################################################
# FILE: backend/app/services/langgraph/graph/consult/heuristic_extract.py
################################################################################
     1	from __future__ import annotations
     2	import re
     3	from typing import Dict, Optional
     4	
     5	_NUM = r"-?\d+(?:[.,]\d+)?"
     6	
     7	def _to_float(s: Optional[str]) -> Optional[float]:
     8	    if not s:
     9	        return None
    10	    try:
    11	        return float(s.replace(",", "."))
    12	    except Exception:
    13	        return None
    14	
    15	def pre_extract_params(text: str) -> Dict[str, object]:
    16	    t = text or ""
    17	    out: Dict[str, object] = {}
    18	
    19	    m = re.search(r"(?:\b(?:rwdr|ba|bauform)\b\s*)?(\d{1,3})\s*[x×]\s*(\d{1,3})\s*[x×]\s*(\d{1,3})", t, re.I)
    20	    if m:
    21	        out["wellen_mm"]  = int(m.group(1))
    22	        out["gehause_mm"] = int(m.group(2))
    23	        out["breite_mm"]  = int(m.group(3))
    24	
    25	    if re.search(r"\bhydraulik ?öl\b", t, re.I):
    26	        out["medium"] = "Hydrauliköl"
    27	    elif re.search(r"\böl\b", t, re.I):
    28	        out["medium"] = "Öl"
    29	    elif re.search(r"\bwasser\b", t, re.I):
    30	        out["medium"] = "Wasser"
    31	
    32	    m = re.search(r"(?:t\s*max|temp(?:eratur)?(?:\s*max)?|t)\s*[:=]?\s*(" + _NUM + r")\s*°?\s*c\b", t, re.I)
    33	    if not m:
    34	        m = re.search(r"\b(" + _NUM + r")\s*°?\s*c\b", t, re.I)
    35	    if m:
    36	        out["temp_max_c"] = _to_float(m.group(1))
    37	
    38	    m = re.search(r"(?:\bdruck\b|[^a-z]p)\s*[:=]?\s*(" + _NUM + r")\s*bar\b", t, re.I)
    39	    if not m:
    40	        m = re.search(r"\b(" + _NUM + r")\s*bar\b", t, re.I)
    41	    if m:
    42	        out["druck_bar"] = _to_float(m.group(1))
    43	
    44	    m = re.search(r"(?:\bn\b|drehzahl)\s*[:=]?\s*(\d{1,7})\s*(?:u/?min|rpm)\b", t, re.I)
    45	    if not m:
    46	        m = re.search(r"\b(\d{1,7})\s*(?:u/?min|rpm)\b", t, re.I)
    47	    if m:
    48	        out["drehzahl_u_min"] = int(m.group(1))
    49	
    50	    m = re.search(r"\bbauform\s*[:=]?\s*([A-Z0-9]{1,4})\b|\b(BA|B1|B2|TC|SC)\b", t, re.I)
    51	    if m:
    52	        out["bauform"] = (m.group(1) or m.group(2) or "").upper()
    53	
    54	    return out

################################################################################
# FILE: backend/app/services/langgraph/graph/consult/__init__.py
################################################################################

################################################################################
# FILE: backend/app/services/langgraph/graph/consult/intent_llm.py
################################################################################
     1	from __future__ import annotations
     2	
     3	"""
     4	LLM-basierter Intent-Router (verpflichtend).
     5	- Nutzt ChatOpenAI (Model per ENV, Default: gpt-5-nano).
     6	- Gibt eines der erlaubten Labels zurück, sonst 'chitchat'.
     7	- Fallback: robuste Heuristik, falls LLM fehlschlägt.
     8	"""
     9	
    10	import os
    11	import re
    12	import logging
    13	from typing import Any, Dict, List, Optional, TypedDict
    14	
    15	log = logging.getLogger("uvicorn.error")
    16	
    17	# Erlaubte Ziele (müssen mit build.py übereinstimmen)
    18	ALLOWED_ROUTES: List[str] = [
    19	    "rag_qa",
    20	    "material_agent",
    21	    "profile_agent",
    22	    "calc_agent",
    23	    "report_agent",
    24	    "memory_export",
    25	    "memory_delete",
    26	    "chitchat",
    27	]
    28	
    29	# Prompt für den reinen Label-Output
    30	_INTENT_PROMPT = """Du bist ein Intent-Router. Antworte NUR mit einem Label (genau wie angegeben):
    31	{allowed}
    32	
    33	Eingabe: {query}
    34	Label:"""
    35	
    36	# Heuristiken als Fallback
    37	_HEURISTICS: List[tuple[str, re.Pattern]] = [
    38	    ("memory_export", re.compile(r"\b(export|download|herunterladen|daten\s*export)\b", re.I)),
    39	    ("memory_delete", re.compile(r"\b(löschen|delete|entfernen)\b", re.I)),
    40	    ("calc_agent",    re.compile(r"\b(rechnen|berechne|calculate|calc|formel|formulas?)\b", re.I)),
    41	    ("report_agent",  re.compile(r"\b(report|bericht|pdf|zusammenfassung|protokoll)\b", re.I)),
    42	    ("material_agent",re.compile(r"\b(material|werkstoff|elastomer|ptfe|fkm|nbr|epdm)\b", re.I)),
    43	    ("profile_agent", re.compile(r"\b(profil|o-ring|x-ring|u-profil|lippe|dichtung\s*profil)\b", re.I)),
    44	    ("rag_qa",        re.compile(r"\b(warum|wie|quelle|dokument|datenblatt|docs?)\b", re.I)),
    45	]
    46	
    47	# State-Shape (nur für Typing; zur Laufzeit wird ein dict genutzt)
    48	class ConsultState(TypedDict, total=False):
    49	    user: str
    50	    chat_id: Optional[str]
    51	    input: str
    52	    route: str
    53	    response: str
    54	    citations: List[Dict[str, Any]]
    55	
    56	# LLM-Konfiguration
    57	try:
    58	    from langchain_openai import ChatOpenAI
    59	    _LLM_OK = bool(os.getenv("OPENAI_API_KEY"))
    60	except Exception:
    61	    ChatOpenAI = None  # type: ignore
    62	    _LLM_OK = False
    63	
    64	def _classify_heuristic(query: str) -> str:
    65	    q = (query or "").lower()
    66	    for label, pattern in _HEURISTICS:
    67	        if pattern.search(q):
    68	            return label
    69	    if re.search(r"[?]|(wie|warum|wieso|quelle|beleg)", q):
    70	        return "rag_qa"
    71	    return "chitchat"
    72	
    73	def _classify_llm(query: str) -> str:
    74	    if not (_LLM_OK and ChatOpenAI):
    75	        raise RuntimeError("LLM not available")
    76	    model_name = os.getenv("OPENAI_ROUTER_MODEL", "gpt-5-nano")
    77	    llm = ChatOpenAI(model=model_name, temperature=0, max_tokens=6)  # type: ignore
    78	    prompt = _INTENT_PROMPT.format(allowed=", ".join(ALLOWED_ROUTES), query=query.strip())
    79	    try:
    80	        resp = llm.invoke(prompt)  # type: ignore
    81	        label = str(getattr(resp, "content", "")).strip().lower()
    82	        if label in ALLOWED_ROUTES:
    83	            return label
    84	    except Exception as exc:
    85	        log.warning("LLM Intent error: %r", exc)
    86	    # Fallback falls Ausgabe nicht sauber ist
    87	    return _classify_heuristic(query)
    88	
    89	def intent_router_node(state: ConsultState) -> ConsultState:
    90	    """Graph-Node: setzt state['route'] über LLM (mit Heuristik-Fallback)."""
    91	    query = state.get("input", "") or ""
    92	    try:
    93	        route = _classify_llm(query)
    94	    except Exception:
    95	        route = _classify_heuristic(query)
    96	
    97	    if route not in ALLOWED_ROUTES:
    98	        route = "chitchat"
    99	
   100	    state["route"] = route
   101	    return state

################################################################################
# FILE: backend/app/services/langgraph/graph/consult/io.py
################################################################################
     1	# backend/app/services/langgraph/graph/consult/io.py
     2	from __future__ import annotations
     3	
     4	import logging
     5	import os
     6	import re
     7	from typing import Any, Dict
     8	
     9	from langchain_core.messages import HumanMessage, AIMessage
    10	from langgraph.checkpoint.base import BaseCheckpointSaver  # Typkompatibel
    11	
    12	from .build import build_consult_graph
    13	
    14	log = logging.getLogger(__name__)
    15	
    16	OFFLINE_MODE = os.getenv("OFFLINE_MODE", "0") == "1"
    17	
    18	# Caches (prozessweit)
    19	_graph_no_cp = None
    20	_graph_by_cp_id: dict[int, Any] = {}  # id(checkpointer) -> compiled graph
    21	
    22	
    23	def _last_ai_text(msgs) -> str:
    24	    for m in reversed(msgs or []):
    25	        if isinstance(m, AIMessage) and isinstance(getattr(m, "content", None), str):
    26	            return (m.content or "").strip()
    27	    return ""
    28	
    29	
    30	# ––– Fallback ohne LLM –––
    31	_DIM_RX = re.compile(r"(\d{1,3})\s*[xX]\s*(\d{1,3})\s*[xX]\s*(\d{1,3})")
    32	_BAR_RX = re.compile(r"(-?\d+(?:[.,]\d+)?)\s*bar", re.I)
    33	
    34	def _local_fallback_reply(user_text: str) -> str:
    35	    t = (user_text or "").strip()
    36	    tl = t.lower()
    37	
    38	    dims = None
    39	    m = _DIM_RX.search(t)
    40	    if m:
    41	        dims = f"{int(m.group(1))}x{int(m.group(2))}x{int(m.group(3))}"
    42	
    43	    if "öl" in tl or "oel" in tl or "oil" in tl:
    44	        medium = "Öl"
    45	        material_hint = "FKM"
    46	        vorteile = "hohe Temperatur- und Ölbeständigkeit, gute Alterungsbeständigkeit"
    47	        einschraenkungen = "nicht ideal für Wasser/Heißwasser"
    48	    elif "wasser" in tl or "water" in tl:
    49	        medium = "Wasser"
    50	        material_hint = "EPDM (alternativ HNBR, je nach Temperatur)"
    51	        vorteile = "gute Wasser-/Dampfbeständigkeit (EPDM)"
    52	        einschraenkungen = "nicht ölbeständig (EPDM)"
    53	    else:
    54	        medium = "nicht angegeben"
    55	        material_hint = "NBR (Preis/Leistung) oder FKM (Temperatur/Öl)"
    56	        vorteile = "solide Beständigkeit je nach Materialwahl"
    57	        einschraenkungen = "Materialwahl abhängig von Medium/Temperatur"
    58	
    59	    pbar = None
    60	    mp = _BAR_RX.search(tl)
    61	    if mp:
    62	        try:
    63	            pbar = float(mp.group(1).replace(",", "."))
    64	        except Exception:
    65	            pbar = None
    66	
    67	    druck_hinweis = ""
    68	    if pbar is not None and pbar > 2:
    69	        druck_hinweis = (
    70	            "\n- **Hinweis:** Überdruck >2 bar ist für Standard-Radialdichtringe kritisch. "
    71	            "Bitte Druckstufen-/Entlastungslösungen prüfen."
    72	        )
    73	
    74	    typ = f"BA {dims}" if dims else "BA (Standard-Profil)"
    75	
    76	    return (
    77	        "🔎 **Empfehlung (Fallback – LLM temporär nicht erreichbar)**\n\n"
    78	        f"**Typ:** {typ}\n"
    79	        f"**Werkstoff (Hint):** {material_hint}\n"
    80	        f"**Medium:** {medium}\n\n"
    81	        f"**Vorteile:** {vorteile}\n"
    82	        f"**Einschränkungen:** {einschraenkungen}\n"
    83	        f"{druck_hinweis}\n\n"
    84	        "**Nächste Schritte:**\n"
    85	        "- Wenn du **Maße (Welle/Gehäuse/Breite)**, **Medium**, **Tmax**, **Druck** und **Drehzahl/Relativgeschwindigkeit** angibst,\n"
    86	        "  erstelle ich eine präzisere Empfehlung inkl. Alternativen.\n"
    87	        "_(Dies ist eine lokale Antwort ohne LLM; die Detailberatung folgt automatisch, sobald der Dienst wieder verfügbar ist.)_"
    88	    )
    89	
    90	
    91	def _get_graph(checkpointer: BaseCheckpointSaver | None):
    92	    """
    93	    Liefert einen kompilierten Consult-Graphen.
    94	    - Ohne Checkpointer: Singleton.
    95	    - Mit Checkpointer: pro-Saver gecachter Graph (id(checkpointer) als Key).
    96	    """
    97	    global _graph_no_cp, _graph_by_cp_id
    98	
    99	    if checkpointer is None:
   100	        if _graph_no_cp is None:
   101	            g = build_consult_graph()
   102	            _graph_no_cp = g.compile()
   103	            log.info("[consult.io] Graph compiled WITHOUT checkpointer.")
   104	        return _graph_no_cp
   105	
   106	    key = id(checkpointer)
   107	    if key not in _graph_by_cp_id:
   108	        try:
   109	            g = build_consult_graph()
   110	            _graph_by_cp_id[key] = g.compile(checkpointer=checkpointer)
   111	            log.info("[consult.io] Graph compiled WITH provided checkpointer.")
   112	        except Exception as e:
   113	            log.warning("[consult.io] Failed to compile with provided checkpointer: %s. Falling back.", e)
   114	            if _graph_no_cp is None:
   115	                g = build_consult_graph()
   116	                _graph_no_cp = g.compile()
   117	            return _graph_no_cp
   118	
   119	    return _graph_by_cp_id[key]
   120	
   121	
   122	def invoke_consult(
   123	    text: str,
   124	    *,
   125	    thread_id: str,
   126	    checkpointer: BaseCheckpointSaver | None = None,
   127	) -> str:
   128	    """
   129	    Führt eine Consult-Anfrage aus.
   130	    - checkpointer: Optionaler externer Saver (z. B. aus app.state).
   131	    """
   132	    user_text = (text or "").strip()
   133	    if not user_text:
   134	        return ""
   135	
   136	    if OFFLINE_MODE:
   137	        return _local_fallback_reply(user_text)
   138	
   139	    try:
   140	        graph = _get_graph(checkpointer)
   141	        cfg: Dict[str, Any] = {"configurable": {"thread_id": thread_id}}
   142	        initial = {"messages": [HumanMessage(content=user_text)]}
   143	        result = graph.invoke(initial, config=cfg)
   144	
   145	        out = _last_ai_text((result or {}).get("messages", []))
   146	        if out:
   147	            return out
   148	
   149	        log.warning("[consult.io] Graph returned no AI text; using local fallback.")
   150	        return _local_fallback_reply(user_text)
   151	
   152	    except Exception as e:
   153	        log.error("[consult.io] Graph invocation failed, using fallback. Error: %s", e)
   154	        return _local_fallback_reply(user_text)

################################################################################
# FILE: backend/app/services/langgraph/graph/consult/memory_utils.py
################################################################################
     1	from __future__ import annotations
     2	
     3	import os
     4	import json
     5	from typing import List, Dict, Literal, TypedDict
     6	
     7	from redis import Redis
     8	from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, AnyMessage
     9	
    10	
    11	def _redis() -> Redis:
    12	    url = os.getenv("REDIS_URL", "redis://redis:6379/0")
    13	    return Redis.from_url(url, decode_responses=True)
    14	
    15	def _conv_key(thread_id: str) -> str:
    16	    # Gleicher Key wie im SSE: chat:stm:{thread_id}:messages
    17	    return f"chat:stm:{thread_id}:messages"
    18	
    19	
    20	def write_message(*, thread_id: str, role: Literal["user", "assistant", "system"], content: str) -> None:
    21	    if not content:
    22	        return
    23	    r = _redis()
    24	    key = _conv_key(thread_id)
    25	    item = json.dumps({"role": role, "content": content}, ensure_ascii=False)
    26	    pipe = r.pipeline()
    27	    pipe.lpush(key, item)
    28	    pipe.ltrim(key, 0, int(os.getenv("STM_MAX_ITEMS", "200")) - 1)
    29	    pipe.expire(key, int(os.getenv("STM_TTL_SEC", "604800")))  # 7 Tage
    30	    pipe.execute()
    31	
    32	
    33	def read_history_raw(thread_id: str, limit: int = 80) -> List[Dict[str, str]]:
    34	    """Rohdaten (älteste -> neueste)."""
    35	    r = _redis()
    36	    key = _conv_key(thread_id)
    37	    items = r.lrange(key, 0, limit - 1) or []
    38	    out: List[Dict[str, str]] = []
    39	    for s in reversed(items):  # Redis speichert neueste zuerst
    40	        try:
    41	            obj = json.loads(s)
    42	            role = (obj.get("role") or "").lower()
    43	            content = obj.get("content") or ""
    44	            if role and content:
    45	                out.append({"role": role, "content": content})
    46	        except Exception:
    47	            continue
    48	    return out
    49	
    50	
    51	def read_history(thread_id: str, limit: int = 80) -> List[AnyMessage]:
    52	    """LangChain-Messages (älteste -> neueste)."""
    53	    msgs: List[AnyMessage] = []
    54	    for item in read_history_raw(thread_id, limit=limit):
    55	        role = item["role"]
    56	        content = item["content"]
    57	        if role in ("user", "human"):
    58	            msgs.append(HumanMessage(content=content))
    59	        elif role in ("assistant", "ai"):
    60	            msgs.append(AIMessage(content=content))
    61	        elif role == "system":
    62	            msgs.append(SystemMessage(content=content))
    63	    return msgs

################################################################################
# FILE: backend/app/services/langgraph/graph/consult/nodes/ask_missing.py
################################################################################
     1	# backend/app/services/langgraph/graph/consult/nodes/ask_missing.py
     2	from __future__ import annotations
     3	
     4	import logging
     5	from typing import Any, Dict, List
     6	
     7	from langchain_core.messages import AIMessage
     8	from app.services.langgraph.prompting import render_template
     9	from ..utils import missing_by_domain, anomaly_messages, normalize_messages
    10	
    11	log = logging.getLogger(__name__)
    12	
    13	# ---------- Feldlabels ----------
    14	FIELD_LABELS_RWDR = {
    15	    "falltyp": "Anwendungsfall (Ersatz/Neu/Optimierung)",
    16	    "wellen_mm": "Welle (mm)",
    17	    "gehause_mm": "Gehäuse (mm)",
    18	    "breite_mm": "Breite (mm)",
    19	    "bauform": "Bauform/Profil",
    20	    "medium": "Medium",
    21	    "temp_min_c": "Temperatur min (°C)",
    22	    "temp_max_c": "Temperatur max (°C)",
    23	    "druck_bar": "Druck (bar)",
    24	    "drehzahl_u_min": "Drehzahl (U/min)",
    25	    "geschwindigkeit_m_s": "Relativgeschwindigkeit (m/s)",
    26	    "umgebung": "Umgebung",
    27	    "prioritaet": "Priorität (z. B. Preis, Lebensdauer)",
    28	    "besondere_anforderungen": "Besondere Anforderungen",
    29	    "bekannte_probleme": "Bekannte Probleme",
    30	}
    31	DISPLAY_ORDER_RWDR = [
    32	    "falltyp","wellen_mm","gehause_mm","breite_mm","bauform","medium",
    33	    "temp_min_c","temp_max_c","druck_bar","drehzahl_u_min","geschwindigkeit_m_s",
    34	    "umgebung","prioritaet","besondere_anforderungen","bekannte_probleme",
    35	]
    36	
    37	FIELD_LABELS_HYD = {
    38	    "falltyp": "Anwendungsfall (Ersatz/Neu/Optimierung)",
    39	    "stange_mm": "Stange (mm)",
    40	    "nut_d_mm": "Nut-Ø D (mm)",
    41	    "nut_b_mm": "Nutbreite B (mm)",
    42	    "medium": "Medium",
    43	    "temp_max_c": "Temperatur max (°C)",
    44	    "druck_bar": "Druck (bar)",
    45	    "geschwindigkeit_m_s": "Relativgeschwindigkeit (m/s)",
    46	}
    47	DISPLAY_ORDER_HYD = [
    48	    "falltyp","stange_mm","nut_d_mm","nut_b_mm","medium","temp_max_c","druck_bar","geschwindigkeit_m_s",
    49	]
    50	
    51	def _friendly_list(keys: List[str], domain: str) -> str:
    52	    if domain == "hydraulics_rod":
    53	        labels, order = FIELD_LABELS_HYD, DISPLAY_ORDER_HYD
    54	    else:
    55	        labels, order = FIELD_LABELS_RWDR, DISPLAY_ORDER_RWDR
    56	    ordered = [k for k in order if k in keys]
    57	    return ", ".join(f"**{labels.get(k, k)}**" for k in ordered)
    58	
    59	# ---------- Node ----------
    60	def ask_missing_node(state: Dict[str, Any]) -> Dict[str, Any]:
    61	    """
    62	    Stellt Rückfragen nur bei Beratungsbedarf.
    63	    Liefert zusätzlich ein UI-Event zum Öffnen des Formular-Drawers.
    64	    """
    65	    consult_required = bool(state.get("consult_required", True))
    66	    if not consult_required:
    67	        return {**state, "messages": [], "phase": "ask_missing"}
    68	
    69	    _ = normalize_messages(state.get("messages", []))
    70	    params: Dict[str, Any] = state.get("params") or {}
    71	    domain: str = (state.get("domain") or "rwdr").strip().lower()
    72	    derived: Dict[str, Any] = state.get("derived") or {}
    73	
    74	    # Sprache (Fallback de)
    75	    lang = (params.get("lang") or state.get("lang") or "de").lower()
    76	
    77	    missing = missing_by_domain(domain, params)
    78	    log.info("[ask_missing_node] fehlend=%s domain=%s consult_required=%s", missing, domain, consult_required)
    79	
    80	    if missing:
    81	        friendly = _friendly_list(missing, domain)
    82	        # Einzeilenbeispiel je Domäne
    83	        example = (
    84	            "Welle 25, Gehäuse 47, Breite 7, Medium Öl, Tmax 80, Druck 2 bar, n 1500"
    85	            if domain != "hydraulics_rod"
    86	            else "Stange 25, Nut D 32, Nut B 6, Medium Öl, Tmax 80, Druck 160 bar, v 0,3 m/s"
    87	        )
    88	
    89	        content = render_template(
    90	            "ask_missing.jinja2",
    91	            domain=domain,
    92	            friendly=friendly,
    93	            example=example,
    94	            lang=lang,
    95	        )
    96	
    97	        ui_event = {
    98	            "ui_action": "open_form",
    99	            "form_id": f"{domain}_params_v1",
   100	            "schema_ref": f"domains/{domain}/params@1.0.0",
   101	            "missing": missing,
   102	            "prefill": {k: v for k, v in params.items() if v not in (None, "", [])},
   103	        }
   104	        return {
   105	            **state,
   106	            "messages": [AIMessage(content=content)],
   107	            "phase": "ask_missing",
   108	            "ui_event": ui_event,
   109	            "missing_fields": missing,
   110	        }
   111	
   112	    followups = anomaly_messages(domain, params, derived)
   113	    if followups:
   114	        content = render_template("ask_missing_followups.jinja2", followups=followups[:2], lang=lang)
   115	        ui_event = {
   116	            "ui_action": "open_form",
   117	            "form_id": f"{domain}_params_v1",
   118	            "schema_ref": f"domains/{domain}/params@1.0.0",
   119	            "missing": [],
   120	            "prefill": {k: v for k, v in params.items() if v not in (None, "", [])},
   121	        }
   122	        return {
   123	            **state,
   124	            "messages": [AIMessage(content=content)],
   125	            "phase": "ask_missing",
   126	            "ui_event": ui_event,
   127	            "missing_fields": [],
   128	        }
   129	
   130	    return {**state, "messages": [], "phase": "ask_missing"}

################################################################################
# FILE: backend/app/services/langgraph/graph/consult/nodes/calc_agent.py
################################################################################
     1	# backend/app/services/langgraph/graph/consult/nodes/calc_agent.py
     2	from __future__ import annotations
     3	
     4	import logging
     5	from typing import Any, Dict
     6	
     7	log = logging.getLogger(__name__)
     8	
     9	
    10	def _num(x: Any) -> float | None:
    11	    try:
    12	        if x in (None, "", []):
    13	            return None
    14	        if isinstance(x, bool):
    15	            return None
    16	        return float(x)
    17	    except Exception:
    18	        return None
    19	
    20	
    21	def _deep_merge(a: Dict[str, Any], b: Dict[str, Any]) -> Dict[str, Any]:
    22	    """
    23	    flache & verschachtelte Dicts zusammenführen (b gewinnt),
    24	    nützlich um 'derived.calculated' nicht zu überschreiben.
    25	    """
    26	    out = dict(a or {})
    27	    for k, v in (b or {}).items():
    28	        if isinstance(v, dict) and isinstance(out.get(k), dict):
    29	            out[k] = _deep_merge(out[k], v)
    30	        else:
    31	            out[k] = v
    32	    return out
    33	
    34	
    35	def _calc_rwdr(params: Dict[str, Any]) -> Dict[str, Any]:
    36	    """Berechnungen für Radial-Wellendichtringe (RWDR)."""
    37	    d_mm = _num(params.get("wellen_mm"))
    38	    n_rpm = _num(params.get("drehzahl_u_min"))
    39	    p_bar = _num(params.get("druck_bar"))
    40	    tmax = _num(params.get("temp_max_c"))
    41	
    42	    calc: Dict[str, Any] = {}
    43	
    44	    # Umfangsgeschwindigkeit v = π * d[m] * n[1/s]
    45	    if d_mm is not None and n_rpm is not None and d_mm > 0 and n_rpm >= 0:
    46	        d_m = d_mm / 1000.0
    47	        v_ms = 3.141592653589793 * d_m * (n_rpm / 60.0)
    48	        calc["umfangsgeschwindigkeit_m_s"] = v_ms
    49	        # kompatibel zu älteren Keys
    50	        calc["surface_speed_m_s"] = round(v_ms, 3)
    51	
    52	    # PV-Indikator (einfaches Produkt) → Orientierung für thermische Last
    53	    if p_bar is not None and calc.get("umfangsgeschwindigkeit_m_s") is not None:
    54	        calc["pv_indicator_bar_ms"] = p_bar * calc["umfangsgeschwindigkeit_m_s"]
    55	
    56	    # Material-Hinweise (leichtgewichtig / erweiterbar)
    57	    mat_whitelist: list[str] = []
    58	    mat_blacklist: list[str] = []
    59	
    60	    medium = (params.get("medium") or "").strip().lower()
    61	    if "wasser" in medium:
    62	        # Wasser → NBR okay, FKM oft okay, PTFE nur bei spezieller Ausführung
    63	        mat_blacklist.append("PTFE")
    64	    if tmax is not None:
    65	        if tmax > 120:
    66	            mat_whitelist.append("FKM")
    67	        if tmax > 200:
    68	            # sehr hohe T → PTFE grundsätzlich denkbar (aber oben ggf. ausgeschlossen)
    69	            mat_whitelist.append("PTFE")
    70	
    71	    reqs: list[str] = []
    72	    if "PTFE" in mat_blacklist:
    73	        reqs.append("Vermeide Materialien: PTFE")
    74	
    75	    flags: Dict[str, Any] = {}
    76	    if p_bar is not None and p_bar > 1.0:
    77	        flags["druckbelastet"] = True
    78	
    79	    out = {
    80	        "calculated": calc,
    81	        "material_whitelist": mat_whitelist,
    82	        "material_blacklist": mat_blacklist,
    83	        "requirements": reqs,
    84	        "flags": flags,
    85	    }
    86	    return out
    87	
    88	
    89	def _calc_hydraulics_rod(params: Dict[str, Any]) -> Dict[str, Any]:
    90	    """Berechnungen für Hydraulik-Stangendichtungen."""
    91	    p_bar = _num(params.get("druck_bar"))
    92	    v_lin = _num(params.get("geschwindigkeit_m_s"))  # lineare Stangengeschwindigkeit
    93	    tmax = _num(params.get("temp_max_c"))
    94	
    95	    calc: Dict[str, Any] = {}
    96	    if p_bar is not None and v_lin is not None:
    97	        # einfacher PV-Indikator (lineare Geschwindigkeit)
    98	        calc["pv_indicator_bar_ms"] = p_bar * v_lin
    99	
   100	    # kleine Heuristik zur Extrusionsgefahr bei hohen Drücken
   101	    flags: Dict[str, Any] = {}
   102	    reqs: list[str] = []
   103	    if p_bar is not None and p_bar >= 160:
   104	        flags["extrusion_risk"] = True
   105	        reqs.append("Stütz-/Back-up-Ring prüfen (≥160 bar).")
   106	
   107	    # Materialpräferenz bei hohen Temperaturen
   108	    mat_whitelist: list[str] = []
   109	    if tmax is not None and tmax > 100:
   110	        mat_whitelist.append("FKM")
   111	
   112	    out = {
   113	        "calculated": calc,
   114	        "flags": flags,
   115	        "requirements": reqs,
   116	        "material_whitelist": mat_whitelist,
   117	        "material_blacklist": [],
   118	    }
   119	    return out
   120	
   121	
   122	def calc_agent_node(state: Dict[str, Any]) -> Dict[str, Any]:
   123	    """
   124	    Dedizierter Kalkulations-Node:
   125	    - führt domänenspezifische Rechen- & Heuristikschritte aus,
   126	    - schreibt Ergebnisse nach state['derived'] (nicht destruktiv),
   127	    - hinterlässt 'phase': 'calc_agent'.
   128	    """
   129	    domain = (state.get("domain") or "rwdr").strip().lower()
   130	    params = dict(state.get("params") or {})
   131	    derived_existing = dict(state.get("derived") or {})
   132	
   133	    try:
   134	        if domain == "hydraulics_rod":
   135	            derived_new = _calc_hydraulics_rod(params)
   136	        else:
   137	            # Default: RWDR
   138	            derived_new = _calc_rwdr(params)
   139	    except Exception as e:
   140	        log.warning("[calc_agent] calc_failed", exc=str(e))
   141	        # Fehler nicht eskalieren – einfach Phase setzen
   142	        return {**state, "phase": "calc_agent"}
   143	
   144	    # nicht-destruktiv zusammenführen
   145	    derived_merged = _deep_merge(derived_existing, derived_new)
   146	
   147	    # Kompatibilität: einzelner Key für v [m/s], falls benötigt
   148	    v = (
   149	        derived_merged.get("calculated", {}).get("umfangsgeschwindigkeit_m_s")
   150	        or params.get("relativgeschwindigkeit_ms")
   151	    )
   152	    if v is not None:
   153	        derived_merged["relativgeschwindigkeit_ms"] = v
   154	
   155	    new_state = {**state, "derived": derived_merged, "phase": "calc_agent"}
   156	    return new_state

################################################################################
# FILE: backend/app/services/langgraph/graph/consult/nodes/calc_node.py
################################################################################
     1	# backend/app/services/langgraph/graph/consult/nodes/calc_node.py
     2	from __future__ import annotations
     3	
     4	import math
     5	from typing import Any, Dict, Tuple
     6	
     7	def _norm_float(x: Any) -> float | None:
     8	    try:
     9	        if x is None or x == "" or x == []:
    10	            return None
    11	        return float(x)
    12	    except Exception:
    13	        return None
    14	
    15	def _merge_derived(base: Dict[str, Any], add: Dict[str, Any]) -> Dict[str, Any]:
    16	    """Non-destruktiv in state['derived'] zusammenführen."""
    17	    out = dict(base or {})
    18	    for k, v in (add or {}).items():
    19	        if isinstance(v, dict) and isinstance(out.get(k), dict):
    20	            out[k] = {**out[k], **v}
    21	        else:
    22	            out[k] = v
    23	    return out
    24	
    25	# -------------------------
    26	#   RWDR – Berechnungen
    27	# -------------------------
    28	def _calc_rwdr(params: Dict[str, Any]) -> Dict[str, Any]:
    29	    d_mm = _norm_float(params.get("wellen_mm"))
    30	    n_rpm = _norm_float(params.get("drehzahl_u_min"))
    31	    druck_bar = _norm_float(params.get("druck_bar"))
    32	    tmax = _norm_float(params.get("temp_max_c"))
    33	
    34	    v_ms = None
    35	    if d_mm and n_rpm:
    36	        d_m = d_mm / 1000.0
    37	        n_rps = n_rpm / 60.0
    38	        v_ms = math.pi * d_m * n_rps  # Umfangsgeschwindigkeit
    39	
    40	    pv = None
    41	    if v_ms is not None and druck_bar is not None:
    42	        pv = druck_bar * v_ms  # einfacher PV-Indikator
    43	
    44	    # leichte Material-Whitelist/Blacklist-Heuristiken
    45	    medium_raw = (params.get("medium") or "").strip().lower()
    46	    mat_whitelist: list[str] = []
    47	    mat_blacklist: list[str] = []
    48	
    49	    if "wasser" in medium_raw:
    50	        mat_whitelist += ["EPDM"]
    51	        mat_blacklist += ["NBR"]
    52	    if "öl" in medium_raw or "oel" in medium_raw:
    53	        mat_whitelist += ["NBR", "FKM"]
    54	
    55	    calculated = {
    56	        "umfangsgeschwindigkeit_m_s": v_ms,
    57	        "surface_speed_m_s": round(v_ms, 3) if isinstance(v_ms, (int, float)) else None,
    58	        "pv_indicator_bar_ms": pv,
    59	        "druck_bar": druck_bar,
    60	        "temp_max_c": tmax,
    61	    }
    62	    # Filtere Nones raus
    63	    calculated = {k: v for k, v in calculated.items() if v is not None}
    64	
    65	    return {
    66	        "calculated": calculated,
    67	        "flags": {},
    68	        "warnings": [],
    69	        "requirements": [],
    70	        "material_whitelist": mat_whitelist,
    71	        "material_blacklist": mat_blacklist,
    72	    }
    73	
    74	# -------------------------
    75	#   Hydraulik-Stange
    76	# -------------------------
    77	def _calc_hydraulics_rod(params: Dict[str, Any]) -> Dict[str, Any]:
    78	    v_ms = _norm_float(params.get("geschwindigkeit_m_s"))
    79	    druck_bar = _norm_float(params.get("druck_bar"))
    80	    tmax = _norm_float(params.get("temp_max_c"))
    81	    stange_mm = _norm_float(params.get("stange_mm"))
    82	
    83	    # Umfangsgeschwindigkeit nur, wenn Drehzahl bekannt; sonst weglassen.
    84	    u_ms = None
    85	    n_rpm = _norm_float(params.get("drehzahl_u_min"))
    86	    if stange_mm and n_rpm:
    87	        d_m = stange_mm / 1000.0
    88	        n_rps = n_rpm / 60.0
    89	        u_ms = math.pi * d_m * n_rps
    90	
    91	    pv = None
    92	    if v_ms is not None and druck_bar is not None:
    93	        pv = druck_bar * v_ms
    94	
    95	    flags: Dict[str, bool] = {}
    96	    requirements: list[str] = []
    97	
    98	    if (druck_bar or 0) >= 160:
    99	        flags["extrusion_risk"] = True
   100	        requirements.append("Stütz-/Back-up-Ring prüfen (≥160 bar).")
   101	
   102	    calculated = {
   103	        "geschwindigkeit_m_s": v_ms,
   104	        "umfangsgeschwindigkeit_m_s": u_ms,
   105	        "pv_indicator_bar_ms": pv,
   106	        "druck_bar": druck_bar,
   107	        "temp_max_c": tmax,
   108	        "stange_mm": stange_mm,
   109	        "bohrung_mm": _norm_float(params.get("nut_d_mm")),
   110	    }
   111	    calculated = {k: v for k, v in calculated.items() if v is not None}
   112	
   113	    return {
   114	        "calculated": calculated,
   115	        "flags": flags,
   116	        "warnings": [],
   117	        "requirements": requirements,
   118	        "material_whitelist": [],
   119	        "material_blacklist": [],
   120	    }
   121	
   122	# --------------------------------
   123	#   Öffentlicher LangGraph-Node
   124	# --------------------------------
   125	def calc_node(state: Dict[str, Any]) -> Dict[str, Any]:
   126	    """
   127	    Aggregiert domänenspezifische Vorberechnungen und schreibt sie nach
   128	    state['derived'] zurück. Keine I/O, keine LLM-Aufrufe.
   129	    """
   130	    params: Dict[str, Any] = dict(state.get("params") or {})
   131	    domain = (state.get("domain") or "rwdr").strip().lower()
   132	
   133	    # Bestehende derived (falls vorher schon vorhanden) übernehmen
   134	    derived_in = dict(state.get("derived") or {})
   135	
   136	    try:
   137	        if domain in ("rwdr", "radial_wellendichtring", "wellendichtring"):
   138	            out = _calc_rwdr(params)
   139	        elif domain in ("hydraulics_rod", "rod", "hydraulik_stange"):
   140	            out = _calc_hydraulics_rod(params)
   141	        else:
   142	            out = {"calculated": {}, "flags": {}, "warnings": [], "requirements": []}
   143	
   144	        # zusammenführen
   145	        derived_out = _merge_derived(
   146	            derived_in,
   147	            {
   148	                "calculated": out.get("calculated", {}),
   149	                "flags": {**derived_in.get("flags", {}), **out.get("flags", {})},
   150	                "warnings": list({*(derived_in.get("warnings") or []), *(out.get("warnings") or [])}),
   151	                "requirements": list({*(derived_in.get("requirements") or []), *(out.get("requirements") or [])}),
   152	                "material_whitelist": list({*derived_in.get("material_whitelist", []), *out.get("material_whitelist", [])}),
   153	                "material_blacklist": list({*derived_in.get("material_blacklist", []), *out.get("material_blacklist", [])}),
   154	            },
   155	        )
   156	
   157	        return {**state, "derived": derived_out, "phase": "calc"}
   158	    except Exception:
   159	        # Fallback: state unverändert weiterreichen
   160	        return {**state, "derived": derived_in, "phase": "calc"}

################################################################################
# FILE: backend/app/services/langgraph/graph/consult/nodes/explain.py
################################################################################
     1	# backend/app/services/langgraph/graph/consult/nodes/explain.py
     2	from __future__ import annotations
     3	
     4	from typing import Any, Dict, List, Optional, Callable
     5	import json
     6	import structlog
     7	from langchain_core.messages import AIMessage
     8	from app.services.langgraph.prompting import render_template
     9	
    10	log = structlog.get_logger(__name__)
    11	
    12	def _top_sources(docs: List[Dict[str, Any]], k: int = 3) -> List[str]:
    13	    if not docs:
    14	        return []
    15	    def _score(d: Dict[str, Any]) -> float:
    16	        try:
    17	            if d.get("fused_score") is not None:
    18	                return float(d["fused_score"])
    19	            return max(float(d.get("vector_score") or 0.0),
    20	                       float(d.get("keyword_score") or 0.0) / 100.0)
    21	        except Exception:
    22	            return 0.0
    23	    tops = sorted(docs, key=_score, reverse=True)[:k]
    24	    out: List[str] = []
    25	    for d in tops:
    26	        src = d.get("source") or (d.get("metadata") or {}).get("source") or ""
    27	        if src:
    28	            out.append(str(src))
    29	    seen, uniq = set(), []
    30	    for s in out:
    31	        if s not in seen:
    32	            seen.add(s)
    33	            uniq.append(s)
    34	    return uniq
    35	
    36	def _emit_text(events: Optional[Callable[[Dict[str, Any]], None]],
    37	               node: str, text: str, chunk_size: int = 180) -> None:
    38	    if not events or not text:
    39	        return
    40	    for i in range(0, len(text), chunk_size):
    41	        events({"type": "stream_text", "node": node, "text": text[i:i+chunk_size]})
    42	
    43	def _last_ai_text(state: Dict[str, Any]) -> str:
    44	    """Zieht den Text der letzten AIMessage (string oder tool-structured)."""
    45	    msgs = state.get("messages") or []
    46	    last_ai = None
    47	    for m in reversed(msgs):
    48	        t = (getattr(m, "type", "") or getattr(m, "role", "") or "").lower()
    49	        if t in ("ai", "assistant"):
    50	            last_ai = m
    51	            break
    52	    if not last_ai:
    53	        return ""
    54	    content = getattr(last_ai, "content", None)
    55	    if isinstance(content, str):
    56	        return content.strip()
    57	    # LangChain kann Liste aus {"type":"text","text":"..."} liefern
    58	    out_parts: List[str] = []
    59	    if isinstance(content, list):
    60	        for p in content:
    61	            if isinstance(p, str):
    62	                out_parts.append(p)
    63	            elif isinstance(p, dict) and isinstance(p.get("text"), str):
    64	                out_parts.append(p["text"])
    65	    return "\n".join(out_parts).strip()
    66	
    67	def _parse_recommendation(text: str) -> Dict[str, Any]:
    68	    """
    69	    Akzeptiert:
    70	      1) {"empfehlungen":[{typ, werkstoff, begruendung, vorteile, einschraenkungen, ...}, ...]}
    71	      2) {"main": {...}, "alternativen": [...], "hinweise":[...]}
    72	      3) {"text": "<JSON string>"}  -> wird rekursiv geparst
    73	    """
    74	    if not text:
    75	        return {}
    76	
    77	    def _loads_maybe(s: str):
    78	        try:
    79	            return json.loads(s)
    80	        except Exception:
    81	            return None
    82	
    83	    obj = _loads_maybe(text)
    84	    if isinstance(obj, dict) and "text" in obj and isinstance(obj["text"], str):
    85	        obj2 = _loads_maybe(obj["text"])
    86	        if isinstance(obj2, dict):
    87	            obj = obj2
    88	
    89	    if not isinstance(obj, dict):
    90	        return {}
    91	
    92	    # Form 2
    93	    if "main" in obj or "alternativen" in obj:
    94	        main = obj.get("main") or {}
    95	        alternativen = obj.get("alternativen") or []
    96	        hinweise = obj.get("hinweise") or []
    97	        return {"main": main, "alternativen": alternativen, "hinweise": hinweise}
    98	
    99	    # Form 1
   100	    if isinstance(obj.get("empfehlungen"), list) and obj["empfehlungen"]:
   101	        recs = obj["empfehlungen"]
   102	        main = recs[0] if isinstance(recs[0], dict) else {}
   103	        alternativen = [r for r in recs[1:] if isinstance(r, dict)]
   104	        return {"main": main, "alternativen": alternativen, "hinweise": obj.get("hinweise") or []}
   105	
   106	    return {}
   107	
   108	def explain_node(state: Dict[str, Any], *, events: Optional[Callable[[Dict[str, Any]], None]] = None) -> Dict[str, Any]:
   109	    """
   110	    Rendert die Empfehlung als freundliches Markdown (explain.jinja2),
   111	    streamt Chunks (falls WS-Events übergeben werden) und hängt eine AIMessage an.
   112	    Holt sich – falls nötig – main/alternativen automatisch aus der letzten AI-JSON.
   113	    """
   114	    params: Dict[str, Any] = state.get("params") or {}
   115	    docs: List[Dict[str, Any]] = state.get("retrieved_docs") or state.get("docs") or []
   116	    sources = _top_sources(docs, k=3)
   117	
   118	    # Falls main/alternativen/hinweise fehlen, aus der letzten AI-Message extrahieren
   119	    main = state.get("main") or {}
   120	    alternativen = state.get("alternativen") or []
   121	    hinweise = state.get("hinweise") or []
   122	    if not main and not alternativen:
   123	        parsed = _parse_recommendation(_last_ai_text(state))
   124	        if parsed:
   125	            main = parsed.get("main") or main
   126	            alternativen = parsed.get("alternativen") or alternativen
   127	            if not hinweise:
   128	                hinweise = parsed.get("hinweise") or []
   129	
   130	    md = render_template(
   131	        "explain.jinja2",
   132	        main=main or {},
   133	        alternativen=alternativen or [],
   134	        derived=state.get("derived") or {},
   135	        hinweise=hinweise or [],
   136	        params=params,
   137	        sources=sources,
   138	    ).strip()
   139	
   140	    _emit_text(events, node="explain", text=md)
   141	
   142	    msgs = (state.get("messages") or []) + [AIMessage(content=md)]
   143	    return {
   144	        **state,
   145	        "main": main,
   146	        "alternativen": alternativen,
   147	        "hinweise": hinweise,
   148	        "phase": "explain",
   149	        "messages": msgs,
   150	        "explanation": md,
   151	        "retrieved_docs": docs,
   152	    }

################################################################################
# FILE: backend/app/services/langgraph/graph/consult/nodes/__init__.py
################################################################################
     1	# backend/app/services/langgraph/graph/consult/nodes/__init__.py
     2	# (nur für Paketinitialisierung)

################################################################################
# FILE: backend/app/services/langgraph/graph/consult/nodes/intake.py
################################################################################
     1	# backend/app/services/langgraph/graph/consult/nodes/intake.py
     2	from __future__ import annotations
     3	
     4	import json
     5	import logging
     6	from typing import Any, Dict
     7	
     8	from langchain_core.messages import HumanMessage
     9	from app.services.langgraph.llm_factory import get_llm as create_llm
    10	from app.services.langgraph.prompting import (
    11	    render_template,
    12	    messages_for_template,
    13	    strip_json_fence,
    14	)
    15	from ..utils import normalize_messages
    16	
    17	log = logging.getLogger(__name__)
    18	
    19	
    20	def intake_node(state: Dict[str, Any]) -> Dict[str, Any]:
    21	    """
    22	    Analysiert die Eingabe, klassifiziert den Intent und extrahiert Parameter.
    23	    Schreibt Ergebnis deterministisch in state['triage'] und state['params'].
    24	    """
    25	    msgs = normalize_messages(state.get("messages", []))
    26	    params = dict(state.get("params") or {})
    27	
    28	    # Prompt rendern
    29	    prompt = render_template(
    30	        "intake_triage.jinja2",
    31	        messages=messages_for_template(msgs),
    32	        params=params,
    33	        params_json=json.dumps(params, ensure_ascii=False),
    34	    )
    35	
    36	    llm = create_llm()
    37	    try:
    38	        resp = llm.invoke([HumanMessage(content=prompt)])
    39	        raw = strip_json_fence(getattr(resp, "content", "") or "")
    40	        data = json.loads(raw)
    41	    except Exception as e:
    42	        log.warning("intake_node: Parse- oder LLM-Fehler: %s", e, exc_info=True)
    43	        data = {}
    44	
    45	    # Intent & Params sauber übernehmen
    46	    intent = str((data.get("intent") or "unknown")).strip().lower()
    47	    new_params = dict(params)
    48	    if isinstance(data.get("params"), dict):
    49	        for k, v in data["params"].items():
    50	            # nur nicht-leere Werte übernehmen
    51	            if v not in (None, "", "unknown"):
    52	                new_params[k] = v
    53	
    54	    triage = {
    55	        "intent": intent if intent in ("greeting", "smalltalk", "consult", "unknown") else "unknown",
    56	        "confidence": 1.0 if intent in ("greeting", "smalltalk", "consult") else 0.0,
    57	        "reply": "",
    58	        "flags": {"source": "intake_triage"},
    59	    }
    60	
    61	    # Keine AIMessage nötig – Routing übernimmt build._route_from_intake()
    62	    return {
    63	        "messages": [],
    64	        "params": new_params,
    65	        "triage": triage,
    66	        "phase": "intake",
    67	    }

################################################################################
# FILE: backend/app/services/langgraph/graph/consult/nodes/lite_router.py
################################################################################
     1	# backend/app/services/langgraph/graph/consult/nodes/lite_router.py
     2	from __future__ import annotations
     3	from typing import Any, Dict, List
     4	import logging
     5	
     6	log = logging.getLogger("uvicorn.error")
     7	
     8	
     9	def _last_user_text(messages: List[Dict[str, Any]]) -> str:
    10	    for m in reversed(messages or []):
    11	        role = (m.get("role") or m.get("type") or "").lower()
    12	        if role in ("user", "human") and isinstance(m.get("content"), str):
    13	            return m["content"].strip()
    14	    return ""
    15	
    16	
    17	def lite_router_node(state: Dict[str, Any]) -> Dict[str, Any]:
    18	    """
    19	    Sehr schneller Router:
    20	      - smalltalk=True: Grüße / Smalltalk
    21	      - info=True: Wissens-/Materialfragen (z.B. 'PTFE', 'was weißt du ...')
    22	    Setzt Flags in den State, ändert aber den Flow nicht – die nächste Node
    23	    entscheidet anhand der Flags (z.B. Smalltalk-Node).
    24	    """
    25	    messages = state.get("messages") or []
    26	    text = (_last_user_text(messages) or "").lower()
    27	
    28	    # Heuristiken
    29	    smalltalk_triggers = (
    30	        "hallo", "hi", "servus", "moin", "grüß", "gruss", "wie geht", "na?", "hey"
    31	    )
    32	    info_triggers = (
    33	        "was weißt du", "was weisst du", "what do you know",
    34	        "ptfe", "nbr", "fkm", "datenblatt", "werkstoff", "material", "rag:"
    35	    )
    36	
    37	    is_smalltalk = any(t in text for t in smalltalk_triggers)
    38	    is_info = any(t in text for t in info_triggers) and not is_smalltalk
    39	
    40	    log.info(f"[lite_router] smalltalk={is_smalltalk} info={is_info} text='{text}'")
    41	
    42	    new_state: Dict[str, Any] = dict(state)
    43	    new_state["lite_route"] = {"smalltalk": is_smalltalk, "info": is_info}
    44	    # Phase hier nicht setzen – das macht die nächste Node
    45	    return new_state

################################################################################
# FILE: backend/app/services/langgraph/graph/consult/nodes/rag.py
################################################################################
     1	# backend/app/services/langgraph/graph/consult/nodes/rag.py
     2	"""
     3	RAG-Node: holt Hybrid-Treffer (Qdrant + Redis BM25), baut kompakten
     4	Kontext-String und legt beides in den State (retrieved_docs/docs, context) ab.
     5	"""
     6	from __future__ import annotations
     7	from typing import Any, Dict, List, Optional
     8	import structlog
     9	
    10	from .....rag import rag_orchestrator as ro  # relativer Import
    11	
    12	log = structlog.get_logger(__name__)
    13	
    14	
    15	def _extract_query(state: Dict[str, Any]) -> str:
    16	    return (
    17	        state.get("query")
    18	        or state.get("question")
    19	        or state.get("user_input")
    20	        or state.get("input")
    21	        or ""
    22	    )
    23	
    24	
    25	def _extract_tenant(state: Dict[str, Any]) -> Optional[str]:
    26	    ctx = state.get("context") or {}
    27	    return state.get("tenant") or (ctx.get("tenant") if isinstance(ctx, dict) else None)
    28	
    29	
    30	def _context_from_docs(docs: List[Dict[str, Any]], max_chars: int = 1200) -> str:
    31	    """Kompakter Textkontext für Prompting (inkl. Quelle)."""
    32	    if not docs:
    33	        return ""
    34	    parts: List[str] = []
    35	    for d in docs[:6]:
    36	        t = (d.get("text") or "").strip()
    37	        if not t:
    38	            continue
    39	        src = d.get("source") or (d.get("metadata") or {}).get("source")
    40	        if src:
    41	            t = f"{t}\n[source: {src}]"
    42	        parts.append(t)
    43	    ctx = "\n\n".join(parts)
    44	    return ctx[:max_chars]
    45	
    46	
    47	def run_rag_node(state: Dict[str, Any]) -> Dict[str, Any]:
    48	    """
    49	    Eingänge (optional):
    50	      - query/question/user_input/input
    51	      - tenant bzw. context.tenant
    52	      - rag_filters, rag_k, rag_rerank
    53	    Ausgänge:
    54	      - retrieved_docs/docs: List[Dict[str, Any]]
    55	      - context: str
    56	    """
    57	    query = _extract_query(state)
    58	    tenant = _extract_tenant(state)
    59	    filters = state.get("rag_filters") or None
    60	    k = int(state.get("rag_k") or ro.FINAL_K)
    61	    use_rerank = bool(state.get("rag_rerank", True))
    62	
    63	    if not query.strip():
    64	        return {**state, "retrieved_docs": [], "docs": [], "context": "", "phase": "rag"}
    65	
    66	    docs = ro.hybrid_retrieve(
    67	        query=query,
    68	        tenant=tenant,
    69	        k=k,
    70	        metadata_filters=filters,
    71	        use_rerank=use_rerank,
    72	    )
    73	
    74	    context = state.get("context")
    75	    if not isinstance(context, str) or not context.strip():
    76	        context = _context_from_docs(docs)
    77	
    78	    out = {
    79	        **state,
    80	        "retrieved_docs": docs,
    81	        "docs": docs,              # Alias für nachfolgende Nodes
    82	        "context": context,
    83	        "phase": "rag",
    84	    }
    85	    try:
    86	        log.info("[rag_node] retrieved", n=len(docs), tenant=tenant or "-", ctx_len=len(context or ""))
    87	    except Exception:
    88	        pass
    89	    return out
    90	
    91	
    92	__all__ = ["run_rag_node"]

################################################################################
# FILE: backend/app/services/langgraph/graph/consult/nodes/recommend.py
################################################################################
     1	# backend/app/services/langgraph/graph/consult/nodes/recommend.py
     2	from __future__ import annotations
     3	
     4	import json
     5	import re
     6	from typing import Any, Dict, List, Optional
     7	
     8	import structlog
     9	from langchain_core.messages import AIMessage, SystemMessage
    10	from langchain_core.runnables.config import RunnableConfig
    11	
    12	from app.services.langgraph.llm_factory import get_llm as create_llm
    13	from app.services.langgraph.prompting import (
    14	    render_template,
    15	    messages_for_template,
    16	    strip_json_fence,
    17	)
    18	from app.services.langgraph.prompt_registry import get_agent_prompt
    19	from ..utils import normalize_messages, last_user_text
    20	
    21	log = structlog.get_logger(__name__)
    22	
    23	
    24	def _extract_text_from_chunk(chunk) -> List[str]:
    25	    out: List[str] = []
    26	    if not chunk:
    27	        return out
    28	    c = getattr(chunk, "content", None)
    29	    if isinstance(c, str) and c:
    30	        out.append(c)
    31	    elif isinstance(c, list):
    32	        for part in c:
    33	            if isinstance(part, str):
    34	                out.append(part)
    35	            elif isinstance(part, dict) and isinstance(part.get("text"), str):
    36	                out.append(part["text"])
    37	    ak = getattr(chunk, "additional_kwargs", None)
    38	    if isinstance(ak, dict):
    39	        for k in ("delta", "content", "text", "token"):
    40	            v = ak.get(k)
    41	            if isinstance(v, str) and v:
    42	                out.append(v)
    43	    if isinstance(chunk, dict):
    44	        for k in ("delta", "content", "text", "token"):
    45	            v = chunk.get(k)
    46	            if isinstance(v, str) and v:
    47	                out.append(v)
    48	    return out
    49	
    50	
    51	def _extract_json_any(s: str) -> str:
    52	    s = (s or "").strip()
    53	    if not s:
    54	        return ""
    55	    if (s[:1] in "{[") and (s[-1:] in "}]"):
    56	        return s
    57	    s2 = strip_json_fence(s)
    58	    if (s2[:1] in "{[") and (s2[-1:] in "}]"):
    59	        return s2
    60	    # balanced JSON heuristics
    61	    m = re.search(r"\{(?:[^{}]|(?R))*\}", s, re.S)
    62	    if m:
    63	        return m.group(0)
    64	    m = re.search(r"\[(?:[^\[\]]|(?R))*\]", s, re.S)
    65	    return m.group(0) if m else ""
    66	
    67	
    68	def _parse_empfehlungen(raw: str) -> Optional[List[Dict[str, Any]]]:
    69	    if not raw:
    70	        return None
    71	    try:
    72	        data = json.loads(strip_json_fence(raw))
    73	        if isinstance(data, dict) and isinstance(data.get("empfehlungen"), list):
    74	            return data["empfehlungen"]
    75	    except Exception as e:
    76	        log.warning("[recommend_node] json_parse_error", err=str(e))
    77	    return None
    78	
    79	
    80	# -------- Markdown → strukturierter Fallback --------
    81	_RX = {
    82	    "typ": re.compile(r"(?im)^\s*Typ:\s*(.+?)\s*$"),
    83	    "werkstoff": re.compile(r"(?im)^\s*Werkstoff:\s*(.+?)\s*$"),
    84	    "vorteile": re.compile(
    85	        r"(?is)\bVorteile:\s*(.+?)(?:\n\s*(?:Einschr[aä]nkungen|Begr[üu]ndung|Abgeleiteter|Alternativen)\b|$)"
    86	    ),
    87	    "einschraenkungen": re.compile(
    88	        r"(?is)\bEinschr[aä]nkungen:\s*(.+?)(?:\n\s*(?:Begr[üu]ndung|Abgeleiteter|Alternativen)\b|$)"
    89	    ),
    90	    "begruendung": re.compile(
    91	        r"(?is)\bBegr[üu]ndung:\s*(.+?)(?:\n\s*(?:Abgeleiteter|Alternativen)\b|$)"
    92	    ),
    93	}
    94	
    95	
    96	def _split_items(s: str) -> List[str]:
    97	    if not s:
    98	        return []
    99	    s = re.sub(r"[•\-\u2013\u2014]\s*", ", ", s)
   100	    parts = re.split(r"[;,]\s*|\s{2,}", s.strip())
   101	    return [p.strip(" .") for p in parts if p and not p.isspace()]
   102	
   103	
   104	def _coerce_from_markdown(text: str) -> Optional[List[Dict[str, Any]]]:
   105	    if not text:
   106	        return None
   107	
   108	    def _m(rx):
   109	        m = rx.search(text)
   110	        return (m.group(1).strip() if m else "")
   111	
   112	    typ = _m(_RX["typ"])
   113	    werkstoff = _m(_RX["werkstoff"])
   114	    vorteile = _split_items(_m(_RX["vorteile"]))
   115	    einschr = _split_items(_m(_RX["einschraenkungen"]))
   116	    begr = _m(_RX["begruendung"])
   117	    if not (typ or werkstoff or begr or vorteile or einschr):
   118	        return None
   119	    return [{
   120	        "typ": typ or "",
   121	        "werkstoff": werkstoff or "",
   122	        "begruendung": begr or "",
   123	        "vorteile": vorteile or [],
   124	        "einschraenkungen": einschr or [],
   125	        "geeignet_fuer": [],
   126	    }]
   127	
   128	
   129	def _context_from_docs(docs: List[Dict[str, Any]], max_chars: int = 1200) -> str:
   130	    if not docs:
   131	        return ""
   132	    parts: List[str] = []
   133	    for d in docs[:6]:
   134	        t = (d.get("text") or "").strip()
   135	        if not t:
   136	            continue
   137	        src = d.get("source") or (d.get("metadata") or {}).get("source")
   138	        if src:
   139	            t = f"{t}\n[source: {src}]"
   140	        parts.append(t)
   141	    ctx = "\n\n".join(parts)
   142	    return ctx[:max_chars]
   143	
   144	
   145	def recommend_node(state: Dict[str, Any], config: Optional[RunnableConfig] = None) -> Dict[str, Any]:
   146	    msgs = normalize_messages(state.get("messages", []))
   147	    params: Dict[str, Any] = state.get("params") or {}
   148	    domain = (state.get("domain") or "").strip().lower()
   149	    derived = state.get("derived") or {}
   150	    retrieved_docs: List[Dict[str, Any]] = state.get("retrieved_docs") or []
   151	
   152	    context = state.get("context") or _context_from_docs(retrieved_docs)
   153	    if context:
   154	        log.info("[recommend_node] using_context", n_docs=len(retrieved_docs), ctx_len=len(context))
   155	
   156	    base_llm = create_llm()
   157	    try:
   158	        llm = base_llm.bind(response_format={"type": "json_object"})
   159	    except Exception:
   160	        llm = base_llm
   161	
   162	    recent_user = (last_user_text(msgs) or "").strip()
   163	
   164	    prompt = render_template(
   165	        "recommend.jinja2",
   166	        messages=messages_for_template(msgs),
   167	        params=params,
   168	        domain=domain,
   169	        derived=derived,
   170	        recent_user=recent_user,
   171	        context=context,
   172	    )
   173	
   174	    effective_cfg: RunnableConfig = (config or {}).copy()  # type: ignore[assignment]
   175	    if "run_name" not in (effective_cfg or {}):
   176	        effective_cfg = {**effective_cfg, "run_name": "recommend"}  # type: ignore[dict-item]
   177	
   178	    # Domänen-spezifischen Systemprompt + JSON-Instruktion kombinieren
   179	    sys_msgs = [
   180	        SystemMessage(content=get_agent_prompt(domain or "rwdr")),
   181	        SystemMessage(content=prompt),
   182	    ]
   183	
   184	    content_parts: List[str] = []
   185	    try:
   186	        for chunk in llm.with_config(effective_cfg).stream(sys_msgs):
   187	            content_parts.extend(_extract_text_from_chunk(chunk))
   188	    except Exception as e:
   189	        log.warning("[recommend_node] stream_failed", err=str(e))
   190	        try:
   191	            resp = llm.invoke(sys_msgs, config=effective_cfg)
   192	            content_parts = [getattr(resp, "content", "") or ""]
   193	        except Exception as e2:
   194	            log.error("[recommend_node] invoke_failed", err=str(e2))
   195	            payload = json.dumps({"empfehlungen": []}, ensure_ascii=False, separators=(",", ":"))
   196	            ai_msg = AIMessage(content=payload)
   197	            return {
   198	                **state,
   199	                "messages": msgs + [ai_msg],
   200	                "answer": payload,
   201	                "phase": "recommend",
   202	                "empfehlungen": [],
   203	                "retrieved_docs": retrieved_docs,
   204	                "docs": retrieved_docs,
   205	                "context": context,
   206	            }
   207	
   208	    raw = ("".join(content_parts) or "").strip()
   209	    log.info("[recommend_node] stream_len", chars=len(raw))
   210	
   211	    # 1) echtes JSON?
   212	    json_snippet = _extract_json_any(raw)
   213	    recs = _parse_empfehlungen(json_snippet) or _parse_empfehlungen(raw)
   214	
   215	    # 2) Fallback: Markdown → strukturieren
   216	    if not recs:
   217	        recs = _coerce_from_markdown(raw)
   218	
   219	    # 3) Letzter Fallback
   220	    if not recs:
   221	        recs = [{
   222	            "typ": "",
   223	            "werkstoff": "",
   224	            "begruendung": (raw[:600] if raw else "Keine strukturierte Empfehlung erhalten."),
   225	            "vorteile": [],
   226	            "einschraenkungen": [],
   227	            "geeignet_fuer": [],
   228	        }]
   229	
   230	    content_out = json.dumps({"empfehlungen": recs}, ensure_ascii=False, separators=(",", ":"))
   231	    content_out = content_out.replace("\n", " ").strip()  # sicher einzeilig
   232	
   233	    log.info("[recommend_node] emitting_json", length=len(content_out))
   234	
   235	    ai_msg = AIMessage(content=content_out)
   236	    return {
   237	        **state,
   238	        "messages": msgs + [ai_msg],
   239	        "answer": content_out,
   240	        "phase": "recommend",
   241	        "empfehlungen": recs,
   242	        "retrieved_docs": retrieved_docs,
   243	        "docs": retrieved_docs,
   244	        "context": context,
   245	    }

################################################################################
# FILE: backend/app/services/langgraph/graph/consult/nodes/smalltalk.py
################################################################################
     1	# backend/app/services/langgraph/graph/consult/nodes/smalltalk.py
     2	from __future__ import annotations
     3	from typing import Any, Dict, List
     4	import logging
     5	
     6	# Verwende die bestehende LLM-Factory in diesem Consult-Paket
     7	from ..config import create_llm
     8	
     9	log = logging.getLogger("uvicorn.error")
    10	
    11	SYSTEM = (
    12	    "Du bist ein freundlicher Assistent. Antworte kurz, natürlich und auf Deutsch. "
    13	    "Bei Smalltalk: keine Fachberatung und keine Rückfragen zu Parametern."
    14	)
    15	
    16	
    17	def _last_user_text(messages: List[Dict[str, Any]]) -> str:
    18	    for m in reversed(messages or []):
    19	        role = (m.get("role") or m.get("type") or "").lower()
    20	        if role in ("user", "human") and isinstance(m.get("content"), str):
    21	            return m["content"].strip()
    22	    return ""
    23	
    24	
    25	def smalltalk_node(state: Dict[str, Any]) -> Dict[str, Any]:
    26	    """
    27	    Smalltalk über das Standard-Chatmodell laufen lassen.
    28	    Wenn Streaming-Callbacks konfiguriert sind, wird gestreamt; sonst fällt es
    29	    sauber auf eine kurze, synchron erzeugte Antwort zurück.
    30	    """
    31	    messages = state.get("messages") or []
    32	    user = _last_user_text(messages) or "Kleiner Smalltalk."
    33	
    34	    try:
    35	        # Das create_llm kommt aus consult/config.py und ist in deinem Projekt vorhanden.
    36	        # Viele Setups akzeptieren stream=True (wird intern vom WS-Handler genutzt).
    37	        llm = create_llm(stream=True)
    38	        prompt = [
    39	            {"role": "system", "content": SYSTEM},
    40	            {"role": "user", "content": user},
    41	        ]
    42	        completion = llm.invoke(prompt)
    43	        text = getattr(completion, "content", None) or ""
    44	        if not text:
    45	            text = "Hallo! 😊 Wie kann ich dir helfen?"
    46	    except Exception as e:
    47	        log.warning(f"[smalltalk] fallback without LLM due to: {e}")
    48	        # Minimaler, robuster Fallback ohne Modell
    49	        greetings = ("hallo", "hi", "servus", "moin", "grüß", "gruss")
    50	        low = user.lower()
    51	        if any(g in low for g in greetings):
    52	            text = "Hallo! 👋 Wie kann ich dir helfen?"
    53	        elif "wie geht" in low:
    54	            text = "Mir geht’s gut, danke! 😊 Und dir?"
    55	        else:
    56	            text = "Klingt gut! Wie kann ich dich unterstützen?"
    57	
    58	    new_messages = list(messages) + [{"role": "assistant", "content": text}]
    59	    new_state = dict(state)
    60	    new_state["messages"] = new_messages
    61	    new_state["phase"] = "respond"
    62	    return new_state

################################################################################
# FILE: backend/app/services/langgraph/graph/consult/nodes/validate_answer.py
################################################################################
     1	# backend/app/services/langgraph/graph/consult/nodes/validate_answer.py
     2	from __future__ import annotations
     3	
     4	import math
     5	from typing import Any, Dict, List
     6	import structlog
     7	
     8	from ..state import ConsultState
     9	
    10	log = structlog.get_logger(__name__)
    11	
    12	
    13	def _sigmoid(x: float) -> float:
    14	    try:
    15	        return 1.0 / (1.0 + math.exp(-x))
    16	    except Exception:
    17	        return 0.5
    18	
    19	
    20	def _confidence_from_docs(docs: List[Dict[str, Any]]) -> float:
    21	    """
    22	    Grobe Konfidenzabschätzung aus RAG-Scores.
    23	    Nutzt fused_score, sonst max(vector_score, keyword_score/100).
    24	    """
    25	    if not docs:
    26	        return 0.15
    27	
    28	    vals: List[float] = []
    29	    for d in docs[:6]:
    30	        vs = d.get("vector_score")
    31	        ks = d.get("keyword_score")
    32	        fs = d.get("fused_score")
    33	        try:
    34	            base = float(fs if fs is not None else max(float(vs or 0.0), float(ks or 0.0) / 100.0))
    35	        except Exception:
    36	            base = 0.0
    37	        vals.append(_sigmoid(base))
    38	
    39	    conf = sum(vals) / max(1, len(vals))
    40	    return max(0.05, min(0.98, conf))
    41	
    42	
    43	def _top_source(d: Dict[str, Any]) -> str:
    44	    return (d.get("source")
    45	            or (d.get("metadata") or {}).get("source")
    46	            or "")
    47	
    48	
    49	def validate_answer(state: ConsultState) -> ConsultState:
    50	    """
    51	    Bewertet die Antwortqualität (Konfidenz/Quellen) und MERGT den State,
    52	    ohne RAG-Felder zu verlieren.
    53	    """
    54	    retrieved_docs: List[Dict[str, Any]] = state.get("retrieved_docs") or state.get("docs") or []
    55	    context: str = state.get("context") or ""
    56	
    57	    conf = _confidence_from_docs(retrieved_docs)
    58	    needs_more = bool(state.get("needs_more_params")) or conf < 0.35
    59	
    60	    validation: Dict[str, Any] = {
    61	        "n_docs": len(retrieved_docs),
    62	        "confidence": round(conf, 3),
    63	        "top_source": _top_source(retrieved_docs[0]) if retrieved_docs else "",
    64	    }
    65	
    66	    log.info(
    67	        "validate_answer",
    68	        confidence=validation["confidence"],
    69	        needs_more_params=needs_more,
    70	        n_docs=validation["n_docs"],
    71	        top_source=validation["top_source"],
    72	    )
    73	
    74	    return {
    75	        **state,
    76	        "phase": "validate_answer",
    77	        "validation": validation,
    78	        "confidence": conf,
    79	        "needs_more_params": needs_more,
    80	        # explizit erhalten
    81	        "retrieved_docs": retrieved_docs,
    82	        "docs": retrieved_docs,
    83	        "context": context,
    84	    }

################################################################################
# FILE: backend/app/services/langgraph/graph/consult/nodes/validate.py
################################################################################
     1	# backend/app/services/langgraph/graph/consult/nodes/validate.py
     2	from __future__ import annotations
     3	from typing import Any, Dict
     4	
     5	
     6	def _to_float(x: Any) -> Any:
     7	    try:
     8	        if isinstance(x, bool):
     9	            return x
    10	        return float(x)
    11	    except Exception:
    12	        return x
    13	
    14	
    15	def validate_node(state: Dict[str, Any]) -> Dict[str, Any]:
    16	    """
    17	    Leichter Parameter-Check/Normalisierung vor RAG.
    18	    WICHTIG: Keine Berechnungen, kein Calculator-Aufruf – das macht calc_agent.
    19	    """
    20	    params = dict(state.get("params") or {})
    21	
    22	    # numerische Felder best-effort in float wandeln
    23	    for k in (
    24	        "temp_max_c", "temp_min_c", "druck_bar", "drehzahl_u_min",
    25	        "wellen_mm", "gehause_mm", "breite_mm",
    26	        "relativgeschwindigkeit_ms",
    27	        "tmax_c", "pressure_bar", "n_u_min", "rpm", "v_ms",
    28	    ):
    29	        if k in params and params[k] not in (None, "", []):
    30	            params[k] = _to_float(params[k])
    31	
    32	    # einfache Alias-Harmonisierung (falls Ziel noch leer)
    33	    alias = {
    34	        "tmax_c": "temp_max_c",
    35	        "pressure_bar": "druck_bar",
    36	        "n_u_min": "drehzahl_u_min",
    37	        "rpm": "drehzahl_u_min",
    38	        "v_ms": "relativgeschwindigkeit_ms",
    39	    }
    40	    for src, dst in alias.items():
    41	        if (params.get(dst) in (None, "", [])) and (params.get(src) not in (None, "", [])):
    42	            params[dst] = params[src]
    43	
    44	    return {**state, "params": params, "phase": "validate"}

################################################################################
# FILE: backend/app/services/langgraph/graph/consult/state.py
################################################################################
     1	# backend/app/services/langgraph/graph/consult/state.py
     2	from __future__ import annotations
     3	
     4	from typing import Any, Dict, List, Optional, TypedDict
     5	from typing_extensions import Annotated
     6	from langchain_core.messages import AnyMessage
     7	from langgraph.graph import add_messages
     8	
     9	
    10	# ---- Parameter- & Derived-Typen -------------------------------------------------
    11	class Parameters(TypedDict, total=False):
    12	    # Kernparameter
    13	    temp_max_c: float
    14	    druck_bar: float
    15	    drehzahl_u_min: float
    16	    wellen_mm: float
    17	    relativgeschwindigkeit_ms: float
    18	    # Aliasse / Harmonisierung
    19	    tmax_c: float
    20	    pressure_bar: float
    21	    n_u_min: float
    22	    rpm: float
    23	    v_ms: float
    24	    # optionale Filter/Routing
    25	    material: str
    26	    profile: str
    27	    domain: str
    28	    norm: str
    29	    lang: str
    30	
    31	
    32	class Derived(TypedDict, total=False):
    33	    relativgeschwindigkeit_ms: float
    34	    # Platzhalter für weitere abgeleitete Größen
    35	    # z. B. reibleistung_w: float
    36	
    37	
    38	# ---- Graph-State ----------------------------------------------------------------
    39	class ConsultState(TypedDict, total=False):
    40	    # Dialog
    41	    messages: Annotated[List[AnyMessage], add_messages]
    42	    query: str
    43	
    44	    # Parameter
    45	    params: Parameters
    46	    derived: Derived
    47	
    48	    # Routing / Kontext
    49	    user_id: Optional[str]
    50	    tenant: Optional[str]
    51	    domain: Optional[str]
    52	    phase: Optional[str]
    53	    consult_required: Optional[bool]   # <-- aufgenommen, da im Flow genutzt
    54	
    55	    # ---- UI/Frontend-Integration (muss explizit im State sein!) -------------------
    56	    ui_event: Dict[str, Any]           # <-- NEW: Formular-/Sidebar-Trigger
    57	    missing_fields: List[str]          # <-- NEW: für Frontend-Anzeige
    58	
    59	    # --- RAG-Ergebnis (muss hier definiert sein, sonst wird es vom Graph gedroppt!) ---
    60	    retrieved_docs: List[Dict[str, Any]]   # Hybrid-Treffer inkl. Scores/Quellen
    61	    context: str                            # kompakter Prompt-Kontext aus Treffern
    62	
    63	    # Empfehlungen / Ergebnis
    64	    empfehlungen: List[Dict[str, Any]]
    65	
    66	    # Qualitäts-/Validierungsinfos
    67	    validation: Dict[str, Any]
    68	    confidence: float
    69	    needs_more_params: bool
    70	
    71	    # --- Legacy-Felder (Kompatibilität; werden vom aktuellen Flow nicht aktiv gesetzt) ---
    72	    docs: List[Dict[str, Any]]
    73	    citations: List[str]
    74	    answer: Optional[str]

################################################################################
# FILE: backend/app/services/langgraph/graph/consult/utils.py
################################################################################
     1	# backend/app/services/langgraph/graph/consult/utils.py
     2	from __future__ import annotations
     3	
     4	import logging
     5	import re
     6	from typing import Any, Dict, Iterable, List, Optional
     7	
     8	from langgraph.graph.message import add_messages
     9	from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, SystemMessage
    10	
    11	log = logging.getLogger(__name__)
    12	
    13	# -------------------------------------------------------------------
    14	# Message utilities
    15	# -------------------------------------------------------------------
    16	
    17	def deserialize_message(x: Any) -> AnyMessage:
    18	    """Robuste Konvertierung nach LangChain-Message-Objekten."""
    19	    if isinstance(x, (HumanMessage, AIMessage, SystemMessage)):
    20	        return x
    21	    if isinstance(x, dict) and "role" in x:
    22	        role = (x.get("role") or "").lower()
    23	        content = x.get("content") or ""
    24	        if role in ("user", "human"):
    25	            return HumanMessage(content=content)
    26	        if role in ("assistant", "ai"):
    27	            return AIMessage(content=content)
    28	        if role == "system":
    29	            return SystemMessage(content=content)
    30	    if isinstance(x, str):
    31	        return HumanMessage(content=x)
    32	    return HumanMessage(content=str(x))
    33	
    34	
    35	def normalize_messages(seq: Iterable[Any]) -> List[AnyMessage]:
    36	    return [deserialize_message(m) for m in (seq or [])]
    37	
    38	
    39	def merge_messages(left: Iterable[Any], right: Iterable[Any]) -> List[AnyMessage]:
    40	    return add_messages(normalize_messages(left), normalize_messages(right))
    41	
    42	
    43	def last_user_text(msgs: List[AnyMessage]) -> str:
    44	    for m in reversed(msgs or []):
    45	        if isinstance(m, HumanMessage):
    46	            return (m.content or "").strip()
    47	    return ""
    48	
    49	
    50	def messages_text(msgs: List[AnyMessage], *, only_user: bool = False) -> str:
    51	    """
    52	    Verkettet Text aller Messages.
    53	    - only_user=True -> nur HumanMessage.
    54	    """
    55	    parts: List[str] = []
    56	    for m in msgs or []:
    57	        if only_user and not isinstance(m, HumanMessage):
    58	            continue
    59	        c = getattr(m, "content", None)
    60	        if isinstance(c, str) and c:
    61	            parts.append(c)
    62	    return "\n".join(parts)
    63	
    64	
    65	# Kompatibilitäts-Alias (einige Module importieren 'msgs_text')
    66	msgs_text = messages_text
    67	
    68	
    69	def only_user_text(msgs: List[AnyMessage]) -> str:
    70	    """Nur die User-Texte zusammengefasst (ohne Lowercasing)."""
    71	    return messages_text(msgs, only_user=True)
    72	
    73	
    74	def only_user_text_lower(msgs: List[AnyMessage]) -> str:
    75	    """Nur die User-Texte, zu Kleinbuchstaben normalisiert."""
    76	    return only_user_text(msgs).lower()
    77	
    78	
    79	# -------------------------------------------------------------------
    80	# Numeric parsing & heuristics
    81	# -------------------------------------------------------------------
    82	
    83	def _num_from_str(raw: str) -> Optional[float]:
    84	    """Float aus Strings wie '1 200,5' oder '1.200,5' oder '1200.5' extrahieren."""
    85	    try:
    86	        s = (raw or "").replace(" ", "").replace(".", "").replace(",", ".")
    87	        return float(s)
    88	    except Exception:
    89	        return None
    90	
    91	
    92	def apply_heuristics_from_text(params: Dict[str, Any], text: str) -> Dict[str, Any]:
    93	    """
    94	    Deterministische Fallbacks, falls das LLM Werte nicht gesetzt hat:
    95	      - 'kein/ohne Überdruck/Druck' -> druck_bar = 0
    96	      - '... Druck: 5 bar'          -> druck_bar = 5
    97	      - 'Drehzahl 1.200 U/min'      -> drehzahl_u_min = 1200
    98	      - 'dauerhaft X U/min'         -> drehzahl_u_min = X
    99	      - 'Geschwindigkeit 0.5 m/s'   -> geschwindigkeit_m_s = 0.5
   100	    """
   101	    t = (text or "").lower()
   102	    merged: Dict[str, Any] = dict(params or {})
   103	
   104	    # Druck
   105	    if merged.get("druck_bar") in (None, "", "unknown"):
   106	        if re.search(r"\b(kein|ohne)\s+(überdruck|ueberdruck|druck)\b", t, re.I):
   107	            merged["druck_bar"] = 0.0
   108	        else:
   109	            m = re.search(r"(?:überdruck|ueberdruck|druck)\s*[:=]?\s*([0-9][\d\.\s,]*)\s*bar\b", t, re.I)
   110	            if m:
   111	                val = _num_from_str(m.group(1))
   112	                if val is not None:
   113	                    merged["druck_bar"] = val
   114	
   115	    # Drehzahl (generisch)
   116	    if merged.get("drehzahl_u_min") in (None, "", "unknown"):
   117	        m = re.search(r"drehzahl[^0-9]{0,12}([0-9][\d\.\s,]*)\s*(?:u\s*/?\s*min|rpm)\b", t, re.I)
   118	        if m:
   119	            val = _num_from_str(m.group(1))
   120	            if val is not None:
   121	                merged["drehzahl_u_min"] = int(round(val))
   122	
   123	    # Spezifisch „dauerhaft“
   124	    m_dauer = re.search(
   125	        r"(dauerhaft|kontinuierlich)[^0-9]{0,12}([0-9][\d\.\s,]*)\s*(?:u\s*/?\s*min|rpm)\b",
   126	        t,
   127	        re.I,
   128	    )
   129	    if m_dauer:
   130	        val = _num_from_str(m_dauer.group(2))
   131	        if val is not None:
   132	            merged["drehzahl_u_min"] = int(round(val))
   133	
   134	    # Relativgeschwindigkeit in m/s
   135	    if merged.get("geschwindigkeit_m_s") in (None, "", "unknown"):
   136	        m_speed = re.search(r"(geschwindigkeit|v)[^0-9]{0,12}([0-9][\d\.\s,]*)\s*m\s*/\s*s", t, re.I)
   137	        if m_speed:
   138	            val = _num_from_str(m_speed.group(2))
   139	            if val is not None:
   140	                merged["geschwindigkeit_m_s"] = float(val)
   141	
   142	    return merged
   143	
   144	
   145	# -------------------------------------------------------------------
   146	# Validation & anomaly messages
   147	# -------------------------------------------------------------------
   148	
   149	def _is_missing_value(key: str, val: Any) -> bool:
   150	    if val is None or val == "" or val == "unknown":
   151	        return True
   152	    # 0 bar ist gültig
   153	    if key == "druck_bar":
   154	        try:
   155	            float(val)
   156	            return False
   157	        except Exception:
   158	            return True
   159	    # Positive Größen brauchen > 0
   160	    if key in (
   161	        "wellen_mm", "gehause_mm", "breite_mm", "drehzahl_u_min", "geschwindigkeit_m_s",
   162	        "stange_mm", "nut_d_mm", "nut_b_mm"
   163	    ):
   164	        try:
   165	            return float(val) <= 0
   166	        except Exception:
   167	            return True
   168	    # temp_max_c: nur presence check
   169	    if key == "temp_max_c":
   170	        try:
   171	            float(val)
   172	            return False
   173	        except Exception:
   174	            return True
   175	    return False
   176	
   177	
   178	def _required_fields_by_domain(domain: str) -> List[str]:
   179	    # Hydraulik-Stange nutzt stange_mm / nut_d_mm / nut_b_mm
   180	    if (domain or "rwdr") == "hydraulics_rod":
   181	        return [
   182	            "falltyp",
   183	            "stange_mm",
   184	            "nut_d_mm",
   185	            "nut_b_mm",
   186	            "medium",
   187	            "temp_max_c",
   188	            "druck_bar",
   189	            "geschwindigkeit_m_s",
   190	        ]
   191	    # default: rwdr
   192	    return [
   193	        "falltyp",
   194	        "wellen_mm",
   195	        "gehause_mm",
   196	        "breite_mm",
   197	        "medium",
   198	        "temp_max_c",
   199	        "druck_bar",
   200	        "drehzahl_u_min",
   201	    ]
   202	
   203	
   204	def _missing_by_domain(domain: str, params: Dict[str, Any]) -> List[str]:
   205	    req = _required_fields_by_domain(domain or "rwdr")
   206	    return [k for k in req if _is_missing_value(k, (params or {}).get(k))]
   207	
   208	
   209	# Öffentlicher Alias (manche Module importieren ohne Unterstrich)
   210	missing_by_domain = _missing_by_domain
   211	
   212	
   213	def _anomaly_messages(domain: str, params: Dict[str, Any], derived: Dict[str, Any]) -> List[str]:
   214	    """
   215	    Erzeugt Rückfragen basierend auf abgeleiteten Flags (domainabhängig).
   216	    Erwartet 'derived' z. B.: {"flags": {...}, "warnings": [...], "requirements": [...]}
   217	    """
   218	    msgs: List[str] = []
   219	    flags = (derived.get("flags") or {})
   220	
   221	    # RWDR – Druckstufenfreigabe
   222	    if flags.get("requires_pressure_stage") and not flags.get("pressure_stage_ack"):
   223	        msgs.append(
   224	            "Ein Überdruck >2 bar ist für Standard-Radialdichtringe kritisch. "
   225	            "Dürfen Druckstufenlösungen geprüft werden?"
   226	        )
   227	
   228	    # Hohe Drehzahl/Geschwindigkeit
   229	    if flags.get("speed_high"):
   230	        msgs.append("Die Drehzahl/Umfangsgeschwindigkeit ist hoch – ist sie dauerhaft oder nur kurzzeitig (Spitzen)?")
   231	
   232	    # Sehr hohe Temperatur
   233	    if flags.get("temp_very_high"):
   234	        msgs.append("Die Temperatur ist sehr hoch. Handelt es sich um Dauer- oder Spitzentemperaturen?")
   235	
   236	    # Hydraulik Stange – Extrusions-/Back-up-Ring-Freigabe
   237	    if (domain or "") == "hydraulics_rod" and flags.get("extrusion_risk") and not flags.get("extrusion_risk_ack"):
   238	        msgs.append("Bei dem Druck besteht Extrusionsrisiko. Darf eine Stütz-/Back-up-Ring-Lösung geprüft werden?")
   239	
   240	    return msgs
   241	
   242	
   243	# Öffentlicher Alias
   244	anomaly_messages = _anomaly_messages
   245	
   246	# --- Output-Cleaner: Leading-Meta entfernen, Suffix-Echos entfernen, De-Dupe --
   247	
   248	def _strip(s: str) -> str:
   249	    return (s or "").strip()
   250	
   251	def _normalize_newlines(text: str) -> str:
   252	    """Normalisiert Zeilenenden und trimmt überflüssige Leerzeichen am Zeilenende."""
   253	    if not isinstance(text, str):
   254	        return text
   255	    t = re.sub(r"\r\n?|\r", "\n", text)
   256	    t = "\n".join(line.rstrip() for line in t.split("\n"))
   257	    return t
   258	
   259	def strip_leading_meta_blocks(text: str) -> str:
   260	    """
   261	    Entfernt am *Anfang* der Antwort Meta-Blöcke wie:
   262	      - führende JSON-/YAML-Objekte
   263	      - ```…``` fenced code blocks
   264	      - '# QA-Notiz …' bis zur nächsten Leerzeile
   265	    Wir iterieren, bis kein solcher Block mehr vorne steht.
   266	    """
   267	    if not isinstance(text, str) or not text.strip():
   268	        return text
   269	    t = text.lstrip()
   270	
   271	    changed = True
   272	    # max. 5 Durchläufe als Sicherung
   273	    for _ in range(5):
   274	        if not changed:
   275	            break
   276	        changed = False
   277	
   278	        # Fenced code block (beliebiges fence, inkl. json/yaml)
   279	        m = re.match(r"^\s*```[\s\S]*?```\s*", t)
   280	        if m:
   281	            t = t[m.end():].lstrip()
   282	            changed = True
   283	            continue
   284	
   285	        # Führendes JSON-/YAML-Objekt (heuristisch, nicht perfekt balanciert)
   286	        m = re.match(r"^\s*\{[\s\S]*?\}\s*(?=\n|$)", t)
   287	        if m:
   288	            t = t[m.end():].lstrip()
   289	            changed = True
   290	            continue
   291	        m = re.match(r"^\s*---[\s\S]*?---\s*(?=\n|$)", t)  # YAML frontmatter
   292	        if m:
   293	            t = t[m.end():].lstrip()
   294	            changed = True
   295	            continue
   296	
   297	        # QA-Notiz-Block bis zur nächsten Leerzeile
   298	        m = re.match(r"^\s*#\s*QA-Notiz[^\n]*\n[\s\S]*?(?:\n\s*\n|$)", t, flags=re.IGNORECASE)
   299	        if m:
   300	            t = t[m.end():].lstrip()
   301	            changed = True
   302	            continue
   303	
   304	    return t
   305	
   306	def clean_ai_output(ai_text: str, recent_user_texts: List[str]) -> str:
   307	    """
   308	    Entfernt angehängte Echos zuletzt gesagter User-Texte am Ende der AI-Ausgabe.
   309	    - vergleicht trim-normalisiert (Suffix)
   310	    - entfernt ganze trailing Blöcke, falls sie exakt einem der recent_user_texts entsprechen
   311	    """
   312	    if not isinstance(ai_text, str) or not ai_text:
   313	        return ai_text
   314	
   315	    out = ai_text.rstrip()
   316	
   317	    # Prüfe Kandidaten in abnehmender Länge (stabil gegen Teilmengen)
   318	    for u in sorted(set(recent_user_texts or []), key=len, reverse=True):
   319	        u_s = _strip(u)
   320	        if not u_s:
   321	            continue
   322	
   323	        # Work on a normalized working copy for suffix check
   324	        norm_out = _strip(out)
   325	        if norm_out.endswith(u_s):
   326	            # schneide die letzte (nicht-normalisierte) Vorkommen-Stelle am Ende ab
   327	            raw_idx = out.rstrip().rfind(u_s)
   328	            if raw_idx != -1:
   329	                out = out[:raw_idx].rstrip()
   330	
   331	    return out
   332	
   333	def _norm_key(block: str) -> str:
   334	    """Normierungs-Schlüssel für Block-Vergleich (whitespace-/case-insensitiv)."""
   335	    return re.sub(r"\s+", " ", (block or "").strip()).lower()
   336	
   337	def dedupe_text_blocks(text: str) -> str:
   338	    """
   339	    Entfernt doppelte inhaltlich identische Absätze/Blöcke, robust gegen CRLF
   340	    und gemischte Leerzeilen. Als Absatztrenner gilt: ≥1 (auch nur whitespace-) Leerzeile.
   341	    Zusätzlich werden identische, aufeinanderfolgende Einzelzeilen entfernt.
   342	    """
   343	    if not isinstance(text, str) or not text.strip():
   344	        return text
   345	
   346	    t = _normalize_newlines(text)
   347	
   348	    # Absätze anhand *mindestens* einer Leerzeile trennen (auch wenn nur Whitespace in der Leerzeile steht)
   349	    parts = [p.strip() for p in re.split(r"\n\s*\n+", t.strip()) if p.strip()]
   350	
   351	    seen = set()
   352	    out_blocks = []
   353	    for p in parts:
   354	        k = _norm_key(p)
   355	        if k in seen:
   356	            continue
   357	        seen.add(k)
   358	        out_blocks.append(p)
   359	
   360	    # Zusammensetzen mit Leerzeile zwischen Absätzen
   361	    merged = "\n\n".join(out_blocks)
   362	
   363	    # Zusätzlicher Schutz: identische direkt aufeinanderfolgende Zeilen entfernen
   364	    final_lines = []
   365	    prev_key = None
   366	    for line in merged.split("\n"):
   367	        key = _norm_key(line)
   368	        if key and key == prev_key:
   369	            continue
   370	        final_lines.append(line)
   371	        prev_key = key
   372	
   373	    return "\n".join(final_lines)
   374	
   375	def clean_and_dedupe(ai_text: str, recent_user_texts: List[str]) -> str:
   376	    """
   377	    Reihenfolge:
   378	      1) Führende Meta-Blöcke entfernen
   379	      2) Trailing User-Echos abschneiden
   380	      3) Identische Absätze/Zeilen de-dupen
   381	    """
   382	    head_clean = strip_leading_meta_blocks(ai_text)
   383	    tail_clean = clean_ai_output(head_clean, recent_user_texts)
   384	    return dedupe_text_blocks(tail_clean)

################################################################################
# FILE: backend/app/services/langgraph/graph/__init__.py
################################################################################

################################################################################
# FILE: backend/app/services/langgraph/graph/intent_router.py
################################################################################
     1	# backend/app/services/langgraph/graph/intent_router.py
     2	from __future__ import annotations
     3	
     4	import json
     5	import os
     6	import re
     7	from typing import Any, Literal, Sequence
     8	
     9	from ..prompting import render_template
    10	from app.services.langgraph.llm_router import get_router_llm, get_router_fallback_llm
    11	from langchain_core.messages import HumanMessage
    12	
    13	_TEMPLATE_FILE = "intent_router.jinja2"
    14	
    15	# Fast-Path Regex für Selektionsfälle (Material/Typ/Bauform/RWDR/Hydraulik & Maße)
    16	_FAST_SELECT_RX = re.compile(
    17	    r"(?i)\b(rwdr|wellendichtring|bauform\s*[a-z0-9]{1,4}|hydraulik|stangendichtung|kolbenstange|nut\s*[db]|"
    18	    r"\d{1,3}\s*[x×/]\s*\d{1,3}\s*[x×/\-]?\s*\d{1,3}|material\s*(wahl|auswahl|empfehlung)|"
    19	    r"(ptfe|nbr|hnbr|fk[mh]|epdm)\b)"
    20	)
    21	
    22	def _last_user_text(messages: Sequence[Any]) -> str:
    23	    if not messages:
    24	        return ""
    25	    for m in reversed(messages):
    26	        content = getattr(m, "content", None)
    27	        if content:
    28	            return str(content)
    29	        if isinstance(m, dict):
    30	            c = m.get("content") or m.get("text") or m.get("message")
    31	            if c:
    32	                return str(c)
    33	    return ""
    34	
    35	def _strip_json_fence(text: str) -> str:
    36	    if not text:
    37	        return ""
    38	    m = re.search(r"```(?:json)?\s*(.*?)\s*```", text, flags=re.DOTALL | re.IGNORECASE)
    39	    if m:
    40	        return m.group(1).strip()
    41	    return text.strip().strip("`").strip()
    42	
    43	def _conf_min() -> float:
    44	    try:
    45	        return float(os.getenv("INTENT_CONF_MIN", "0.60"))
    46	    except Exception:
    47	        return 0.60
    48	
    49	def classify_intent(_: Any, messages: Sequence[Any]) -> Literal["material_select", "llm"]:
    50	    """
    51	    LLM-first Routing:
    52	      - "material_select": Graph (Selektions-Workflow) verwenden
    53	      - "llm": Direkter LLM-Stream (Erklärung/Smalltalk/Wissen)
    54	    """
    55	    user_text = _last_user_text(messages).strip()
    56	
    57	    # 1) Fast-Path (regex)
    58	    if _FAST_SELECT_RX.search(user_text):
    59	        return "material_select"
    60	
    61	    # 2) Router-LLMs
    62	    router = get_router_llm()
    63	    fallback = get_router_fallback_llm()
    64	    prompt = render_template(_TEMPLATE_FILE, input_text=user_text)
    65	
    66	    def _ask(llm) -> tuple[str, float]:
    67	        resp = llm.invoke([HumanMessage(content=prompt)])
    68	        content = getattr(resp, "content", None) or str(resp)
    69	        raw = _strip_json_fence(content)
    70	        data = json.loads(raw)
    71	        intent = str(data.get("intent") or "").strip().lower()
    72	        conf = float(data.get("confidence") or 0.0)
    73	        return intent, conf
    74	
    75	    try:
    76	        intent, conf = _ask(router)
    77	    except Exception:
    78	        try:
    79	            intent, conf = _ask(fallback)
    80	        except Exception:
    81	            # fail-open zu material_select, damit echte Selektionsfälle nie „untergehen“
    82	            return "material_select"
    83	
    84	    if conf < _conf_min():
    85	        try:
    86	            i2, c2 = _ask(fallback)
    87	            if c2 >= conf:
    88	                intent, conf = i2, c2
    89	        except Exception:
    90	            pass
    91	
    92	    if intent == "llm" and conf >= _conf_min():
    93	        return "llm"
    94	    return "material_select"

################################################################################
# FILE: backend/app/services/langgraph/graph/nodes/deterministic.py
################################################################################
     1	from __future__ import annotations
     2	from typing import Dict, Any
     3	from math import pi
     4	
     5	def intake_validate(state: Dict[str, Any]) -> Dict[str, Any]:
     6	    p = state.get("params", {}) or {}
     7	    missing = []
     8	    for k in ("medium", "pressure_bar", "temp_max_c"):
     9	        if p.get(k) is None:
    10	            missing.append(k)
    11	    if state.get("mode") == "consult" and p.get("speed_rpm") is None:
    12	        missing.append("speed_rpm")
    13	    state.setdefault("derived", {}).setdefault("notes", [])
    14	    if missing:
    15	        state.setdefault("ui_events", []).append({
    16	            "ui_action": "open_form",
    17	            "payload": {"form_id": "rwdr_params_v1", "missing": missing, "prefill": p}
    18	        })
    19	    return state
    20	
    21	def _v_m_s(d_mm: float, rpm: float) -> float:
    22	    return pi * (d_mm/1000.0) * (rpm/60.0)
    23	
    24	def _dn(d_mm: float, rpm: float) -> float:
    25	    return d_mm * rpm
    26	
    27	def calc_core(state: Dict[str, Any]) -> Dict[str, Any]:
    28	    p = state.get("params", {}) or {}
    29	    d = state.get("derived", {}) or {}
    30	    shaft = p.get("shaft_d")
    31	    rpm = p.get("speed_rpm")
    32	    pressure = p.get("pressure_bar")
    33	    if shaft and rpm:
    34	        v = _v_m_s(shaft, rpm)
    35	        d["v_m_s"] = round(v, 4)
    36	        d["dn_value"] = round(_dn(shaft, rpm), 2)
    37	    if pressure and "v_m_s" in d:
    38	        d["pv_indicator_bar_ms"] = round(float(pressure) * float(d["v_m_s"]), 4)
    39	    state["derived"] = d
    40	    return state
    41	
    42	def calc_advanced(state: Dict[str, Any]) -> Dict[str, Any]:
    43	    p = state.get("params", {}) or {}
    44	    notes = state.setdefault("derived", {}).setdefault("notes", [])
    45	    if p.get("temp_max_c", 0) > 200:
    46	        notes.append("Hohe Temperatur: Fluorpolymere prüfen.")
    47	    if (p.get("pressure_bar") or 0) > 5:
    48	        notes.append("Druck > 5 bar: Stützelement/Extrusionsschutz prüfen.")
    49	    return state

################################################################################
# FILE: backend/app/services/langgraph/graph/nodes/explain_nodes.py
################################################################################
     1	from __future__ import annotations
     2	from typing import Dict, Any
     3	from app.services.langgraph.policies.model_routing import RoutingContext, llm_params_for, should_use_llm
     4	from app.services.langgraph.llm_factory import get_llm
     5	
     6	def explain(state: Dict[str, Any]) -> Dict[str, Any]:
     7	    if not should_use_llm("explain"):
     8	        return state
     9	    ctx = RoutingContext(
    10	        node="explain",
    11	        confidence=state.get("confidence"),
    12	        red_flags=bool(state.get("red_flags")),
    13	        regulatory=bool(state.get("regulatory")),
    14	    )
    15	    llm_cfg = llm_params_for("explain", ctx)
    16	    # sanitize unsupported kwargs
    17	    llm_cfg.pop("top_p", None)
    18	    llm = get_llm(**llm_cfg)
    19	
    20	    params = state.get("params", {})
    21	    derived = state.get("derived", {})
    22	    sources = state.get("sources", [])
    23	    prompt = (
    24	        "Erkläre die Auswahlkriterien kurz und sachlich. Nutze nur PARAMS, abgeleitete Werte und Quellen. "
    25	        "Keine Produkte, keine Entscheidungen. Quellen benennen.\n"
    26	        f"PARAMS: {params}\nDERIVED: {derived}\nSOURCES: {sources}\n"
    27	        "Gib 3–6 Sätze."
    28	    )
    29	    msg = llm.invoke([{"role": "user", "content": prompt}])
    30	    state.setdefault("messages", []).append({"role": "assistant", "content": msg.content})
    31	    return state

################################################################################
# FILE: backend/app/services/langgraph/graph/nodes/__init__.py
################################################################################

################################################################################
# FILE: backend/app/services/langgraph/graph/nodes/rag_nodes.py
################################################################################
     1	from __future__ import annotations
     2	from typing import Dict, Any, List
     3	from datetime import date
     4	from app.services.langgraph.tools.telemetry import telemetry, PARTNER_COVERAGE, NO_MATCH_RATE
     5	
     6	def rag_retrieve(state: Dict[str, Any]) -> Dict[str, Any]:
     7	    cands = state.get("candidates") or []
     8	    state["sources"] = state.get("sources") or []
     9	    state.setdefault("telemetry", {})["candidates_total"] = len(cands)
    10	    return state
    11	
    12	def _is_partner(c: Dict[str, Any]) -> bool:
    13	    tier = (c.get("paid_tier") or "none").lower()
    14	    active = bool(c.get("active", False))
    15	    valid_until = (c.get("contract_valid_until") or "")
    16	    try:
    17	        y, m, d = map(int, valid_until.split("-"))
    18	        ok_date = date(y, m, d) >= date.today()
    19	    except Exception:
    20	        ok_date = False
    21	    return tier != "none" and active and ok_date
    22	
    23	def partner_only_filter(state: Dict[str, Any]) -> Dict[str, Any]:
    24	    cands: List[Dict[str, Any]] = state.get("candidates") or []
    25	    partners = [c for c in cands if _is_partner(c)]
    26	    state["candidates"] = partners
    27	    total = state.get("telemetry", {}).get("candidates_total", 0)
    28	    coverage = (len(partners) / total) if total else 0.0
    29	    telemetry.set_gauge(PARTNER_COVERAGE, coverage)
    30	    if not partners:
    31	        state.setdefault("ui_events", []).append({"ui_action": "no_partner_available", "payload": {}})
    32	        telemetry.incr(NO_MATCH_RATE, 1)
    33	    return state
    34	
    35	def rules_filter(state: Dict[str, Any]) -> Dict[str, Any]:
    36	    return state

################################################################################
# FILE: backend/app/services/langgraph/graph/nodes/rfq_nodes.py
################################################################################
     1	from __future__ import annotations
     2	from typing import Dict, Any
     3	from datetime import datetime
     4	import os, uuid
     5	from app.services.langgraph.pdf.rfq_renderer import generate_rfq_pdf
     6	from app.services.langgraph.tools.telemetry import telemetry, RFQ_GENERATED
     7	from app.services.langgraph.tools.ui_events import UI, make_event
     8	
     9	def decision_ready(state: Dict[str, Any]) -> Dict[str, Any]:
    10	    state.setdefault("ui_events", []).append(make_event(UI["DECISION_READY"], summary={
    11	        "params": state.get("params"),
    12	        "derived": state.get("derived"),
    13	        "candidate_count": len(state.get("candidates") or []),
    14	    }))
    15	    return state
    16	
    17	def await_user_action(state: Dict[str, Any]) -> Dict[str, Any]:
    18	    return state
    19	
    20	def generate_rfq_pdf_node(state: Dict[str, Any]) -> Dict[str, Any]:
    21	    if state.get("user_action") != "export_pdf":
    22	        return state
    23	    out_dir = os.getenv("RFQ_PDF_DIR", "/app/data/rfq")
    24	    os.makedirs(out_dir, exist_ok=True)
    25	    fname = f"rfq_{datetime.utcnow().strftime('%Y%m%dT%H%M%S')}_{uuid.uuid4().hex[:6]}.pdf"
    26	    path = os.path.join(out_dir, fname)
    27	    payload = {
    28	        "params": state.get("params"),
    29	        "derived": state.get("derived"),
    30	        "candidates": state.get("candidates"),
    31	        "sources": state.get("sources"),
    32	        "legal_notice": "Verbindliche Eignungszusage obliegt dem Hersteller.",
    33	    }
    34	    generate_rfq_pdf(payload, path)
    35	    state["rfq_pdf"] = {"path": path, "created_at": datetime.utcnow().isoformat() + "Z", "download_token": uuid.uuid4().hex}
    36	    telemetry.incr(RFQ_GENERATED, 1)
    37	    return state
    38	
    39	def deliver_pdf(state: Dict[str, Any]) -> Dict[str, Any]:
    40	    if state.get("rfq_pdf"):
    41	        state.setdefault("ui_events", []).append(make_event(UI["RFQ_READY"], pdf=state["rfq_pdf"]))
    42	    return state

################################################################################
# FILE: backend/app/services/langgraph/graph/orchestrator.py
################################################################################
     1	from __future__ import annotations
     2	from typing import Dict, Any
     3	from langgraph.graph import StateGraph, END
     4	from app.services.langgraph.graph.state import SealAIState
     5	from app.services.langgraph.graph.nodes.deterministic import intake_validate, calc_core, calc_advanced
     6	from app.services.langgraph.graph.nodes.rag_nodes import rag_retrieve, partner_only_filter, rules_filter
     7	from app.services.langgraph.graph.nodes.explain_nodes import explain
     8	from app.services.langgraph.graph.nodes.rfq_nodes import decision_ready, await_user_action, generate_rfq_pdf_node, deliver_pdf
     9	
    10	def build_sealai_graph() -> StateGraph:
    11	    g = StateGraph(SealAIState)
    12	    g.add_node("intake_validate", intake_validate)
    13	    g.add_node("calc_core", calc_core)
    14	    g.add_node("calc_advanced", calc_advanced)
    15	    g.add_node("rag_retrieve", rag_retrieve)
    16	    g.add_node("partner_only_filter", partner_only_filter)
    17	    g.add_node("rules_filter", rules_filter)
    18	    g.add_node("explain", explain)
    19	    g.add_node("decision_ready", decision_ready)
    20	    g.add_node("await_user_action", await_user_action)
    21	    g.add_node("generate_rfq_pdf", generate_rfq_pdf_node)
    22	    g.add_node("deliver_pdf", deliver_pdf)
    23	
    24	    g.set_entry_point("intake_validate")
    25	    g.add_edge("intake_validate", "calc_core")
    26	    g.add_edge("calc_core", "calc_advanced")
    27	    g.add_edge("calc_advanced", "rag_retrieve")
    28	    g.add_edge("rag_retrieve", "partner_only_filter")
    29	    g.add_edge("partner_only_filter", "rules_filter")
    30	    g.add_edge("rules_filter", "explain")
    31	    g.add_edge("explain", "decision_ready")
    32	    g.add_edge("decision_ready", "await_user_action")
    33	    g.add_edge("await_user_action", "generate_rfq_pdf")
    34	    g.add_edge("generate_rfq_pdf", "deliver_pdf")
    35	    g.add_edge("deliver_pdf", END)
    36	    return g
    37	
    38	def invoke_sealai(state: Dict[str, Any]) -> Dict[str, Any]:
    39	    mode = (state.get("mode") or "consult").lower()
    40	    g = build_sealai_graph().compile()
    41	    out = g.invoke(state)
    42	    if mode != "consult":
    43	        out.pop("rfq_pdf", None)
    44	        out["ui_events"] = [e for e in out.get("ui_events", []) if e.get("ui_action") != "rfq_ready"]
    45	    return out

################################################################################
# FILE: backend/app/services/langgraph/graph/sealai_consult_flow.py
################################################################################
     1	# backend/app/services/langgraph/graph/sealai_consult_flow.py
     2	from __future__ import annotations
     3	
     4	import os
     5	import logging
     6	
     7	from .consult.io import invoke_consult as _invoke_consult_single
     8	
     9	log = logging.getLogger(__name__)
    10	
    11	_SUP_AVAILABLE = True
    12	try:
    13	    from .supervisor_graph import invoke_consult_supervisor as _invoke_consult_supervisor
    14	except Exception as e:
    15	    _SUP_AVAILABLE = False
    16	    log.warning("Supervisor graph not available, falling back to single-agent: %s", e)
    17	
    18	_MODE = os.getenv("CONSULT_MODE", "consult").strip().lower()
    19	
    20	def invoke_consult(prompt: str, *, thread_id: str) -> str:
    21	    use_supervisor = (_MODE == "supervisor" and _SUP_AVAILABLE)
    22	    if use_supervisor:
    23	        try:
    24	            return _invoke_consult_supervisor(prompt, thread_id=thread_id)
    25	        except Exception as e:
    26	            log.exception("Supervisor failed, falling back to single-agent: %s", e)
    27	    return _invoke_consult_single(prompt, thread_id=thread_id)

################################################################################
# FILE: backend/app/services/langgraph/graph/state.py
################################################################################
     1	from __future__ import annotations
     2	from typing import Any, Dict, List, Optional, TypedDict
     3	from typing_extensions import Annotated
     4	from langgraph.graph import add_messages
     5	
     6	class Params(TypedDict, total=False):
     7	    shaft_d: float
     8	    housing_d: float
     9	    width: float
    10	    medium: str
    11	    pressure_bar: float
    12	    temp_min_c: float
    13	    temp_max_c: float
    14	    speed_rpm: float
    15	
    16	class Derived(TypedDict, total=False):
    17	    v_m_s: float
    18	    dn_value: float
    19	    pv_indicator_bar_ms: float
    20	    notes: List[str]
    21	
    22	class Candidate(TypedDict, total=False):
    23	    doc_id: str
    24	    vendor_id: str
    25	    title: str
    26	    profile: str
    27	    material: str
    28	    paid_tier: str
    29	    contract_valid_until: str
    30	    active: bool
    31	    score: float
    32	    url: Optional[str]
    33	
    34	class RFQPdfInfo(TypedDict, total=False):
    35	    path: str
    36	    created_at: str
    37	    download_token: str
    38	
    39	class UIEvent(TypedDict, total=False):
    40	    ui_action: str
    41	    payload: Dict[str, Any]
    42	
    43	class SealAIState(TypedDict, total=False):
    44	    messages: Annotated[List[Any], add_messages]
    45	    mode: str
    46	    params: Params
    47	    derived: Derived
    48	    candidates: List[Candidate]
    49	    sources: List[Dict[str, Any]]
    50	    user_action: Optional[str]
    51	    rfq_pdf: Optional[RFQPdfInfo]
    52	    ui_events: List[UIEvent]
    53	    telemetry: Dict[str, Any]
    54	    confidence: Optional[float]
    55	    red_flags: bool
    56	    regulatory: bool

################################################################################
# FILE: backend/app/services/langgraph/graph/supervisor_graph.py
################################################################################
     1	from __future__ import annotations
     2	
     3	import logging
     4	from functools import lru_cache
     5	from typing import TypedDict, List, Literal, Optional
     6	
     7	from langchain_openai import ChatOpenAI
     8	from langchain_core.messages import BaseMessage, AIMessage
     9	from langchain_core.tools import tool
    10	from langgraph.graph import StateGraph
    11	from langgraph.constants import END
    12	
    13	from app.services.langgraph.tools import long_term_memory as ltm
    14	from .intent_router import classify_intent
    15	from .consult.build import build_consult_graph
    16	from app.services.langgraph.llm_factory import get_llm
    17	
    18	log = logging.getLogger(__name__)
    19	
    20	@lru_cache(maxsize=1)
    21	def create_llm() -> ChatOpenAI:
    22	    """Create a ChatOpenAI instance (streaming enabled) from the central LLM factory."""
    23	    return get_llm(streaming=True)
    24	
    25	@tool
    26	def ltm_search(query: str) -> str:
    27	    """Durchsucht das Long-Term-Memory (Qdrant) nach relevanten Erinnerungen (MMR, top-k=5) und gibt einen zusammenhängenden Kontext-Text zurück."""
    28	    ctx, _hits = ltm.ltm_query(query, strategy="mmr", top_k=5)
    29	    return ctx or "Keine relevanten Erinnerungen gefunden."
    30	
    31	@tool
    32	def ltm_store(user: str, chat_id: str, text: str, kind: str = "note") -> str:
    33	    """Speichert einen Text-Schnipsel im Long-Term-Memory (Qdrant). Parameter: user, chat_id, text, kind."""
    34	    try:
    35	        pid = ltm.upsert_memory(user=user, chat_id=chat_id, text=text, kind=kind)
    36	        return f"Memory gespeichert (ID={pid})"
    37	    except Exception as e:
    38	        return f"Fehler beim Speichern: {e}"
    39	
    40	TOOLS = [ltm_search, ltm_store]
    41	
    42	class ChatState(TypedDict, total=False):
    43	    messages: List[BaseMessage]
    44	    intent: Literal["consult", "chitchat"]
    45	
    46	@lru_cache(maxsize=1)
    47	def _compiled_consult_graph():
    48	    return build_consult_graph().compile()
    49	
    50	def build_chat_builder(llm: Optional[ChatOpenAI] = None) -> StateGraph:
    51	    """Erstellt den Supervisor-Graphen: Router -> (Consult|Chitchat)."""
    52	    log.info("[supervisor] Initialisiere…")
    53	    builder = StateGraph(ChatState)
    54	
    55	    base_llm = llm or create_llm()
    56	    llm_chitchat = base_llm.bind_tools(TOOLS)
    57	    consult_graph = _compiled_consult_graph()
    58	
    59	    def router_node(state: ChatState) -> ChatState:
    60	        intent = classify_intent(base_llm, state.get("messages", []))
    61	        return {"intent": intent}
    62	
    63	    def chitchat_node(state: ChatState) -> ChatState:
    64	        history = state.get("messages", [])
    65	        result = llm_chitchat.invoke(history)
    66	        ai_msg = result if isinstance(result, AIMessage) else AIMessage(content=getattr(result, "content", str(result)))
    67	        return {"messages": [ai_msg]}
    68	
    69	    def consult_node(state: ChatState) -> ChatState:
    70	        result = consult_graph.invoke({"messages": state.get("messages", [])})
    71	        out_msgs = result.get("messages") or []
    72	        ai_txt = ""
    73	        for m in reversed(out_msgs):
    74	            if isinstance(m, AIMessage):
    75	                ai_txt = (m.content or "").strip()
    76	                break
    77	        if not ai_txt:
    78	            ai_txt = "Die Beratung wurde abgeschlossen."
    79	        return {"messages": [AIMessage(content=ai_txt)]}
    80	
    81	    builder.add_node("router", router_node)
    82	    builder.add_node("chitchat", chitchat_node)
    83	    builder.add_node("consult", consult_node)
    84	
    85	    builder.set_entry_point("router")
    86	
    87	    def decide(state: ChatState) -> str:
    88	        intent = state.get("intent") or "chitchat"
    89	        return "consult" if intent == "consult" else "chitchat"
    90	
    91	    builder.add_conditional_edges("router", decide, {"consult": "consult", "chitchat": "chitchat"})
    92	    builder.add_edge("consult", END)
    93	    builder.add_edge("chitchat", END)
    94	
    95	    log.info("[supervisor] Bereit.")
    96	    return builder

################################################################################
# FILE: backend/app/services/langgraph/nodes/__init__.py
################################################################################

################################################################################
# FILE: backend/app/services/langgraph/pdf/__init__.py
################################################################################

################################################################################
# FILE: backend/app/services/langgraph/pdf/rfq_renderer.py
################################################################################
     1	from __future__ import annotations
     2	from typing import Dict, Any
     3	
     4	def _ensure_reportlab():
     5	    # Lazy import to avoid startup crashes if reportlab not yet installed
     6	    global canvas, A4, mm, simpleSplit
     7	    try:
     8	        from reportlab.lib.pagesizes import A4
     9	        from reportlab.pdfgen import canvas
    10	        from reportlab.lib.units import mm
    11	        from reportlab.lib.utils import simpleSplit
    12	    except Exception as e:
    13	        raise RuntimeError("reportlab missing: pip install reportlab>=4.2.0") from e
    14	
    15	def _draw_multiline(c, text: str, x: float, y: float, max_width: float, leading: float = 14):
    16	    lines = simpleSplit(text, "Helvetica", 10, max_width)
    17	    for line in lines:
    18	        c.drawString(x, y, line)
    19	        y -= leading
    20	    return y
    21	
    22	def generate_rfq_pdf(data: Dict[str, Any], out_path: str) -> None:
    23	    _ensure_reportlab()
    24	    c = canvas.Canvas(out_path, pagesize=A4)
    25	    width, height = A4
    26	    x0, y = 20*mm, height - 25*mm
    27	
    28	    c.setFont("Helvetica-Bold", 14)
    29	    c.drawString(x0, y, "RFQ – Request for Quotation")
    30	    c.setFont("Helvetica", 9)
    31	    c.drawString(x0, y-14, "SealAI – Dichtungstechnik Beratung")
    32	    y -= 30
    33	
    34	    # Eingabedaten
    35	    c.setFont("Helvetica-Bold", 11); c.drawString(x0, y, "Eingabedaten"); y -= 12
    36	    c.setFont("Helvetica", 10)
    37	    for k, v in (data.get("params") or {}).items():
    38	        c.drawString(x0, y, f"- {k}: {v}"); y -= 12
    39	
    40	    # Abgeleitete Kennwerte
    41	    y -= 8; c.setFont("Helvetica-Bold", 11); c.drawString(x0, y, "Abgeleitete Kennwerte"); y -= 12
    42	    c.setFont("Helvetica", 10)
    43	    for k, v in (data.get("derived") or {}).items():
    44	        c.drawString(x0, y, f"- {k}: {v}"); y -= 12
    45	
    46	    # Kandidaten
    47	    y -= 8; c.setFont("Helvetica-Bold", 11); c.drawString(x0, y, "Top-Partnerprodukte"); y -= 12
    48	    c.setFont("Helvetica", 10); c.drawString(x0, y, "(Preise/LZ/MOQ durch Hersteller)"); y -= 14
    49	    for cand in (data.get("candidates") or [])[:10]:
    50	        line = f"- {cand.get('title')} | {cand.get('vendor_id')} | Material {cand.get('material')} | Profil {cand.get('profile')}"
    51	        y = _draw_multiline(c, line, x0, y, width - 40*mm)
    52	        if y < 40*mm:
    53	            c.showPage(); y = height - 25*mm; c.setFont("Helvetica", 10)
    54	
    55	    # Quellen
    56	    y -= 8; c.setFont("Helvetica-Bold", 11); c.drawString(x0, y, "Quellen"); y -= 12
    57	    c.setFont("Helvetica", 9)
    58	    for src in (data.get("sources") or [])[:12]:
    59	        y = _draw_multiline(c, f"- {src}", x0, y, width - 40*mm, leading=12)
    60	        if y < 40*mm:
    61	            c.showPage(); y = height - 25*mm; c.setFont("Helvetica", 9)
    62	
    63	    # Rechtshinweis
    64	    y -= 8; c.setFont("Helvetica", 9)
    65	    leg = data.get("legal_notice") or "Verbindliche Eignungszusage obliegt dem Hersteller."
    66	    _draw_multiline(c, f"Rechtshinweis: {leg}", x0, y, width - 40*mm, leading=12)
    67	
    68	    c.showPage()
    69	    c.save()

################################################################################
# FILE: backend/app/services/langgraph/policies/__init__.py
################################################################################

################################################################################
# FILE: backend/app/services/langgraph/policies/model_routing.py
################################################################################
     1	from __future__ import annotations
     2	from dataclasses import dataclass
     3	from typing import Optional, Literal, Dict, Any
     4	import os
     5	
     6	ModelName = Literal["gpt-5-nano", "gpt-5-mini", "gpt-5"]
     7	
     8	@dataclass
     9	class RoutingContext:
    10	    node: str
    11	    confidence: Optional[float] = None
    12	    red_flags: bool = False
    13	    regulatory: bool = False
    14	    ambiguous: bool = False
    15	    hint: Optional[ModelName] = None
    16	
    17	DEFAULTS: Dict[str, ModelName] = {
    18	    "normalize_intent": "gpt-5-nano",
    19	    "pre_extract": "gpt-5-nano",
    20	    "domain_router": "gpt-5-nano",
    21	    "ask_missing": "gpt-5-nano",
    22	    "critic_light": "gpt-5-mini",
    23	    "explain": "gpt-5-mini",
    24	    "info_graph": "gpt-5-mini",
    25	    "market_graph": "gpt-5-nano",
    26	    "service_graph": "gpt-5-mini",
    27	}
    28	
    29	def select_model(ctx: RoutingContext) -> ModelName:
    30	    if ctx.hint:
    31	        return ctx.hint
    32	    if ctx.red_flags or ctx.regulatory:
    33	        return "gpt-5"
    34	    if ctx.confidence is not None:
    35	        if ctx.confidence < 0.70:
    36	            return "gpt-5"
    37	        if 0.70 <= ctx.confidence <= 0.84:
    38	            return "gpt-5-mini"
    39	        return "gpt-5-nano"
    40	    if ctx.ambiguous and ctx.node in ("domain_router", "info_graph"):
    41	        return "gpt-5-mini"
    42	    return DEFAULTS.get(ctx.node, "gpt-5-mini")
    43	
    44	def should_use_llm(node: str) -> bool:
    45	    deterministic = {
    46	        "intake_validate", "calc_core", "calc_advanced",
    47	        "rag_retrieve", "rules_filter",
    48	        "generate_rfq_pdf", "deliver_pdf"
    49	    }
    50	    return node not in deterministic
    51	
    52	def llm_params_for(node: str, ctx: RoutingContext) -> Dict[str, Any]:
    53	    model = select_model(ctx)
    54	    temperature = float(os.getenv("SEALAI_LLM_TEMPERATURE", "0.2"))
    55	    top_p = float(os.getenv("SEALAI_LLM_TOP_P", "0.9"))
    56	    max_tokens = {"gpt-5-nano": 512, "gpt-5-mini": 2048, "gpt-5": 4096}[model]
    57	    return {"model": model, "temperature": temperature, "top_p": top_p, "max_tokens": max_tokens}

################################################################################
# FILE: backend/app/services/langgraph/prompting.py
################################################################################
     1	# backend/app/services/langgraph/prompting.py
     2	from __future__ import annotations
     3	
     4	import json
     5	import logging
     6	import os
     7	import re
     8	from pathlib import Path
     9	from typing import Any, Iterable, List, Dict
    10	
    11	from jinja2 import Environment, FileSystemLoader, StrictUndefined
    12	from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
    13	
    14	log = logging.getLogger(__name__)
    15	
    16	# -------------------------------------------------------------------
    17	# Template-Verzeichnisse einsammeln (mit ENV-Override)
    18	# -------------------------------------------------------------------
    19	_BASE = Path(__file__).resolve().parent
    20	_GLOBAL_PROMPTS = _BASE / "prompts"
    21	_GLOBAL_PROMPT_TEMPLATES = _BASE / "prompt_templates"
    22	_GRAPH_CONSULT_PROMPTS = _BASE / "graph" / "consult" / "prompts"
    23	_DOMAINS_ROOT = _BASE / "domains"
    24	
    25	
    26	def _collect_template_dirs() -> List[Path]:
    27	    # Optional: zusätzliche Pfade per ENV (z. B. "/app/custom_prompts:/mnt/prompts")
    28	    env_paths: List[Path] = []
    29	    raw = os.getenv("SEALAI_TEMPLATE_DIRS", "").strip()
    30	    if raw:
    31	        for p in raw.split(":"):
    32	            pp = Path(p).resolve()
    33	            if pp.is_dir():
    34	                env_paths.append(pp)
    35	
    36	    fixed: List[Path] = [
    37	        _GLOBAL_PROMPTS,
    38	        _GLOBAL_PROMPT_TEMPLATES,
    39	        _GRAPH_CONSULT_PROMPTS,
    40	    ]
    41	
    42	    domain_prompts: List[Path] = []
    43	    if _DOMAINS_ROOT.is_dir():
    44	        for p in _DOMAINS_ROOT.glob("**/prompts"):
    45	            if p.is_dir():
    46	                domain_prompts.append(p)
    47	
    48	    all_candidates = env_paths + fixed + domain_prompts
    49	
    50	    seen = set()
    51	    result: List[Path] = []
    52	    for p in all_candidates:
    53	        try:
    54	            rp = p.resolve()
    55	        except Exception:
    56	            rp = p
    57	        if rp.is_dir():
    58	            key = str(rp)
    59	            if key not in seen:
    60	                seen.add(key)
    61	                result.append(rp)
    62	
    63	    if not result:
    64	        result = [_BASE]
    65	        log.warning("[prompting] Keine Template-Verzeichnisse gefunden; Fallback=%s", _BASE)
    66	
    67	    try:
    68	        log.info("[prompting] template search dirs: %s", ", ".join(str(p) for p in result))
    69	    except Exception:
    70	        pass
    71	
    72	    return result
    73	
    74	
    75	_ENV = Environment(
    76	    loader=FileSystemLoader([str(p) for p in _collect_template_dirs()]),
    77	    autoescape=False,
    78	    undefined=StrictUndefined,  # Fail-fast
    79	    trim_blocks=True,
    80	    lstrip_blocks=True,
    81	)
    82	
    83	# -------------------------------------------------------------------
    84	# Jinja2 Filter
    85	# -------------------------------------------------------------------
    86	def _regex_search(value: Any, pattern: str) -> bool:
    87	    try:
    88	        return re.search(pattern, str(value or ""), flags=re.I) is not None
    89	    except Exception:
    90	        return False
    91	
    92	
    93	def _tojson_compact(value: Any) -> str:
    94	    return json.dumps(value, ensure_ascii=False, separators=(",", ":"))
    95	
    96	
    97	def _tojson_pretty(value: Any) -> str:
    98	    return json.dumps(value, ensure_ascii=False, indent=2)
    99	
   100	
   101	_ENV.filters["regex_search"] = _regex_search
   102	_ENV.filters["tojson_compact"] = _tojson_compact
   103	_ENV.filters["tojson_pretty"] = _tojson_pretty
   104	
   105	# -------------------------------------------------------------------
   106	# Public API
   107	# -------------------------------------------------------------------
   108	def render_template(name: str, /, **kwargs: Any) -> str:
   109	    """Rendert ein Jinja2-Template und loggt die Quelle; fügt params_json automatisch hinzu."""
   110	    if "params" in kwargs and "params_json" not in kwargs:
   111	        try:
   112	            kwargs["params_json"] = safe_json(kwargs["params"])
   113	        except Exception:
   114	            kwargs["params_json"] = "{}"
   115	
   116	    tpl = _ENV.get_template(name)
   117	    src_file = getattr(tpl, "filename", None)
   118	    log.info("[prompting] loaded template '%s' from '%s'", name, src_file or "?")
   119	    return tpl.render(**kwargs)
   120	
   121	
   122	def messages_for_template(seq: Iterable[Any]) -> List[Dict[str, str]]:
   123	    """Normalisiert Nachrichten in [{type, content}]."""
   124	    out: List[Dict[str, str]] = []
   125	
   126	    def _norm_one(m: Any) -> Dict[str, str]:
   127	        if isinstance(m, HumanMessage):
   128	            return {"type": "user", "content": (m.content or "").strip()}
   129	        if isinstance(m, AIMessage):
   130	            return {"type": "ai", "content": (m.content or "").strip()}
   131	        if isinstance(m, SystemMessage):
   132	            return {"type": "system", "content": (m.content or "").strip()}
   133	
   134	        if isinstance(m, dict):
   135	            role = (m.get("role") or m.get("type") or "").lower()
   136	            content = (m.get("content") or "").strip()
   137	            if role in ("user", "human"):
   138	                t = "user"
   139	            elif role in ("assistant", "ai"):
   140	                t = "ai"
   141	            elif role == "system":
   142	                t = "system"
   143	            else:
   144	                t = "user"
   145	            return {"type": t, "content": content}
   146	
   147	        return {"type": "user", "content": (str(m) if m is not None else "").strip()}
   148	
   149	    for m in (seq or []):
   150	        norm = _norm_one(m)
   151	        if norm["content"]:
   152	            out.append(norm)
   153	    return out
   154	
   155	
   156	# -------------------------------------------------------------------
   157	# JSON-Utilities
   158	# -------------------------------------------------------------------
   159	_CODE_FENCE_RX = re.compile(r"^```(?:json|JSON)?\s*(.*?)\s*```$", re.DOTALL)
   160	
   161	
   162	def _extract_balanced_json(s: str) -> str:
   163	    """Extrahiert den ersten ausgewogenen JSON-Block ({...} oder [...]) aus s."""
   164	    if not s:
   165	        return ""
   166	    start_idx = None
   167	    opener = None
   168	    closer = None
   169	    for i, ch in enumerate(s):
   170	        if ch in "{[":
   171	            start_idx = i
   172	            opener = ch
   173	            closer = "}" if ch == "{" else "]"
   174	            break
   175	    if start_idx is None:
   176	        return s.strip()
   177	
   178	    depth = 0
   179	    in_string = False
   180	    escape = False
   181	    for j in range(start_idx, len(s)):
   182	        ch = s[j]
   183	        if in_string:
   184	            if escape:
   185	                escape = False
   186	            elif ch == "\\":
   187	                escape = True
   188	            elif ch == '"':
   189	                in_string = False
   190	        else:
   191	            if ch == '"':
   192	                in_string = True
   193	            elif ch == opener:
   194	                depth += 1
   195	            elif ch == closer:
   196	                depth -= 1
   197	                if depth == 0:
   198	                    return s[start_idx : j + 1].strip()
   199	    return s[start_idx:].strip()
   200	
   201	
   202	def strip_json_fence(text: str) -> str:
   203	    """Entfernt ```json fences``` ODER extrahiert den ersten ausgewogenen JSON-Block."""
   204	    if not isinstance(text, str):
   205	        return ""
   206	    s = text.strip()
   207	
   208	    m = _CODE_FENCE_RX.match(s)
   209	    if m:
   210	        inner = m.group(1).strip()
   211	        if inner.startswith("{") or inner.startswith("["):
   212	            return _extract_balanced_json(inner)
   213	        return inner
   214	
   215	    if s.startswith("{") or s.startswith("["):
   216	        return _extract_balanced_json(s)
   217	
   218	    return _extract_balanced_json(s)
   219	
   220	
   221	def safe_json(obj: Any) -> str:
   222	    """Kompaktes JSON (UTF-8) für Prompt-Übergaben."""
   223	    return json.dumps(obj or {}, ensure_ascii=False, separators=(",", ":"))

################################################################################
# FILE: backend/app/services/langgraph/prompts/__init__.py
################################################################################

################################################################################
# FILE: backend/app/services/langgraph/prompt_templates/__init__.py
################################################################################

################################################################################
# FILE: backend/app/services/langgraph/rag/__init__.py
################################################################################

################################################################################
# FILE: backend/app/services/langgraph/rag/schemas.py
################################################################################
     1	from __future__ import annotations
     2	from dataclasses import dataclass
     3	from datetime import date
     4	from typing import Optional
     5	
     6	@dataclass
     7	class VendorMeta:
     8	    vendor_id: str
     9	    paid_tier: str
    10	    contract_valid_until: date
    11	    active: bool
    12	
    13	    def is_partner(self, today: Optional[date] = None) -> bool:
    14	        t = today or date.today()
    15	        return self.paid_tier != "none" and self.active and self.contract_valid_until >= t

################################################################################
# FILE: backend/app/services/langgraph/rules/__init__.py
################################################################################

################################################################################
# FILE: backend/app/services/langgraph/tests/__init__.py
################################################################################

################################################################################
# FILE: backend/app/services/langgraph/tests/test_ask_missing_prompt.py
################################################################################
     1	from __future__ import annotations
     2	from app.services.langgraph.graph.consult.nodes.ask_missing import ask_missing_node
     3	from app.services.langgraph.prompting import render_template
     4	
     5	def test_rwdr_missing_prompt_contains_expected_labels():
     6	    state = {"consult_required": True, "domain": "rwdr", "params": {}, "messages":[{"role":"user","content":"rwdr"}]}
     7	    res = ask_missing_node(state)
     8	    msg = res["messages"][0].content
     9	    assert "Welle (mm)" in msg
    10	    assert "Gehäuse (mm)" in msg
    11	    assert "Breite (mm)" in msg
    12	    assert "Druck (bar)" in msg
    13	    assert res.get("ui_event", {}).get("form_id") == "rwdr_params_v1"
    14	    assert res.get("phase") == "ask_missing"
    15	
    16	def test_hyd_missing_prompt_contains_expected_labels():
    17	    state = {"consult_required": True, "domain": "hydraulics_rod", "params": {}, "messages":[{"role":"user","content":"hyd"}]}
    18	    res = ask_missing_node(state)
    19	    msg = res["messages"][0].content
    20	    assert "Stange (mm)" in msg
    21	    assert "Nut-Ø D (mm)" in msg
    22	    assert "Nutbreite B (mm)" in msg
    23	    assert "Relativgeschwindigkeit (m/s)" in msg
    24	    assert res.get("ui_event", {}).get("form_id") == "hydraulics_rod_params_v1"
    25	
    26	def test_followups_template_renders_list():
    27	    out = render_template("ask_missing_followups.jinja2",
    28	                          followups=["Tmax plausibel bei v≈3 m/s?", "Druck > 200 bar bestätigt?"])
    29	    assert "Bevor ich empfehle" in out
    30	    assert "- Tmax plausibel bei v≈3 m/s?" in out
    31	    assert "- Druck > 200 bar bestätigt?" in out
    32	    assert "Passt das so?" in out

################################################################################
# FILE: backend/app/services/langgraph/tools/hitl.py
################################################################################
     1	from __future__ import annotations
     2	from typing import Dict, Any
     3	
     4	def hitl_required(reason: str) -> Dict[str, Any]:
     5	    return {"hitl_required": True, "reason": reason, "status": "pending_review"}

################################################################################
# FILE: backend/app/services/langgraph/tools/__init__.py
################################################################################

################################################################################
# FILE: backend/app/services/langgraph/tools/long_term_memory.py
################################################################################
     1	# backend/app/services/langgraph/tools/long_term_memory.py
     2	from __future__ import annotations
     3	
     4	import os
     5	import time
     6	import uuid
     7	import logging
     8	import threading
     9	from typing import Optional, Dict, Any
    10	
    11	log = logging.getLogger(__name__)
    12	log.setLevel(logging.INFO)
    13	
    14	# -------------------- ENV & Defaults --------------------
    15	
    16	_QDRANT_URL = os.getenv("QDRANT_URL", "http://qdrant:6333").strip() or "http://qdrant:6333"
    17	_COLLECTION = os.getenv("LTM_COLLECTION", "sealai_ltm").strip() or "sealai_ltm"
    18	_EMB_MODEL = os.getenv("LTM_EMBED_MODEL", "intfloat/multilingual-e5-base").strip() or "intfloat/multilingual-e5-base"
    19	_DISABLE_PREWARM = os.getenv("LTM_DISABLE_PREWARM", "0").strip() in ("1", "true", "yes", "on")
    20	
    21	# -------------------- Singletons --------------------
    22	
    23	_client = None            # QdrantClient
    24	_embeddings = None        # HuggingFaceEmbeddings
    25	_ready = False
    26	_init_err: Optional[str] = None
    27	_lock = threading.RLock()
    28	
    29	# -------------------- Init helpers --------------------
    30	
    31	def _init_hf_embeddings():
    32	    """Create CPU-safe HuggingFaceEmbeddings."""
    33	    from langchain_huggingface import HuggingFaceEmbeddings
    34	    log.info("LTM: using HuggingFaceEmbeddings model=%s", _EMB_MODEL)
    35	    return HuggingFaceEmbeddings(
    36	        model_name=_EMB_MODEL,
    37	        model_kwargs={"device": "cpu"},
    38	        encode_kwargs={"normalize_embeddings": True},
    39	    )
    40	
    41	def _ensure_collection(client, dim: int):
    42	    from qdrant_client.http.models import Distance, VectorParams
    43	    try:
    44	        info = client.get_collection(_COLLECTION)
    45	        if info and getattr(info, "vectors_count", 0) > 0:
    46	            return
    47	    except Exception:
    48	        pass
    49	    client.recreate_collection(
    50	        collection_name=_COLLECTION,
    51	        vectors_config=VectorParams(size=dim, distance=Distance.COSINE),
    52	    )
    53	
    54	def _do_init_once():
    55	    global _client, _embeddings, _ready, _init_err
    56	    from qdrant_client import QdrantClient
    57	
    58	    log.info("LTM: Connecting Qdrant at %s", _QDRANT_URL)
    59	    client = QdrantClient(url=_QDRANT_URL, prefer_grpc=False, timeout=5.0)
    60	
    61	    embeddings = _init_hf_embeddings()
    62	    # probe to get dimension
    63	    probe_vec = embeddings.embed_query("ltm-probe")
    64	    dim = len(probe_vec)
    65	    _ensure_collection(client, dim)
    66	
    67	    _client = client
    68	    _embeddings = embeddings
    69	    _ready = True
    70	    _init_err = None
    71	
    72	def _do_init(retries: int = 2, backoff_ms: int = 400):
    73	    global _ready, _init_err
    74	    if _ready:
    75	        return
    76	    with _lock:
    77	        if _ready:
    78	            return
    79	        for i in range(retries + 1):
    80	            try:
    81	                _do_init_once()
    82	                log.info("LTM init ok")
    83	                return
    84	            except Exception as e:
    85	                _init_err = f"{e}"
    86	                if i < retries:
    87	                    log.warning("LTM init attempt %s failed: %s – retrying in %dms", i + 1, _init_err, backoff_ms)
    88	                    time.sleep(backoff_ms / 1000.0)
    89	                else:
    90	                    log.error("LTM init failed: %s", _init_err)
    91	
    92	# -------------------- Public API --------------------
    93	
    94	def prewarm_ltm():
    95	    """Optional prewarm – no-op if disabled by ENV."""
    96	    if _DISABLE_PREWARM:
    97	        return
    98	    _do_init()
    99	
   100	def upsert_memory(*, user: str, chat_id: str, text: str, kind: str = "note") -> bool:
   101	    """
   102	    Store a short memory snippet. Returns True if stored, False otherwise.
   103	    Works even if init failed (returns False, no exception).
   104	    """
   105	    try:
   106	        if not _ready:
   107	            _do_init()
   108	        if not (_ready and _client and _embeddings):
   109	            return False
   110	
   111	        vec = _embeddings.embed_query(text or "")
   112	        if not isinstance(vec, list):
   113	            vec = list(vec)
   114	
   115	        payload: Dict[str, Any] = {
   116	            "user": user,
   117	            "chat_id": chat_id,
   118	            "kind": kind,
   119	            "text": text,
   120	        }
   121	
   122	        point_id = str(uuid.uuid4())
   123	        from qdrant_client.http.models import PointStruct
   124	        _client.upsert(
   125	            collection_name=_COLLECTION,
   126	            points=[PointStruct(id=point_id, vector=vec, payload=payload)],
   127	            wait=True,
   128	        )
   129	        return True
   130	    except Exception as e:
   131	        log.warning("LTM upsert failed: %s", e)
   132	        return False

################################################################################
# FILE: backend/app/services/langgraph/tools/rag_search_tool.py
################################################################################
     1	# backend/app/services/langgraph/tools/rag_search_tool.py
     2	from __future__ import annotations
     3	
     4	from typing import Any, Dict, List, Optional, TypedDict
     5	from langchain_core.tools import tool
     6	from ...rag.rag_orchestrator import hybrid_retrieve
     7	
     8	class RagSearchInput(TypedDict, total=False):
     9	    query: str
    10	    tenant: Optional[str]
    11	    k: int
    12	    filters: Dict[str, Any]
    13	
    14	@tool("rag_search", return_direct=False)
    15	def rag_search_tool(query: str, tenant: Optional[str] = None, k: int = 6, **filters: Any) -> List[Dict[str, Any]]:
    16	    """Hybrid Retrieval (Qdrant + BM25 + Rerank). Returns top-k docs with metadata and fused scores."""
    17	    docs = hybrid_retrieve(query=query, tenant=tenant, k=k, metadata_filters=filters or None, use_rerank=True)
    18	    return docs

################################################################################
# FILE: backend/app/services/langgraph/tools/telemetry.py
################################################################################
     1	from __future__ import annotations
     2	import os
     3	try:
     4	    import redis
     5	except Exception:
     6	    redis = None
     7	
     8	class Telemetry:
     9	    def __init__(self) -> None:
    10	        self.client = None
    11	        url = os.getenv("REDIS_URL") or os.getenv("REDIS_HOST")
    12	        if redis and url:
    13	            try:
    14	                self.client = redis.Redis.from_url(url) if "://" in url else redis.Redis(host=url, port=int(os.getenv("REDIS_PORT", "6379")))
    15	            except Exception:
    16	                self.client = None
    17	
    18	    def incr(self, key: str, amount: int = 1) -> None:
    19	        if self.client:
    20	            try:
    21	                self.client.incr(key, amount)
    22	            except Exception:
    23	                pass
    24	
    25	    def set_gauge(self, key: str, value: float) -> None:
    26	        if self.client:
    27	            try:
    28	                self.client.set(key, value)
    29	            except Exception:
    30	                pass
    31	
    32	telemetry = Telemetry()
    33	RFQ_GENERATED = "rfq_generated_count"
    34	PARTNER_COVERAGE = "partner_coverage_rate"
    35	MODEL_USAGE = "model_usage_distribution"
    36	NO_MATCH_RATE = "no_match_rate"

################################################################################
# FILE: backend/app/services/langgraph/tools/ui_events.py
################################################################################
     1	from __future__ import annotations
     2	from typing import Dict, Any
     3	
     4	UI = dict(
     5	    DECISION_READY="decision_ready",
     6	    RFQ_READY="rfq_ready",
     7	    NO_PARTNER_AVAILABLE="no_partner_available",
     8	    OPEN_FORM="open_form",
     9	)
    10	
    11	def make_event(action: str, **payload: Any) -> Dict[str, Any]:
    12	    return {"ui_action": action, "payload": payload}

################################################################################
# FILE: backend/app/services/rag/rag_orchestrator.py
################################################################################
     1	# backend/app/services/rag/rag_orchestrator.py
     2	from __future__ import annotations
     3	
     4	import math
     5	import os
     6	import time
     7	import re
     8	from dataclasses import dataclass, asdict
     9	from typing import Any, Dict, List, Optional
    10	
    11	import structlog
    12	from qdrant_client import QdrantClient
    13	from qdrant_client.http.models import (
    14	    Distance,
    15	    FieldCondition,
    16	    Filter as QFilter,
    17	    MatchValue,
    18	    VectorParams,
    19	)
    20	from sentence_transformers import SentenceTransformer
    21	from sentence_transformers.cross_encoder import CrossEncoder
    22	
    23	log = structlog.get_logger(__name__)
    24	
    25	# ---- Konfiguration ----
    26	QDRANT_HOST = os.getenv("QDRANT_HOST", "qdrant")
    27	QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))
    28	QDRANT_API_KEY = os.getenv("QDRANT_API_KEY") or None
    29	
    30	COLL_PREFIX = os.getenv("QDRANT_COLLECTION_PREFIX", "sealai-docs")
    31	DEFAULT_COLL = os.getenv("QDRANT_DEFAULT_COLLECTION", COLL_PREFIX)
    32	
    33	EMB_MODEL_NAME = os.getenv("EMB_MODEL_NAME", "intfloat/multilingual-e5-base")
    34	RERANK_MODEL_NAME = os.getenv("RERANK_MODEL_NAME", "cross-encoder/ms-marco-MiniLM-L-6-v2")
    35	
    36	HYBRID_K = int(os.getenv("RAG_HYBRID_K", "12"))
    37	FINAL_K = int(os.getenv("RAG_FINAL_K", "6"))
    38	RRF_K = int(os.getenv("RAG_RRF_K", "60"))
    39	VEC_SCORE_THRESHOLD = float(os.getenv("RAG_SCORE_THRESHOLD", "0.0"))
    40	
    41	TENANT_FIELD = os.getenv("RAG_TENANT_FIELD", "tenant")
    42	REDIS_BM25_INDEX = os.getenv("REDIS_BM25_INDEX", "sealai:docs")
    43	REDIS_URL = os.getenv("REDIS_URL", "redis://redis:6379/0")
    44	
    45	# ---- Lazy Singletons ----
    46	_qdrant: Optional[QdrantClient] = None
    47	_emb: Optional[SentenceTransformer] = None
    48	_reranker: Optional[CrossEncoder] = None
    49	_redis_search: Optional[Dict[str, Any]] = None  # {"mode":"raw"/"redisvl", ...}
    50	
    51	@dataclass
    52	class RetrievedDoc:
    53	    id: str
    54	    text: str
    55	    source: Optional[str] = None
    56	    metadata: Optional[Dict[str, Any]] = None
    57	    vector_score: Optional[float] = None
    58	    keyword_score: Optional[float] = None
    59	    fused_score: Optional[float] = None
    60	    def to_payload(self) -> Dict[str, Any]:
    61	        d = asdict(self)
    62	        if self.metadata and len(str(self.metadata)) > 5000:
    63	            d["metadata"] = {"_truncated": True}
    64	        return d
    65	
    66	# ---------------- Qdrant / Embeddings ----------------
    67	def _get_qdrant() -> QdrantClient:
    68	    global _qdrant
    69	    if _qdrant is None:
    70	        _qdrant = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT, api_key=QDRANT_API_KEY, timeout=30.0)
    71	    return _qdrant
    72	
    73	def _get_emb() -> SentenceTransformer:
    74	    global _emb
    75	    if _emb is None:
    76	        t0 = time.time()
    77	        _emb = SentenceTransformer(EMB_MODEL_NAME, device="cpu")
    78	        log.info("embeddings_loaded", model=EMB_MODEL_NAME, ms=round((time.time() - t0) * 1000))
    79	    return _emb
    80	
    81	def _get_reranker() -> CrossEncoder:
    82	    global _reranker
    83	    if _reranker is None:
    84	        t0 = time.time()
    85	        _reranker = CrossEncoder(RERANK_MODEL_NAME)
    86	        log.info("reranker_loaded", model=RERANK_MODEL_NAME, ms=round((time.time() - t0) * 1000))
    87	    return _reranker
    88	
    89	def _collection_for(tenant: Optional[str]) -> str:
    90	    return f"{COLL_PREFIX}-{tenant}" if tenant else DEFAULT_COLL
    91	
    92	def _embed(texts: List[str]) -> List[List[float]]:
    93	    emb = _get_emb()
    94	    prepped = [f"query: {t}" for t in texts]
    95	    return emb.encode(prepped, normalize_embeddings=True).tolist()
    96	
    97	def _ensure_collection(coll: str) -> None:
    98	    client = _get_qdrant()
    99	    try:
   100	        names = {c.name for c in (client.get_collections().collections or [])}
   101	        if coll in names:
   102	            return
   103	    except Exception:
   104	        pass
   105	    dim = len(_embed(["ping"])[0])
   106	    log.info("qdrant_create_collection", collection=coll, dim=dim)
   107	    client.create_collection(collection_name=coll, vectors_config=VectorParams(size=dim, distance=Distance.COSINE))
   108	    for field in (TENANT_FIELD, "material", "profile", "domain", "norm", "lang", "source", "doc_sha1"):
   109	        try:
   110	            client.create_payload_index(collection_name=coll, field_name=field, field_schema="keyword")
   111	        except Exception:
   112	            pass
   113	
   114	def _qdrant_vector_search(query: str, tenant: Optional[str], k: int, metadata_filters: Optional[Dict[str, Any]]) -> List[RetrievedDoc]:
   115	    client = _get_qdrant()
   116	    coll = _collection_for(tenant)
   117	    _ensure_collection(coll)
   118	    vec = _embed([query])[0]
   119	    conditions: List[FieldCondition] = []
   120	    if tenant:
   121	        conditions.append(FieldCondition(key=TENANT_FIELD, match=MatchValue(value=tenant)))
   122	    if metadata_filters:
   123	        for kf, val in metadata_filters.items():
   124	            conditions.append(FieldCondition(key=kf, match=MatchValue(value=val)))
   125	    qfilter: Optional[QFilter] = QFilter(must=conditions) if conditions else None
   126	    try:
   127	        hits = client.search(
   128	            collection_name=coll,
   129	            query_vector=vec,
   130	            limit=k,
   131	            with_payload=True,
   132	            with_vectors=False,
   133	            score_threshold=VEC_SCORE_THRESHOLD if VEC_SCORE_THRESHOLD > 0 else None,
   134	            query_filter=qfilter,
   135	        )
   136	    except Exception as e:
   137	        log.warning("qdrant_search_failed", collection=coll, error=str(e))
   138	        return []
   139	    out: List[RetrievedDoc] = []
   140	    for h in hits:
   141	        payload = h.payload or {}
   142	        text = payload.get("text") or payload.get("content") or ""
   143	        src = payload.get("source") or payload.get("doc_uri") or payload.get("path")
   144	        out.append(
   145	            RetrievedDoc(
   146	                id=str(h.id),
   147	                text=text,
   148	                source=src,
   149	                metadata=payload,
   150	                vector_score=float(h.score) if h.score is not None else None,
   151	            )
   152	        )
   153	    return out
   154	
   155	# ---------------- Redis BM25 ----------------
   156	def _maybe_bind_redis_index() -> None:
   157	    global _redis_search
   158	    if _redis_search is not None:
   159	        return
   160	    index_name = (os.getenv("REDIS_BM25_INDEX") or REDIS_BM25_INDEX or "").strip()
   161	    if not index_name:
   162	        _redis_search = None
   163	        return
   164	    redis_url = os.getenv("REDIS_URL", REDIS_URL)
   165	    try:
   166	        from redisvl.index import SearchIndex  # type: ignore
   167	        try:
   168	            idx = SearchIndex.from_existing(index_name, redis_url=redis_url)
   169	        except Exception:
   170	            idx = SearchIndex.from_existing(index_name)
   171	            idx.connect(redis_url)
   172	        _redis_search = {"mode": "redisvl", "index": idx}
   173	        log.info("redis_bm25_bound", mode="redisvl", index=index_name, url=redis_url)
   174	        return
   175	    except Exception as e:
   176	        log.info("redis_bm25_unavailable", reason=str(e))
   177	    try:
   178	        import redis  # type: ignore
   179	        r = redis.Redis.from_url(redis_url)
   180	        r.execute_command("FT.INFO", index_name)
   181	        _redis_search = {"mode": "raw", "client": r, "name": index_name}
   182	        log.info("redis_bm25_bound", mode="raw", index=index_name, url=redis_url)
   183	    except Exception as e:
   184	        log.info("redis_bm25_unavailable", reason=str(e))
   185	        _redis_search = None
   186	
   187	def _normalize_redisvl_result(res: Any) -> List[Dict[str, Any]]:
   188	    docs: List[Any] = []
   189	    if isinstance(res, dict):
   190	        for key in ("documents", "docs", "results", "data"):
   191	            if key in res and isinstance(res[key], (list, tuple)):
   192	                docs = list(res[key]); break
   193	        else:
   194	            for v in res.values():
   195	                if isinstance(v, (list, tuple)) and v and isinstance(v[0], (dict, object)):
   196	                    docs = list(v); break
   197	    elif isinstance(res, (list, tuple)):
   198	        docs = list(res)
   199	    else:
   200	        for attr in ("documents", "docs", "results", "data"):
   201	            if hasattr(res, attr):
   202	                maybe = getattr(res, attr)
   203	                if isinstance(maybe, (list, tuple)):
   204	                    docs = list(maybe); break
   205	    out: List[Dict[str, Any]] = []
   206	    for d in docs:
   207	        if isinstance(d, dict):
   208	            out.append(d); continue
   209	        tmp: Dict[str, Any] = {}
   210	        for k in ("id", "pk", "text", "source", "__score", TENANT_FIELD):
   211	            if hasattr(d, k):
   212	                tmp[k] = getattr(d, k)
   213	        if hasattr(d, "payload") and isinstance(getattr(d, "payload"), dict):
   214	            tmp.update(getattr(d, "payload"))
   215	        if not tmp and hasattr(d, "__dict__"):
   216	            tmp.update({k: v for k, v in d.__dict__.items() if not k.startswith("_")})
   217	        out.append(tmp)
   218	    return out
   219	
   220	def _redisvl_search(idx, q: str, k: int, return_fields: List[str]) -> List[Dict[str, Any]]:
   221	    variants = [
   222	        {"num_results": k, "return_fields": return_fields},
   223	        {"top_k": k, "return_fields": return_fields},
   224	        {"k": k, "return_fields": return_fields},
   225	        {"paging": {"offset": 0, "limit": k}, "return_fields": return_fields},
   226	        {"return_fields": return_fields},
   227	    ]
   228	    last_err: Optional[Exception] = None
   229	    for kwargs in variants:
   230	        try:
   231	            res = idx.search(query=q, **kwargs)  # type: ignore
   232	            docs = _normalize_redisvl_result(res)
   233	            return docs[:k]
   234	        except TypeError as e:
   235	            last_err = e; continue
   236	        except Exception as e:
   237	            last_err = e; continue
   238	    try:
   239	        res = idx.search(q)  # type: ignore
   240	        docs = _normalize_redisvl_result(res)
   241	        return docs[:k]
   242	    except Exception as e:
   243	        raise RuntimeError(f"Unexpected error while searching: {e}") from last_err
   244	
   245	# NEW: defensives Quoten/Escapen – RediSearch-Sonderzeichen
   246	_RS_SPECIAL = re.compile(r'([\-@{}\[\]:"|><()~*?+^$\\])')
   247	def _sanitize_redis_query(q: str) -> str:
   248	    q = q.replace('"', r"\"")
   249	    q = _RS_SPECIAL.sub(r"\\\1", q)
   250	    return f"\"{q}\""  # als Phrase
   251	
   252	def _redis_bm25_search(query: str, k: int, tenant: Optional[str]) -> List[RetrievedDoc]:
   253	    _maybe_bind_redis_index()
   254	    if _redis_search is None:
   255	        return []
   256	    user_q = _sanitize_redis_query(query)
   257	    q = f'@{TENANT_FIELD}:{{{tenant}}} {user_q}' if tenant else user_q
   258	    rf = ["id", "text", "source", TENANT_FIELD]
   259	    try:
   260	        if _redis_search.get("mode") == "redisvl":
   261	            idx = _redis_search["index"]
   262	            rows = _redisvl_search(idx, q, k, rf)
   263	            docs: List[RetrievedDoc] = []
   264	            for r in rows:
   265	                docs.append(
   266	                    RetrievedDoc(
   267	                        id=str(r.get("id") or r.get("pk") or ""),
   268	                        text=r.get("text") or "",
   269	                        source=r.get("source"),
   270	                        keyword_score=float(r.get("__score", 0.0)) if "__score" in r else None,
   271	                        metadata={kk: vv for kk, vv in r.items() if kk not in {"id", "text", "source"}},
   272	                    )
   273	                )
   274	            return docs
   275	        r = _redis_search["client"]
   276	        name = _redis_search["name"]
   277	        res = r.execute_command("FT.SEARCH", name, q, "RETURN", 3, "text", "source", TENANT_FIELD, "LIMIT", 0, k)
   278	        if not res or len(res) < 2:
   279	            return []
   280	        items = res[1:]; docs: List[RetrievedDoc] = []
   281	        def _dec(x): return x.decode("utf-8") if isinstance(x, (bytes, bytearray)) else str(x)
   282	        for i in range(0, len(items), 2):
   283	            doc_id = _dec(items[i])
   284	            fields = items[i + 1]
   285	            data = {_dec(fields[j]): _dec(fields[j + 1]) for j in range(0, len(fields), 2)}
   286	            docs.append(RetrievedDoc(id=doc_id, text=data.get("text", ""), source=data.get("source"), keyword_score=None, metadata=data))
   287	        return docs
   288	    except Exception as e:
   289	        log.info("redis_search_failed", reason=str(e))
   290	        return []
   291	
   292	# ---------------- Fusion & Reranking ----------------
   293	def _rrf_fuse(vector_docs: List[RetrievedDoc], keyword_docs: List[RetrievedDoc], rrf_k: int, final_k: int) -> List[RetrievedDoc]:
   294	    by_id: Dict[str, RetrievedDoc] = {}
   295	    ranks: Dict[str, float] = {}
   296	    def add_rank(items: List[RetrievedDoc], weight: float = 1.0):
   297	        for idx, d in enumerate(items):
   298	            rid = d.id or f"{hash(d.text)}"
   299	            if rid not in by_id:
   300	                by_id[rid] = d
   301	            ranks[rid] = ranks.get(rid, 0.0) + weight * (1.0 / (rrf_k + (idx + 1)))
   302	    add_rank(vector_docs, 1.0); add_rank(keyword_docs, 1.0)
   303	    fused: List[RetrievedDoc] = []
   304	    for rid, score in sorted(ranks.items(), key=lambda x: x[1], reverse=True):
   305	        doc = by_id[rid]; doc.fused_score = score; fused.append(doc)
   306	    return fused[:final_k]
   307	
   308	def _logistic(x: float) -> float: return 1.0 / (1.0 + math.exp(-x))
   309	
   310	def _rerank(query: str, docs: List[RetrievedDoc]) -> List[RetrievedDoc]:
   311	    if not docs: return docs
   312	    try:
   313	        rer = _get_reranker()
   314	        pairs = [(query, d.text) for d in docs]
   315	        scores = rer.predict(pairs).tolist()
   316	        for d, s in zip(docs, scores):
   317	            d.fused_score = _logistic(float(s))
   318	        docs.sort(key=lambda d: d.fused_score or 0.0, reverse=True)
   319	    except Exception as e:
   320	        log.info("rerank_failed", reason=str(e))
   321	    return docs
   322	
   323	# ---------------- Public API ----------------
   324	def hybrid_retrieve(query: str, tenant: Optional[str], k: int = FINAL_K, metadata_filters: Optional[Dict[str, Any]] = None, use_rerank: bool = True) -> List[Dict[str, Any]]:
   325	    t0 = time.time()
   326	    vec_docs = _qdrant_vector_search(query, tenant=tenant, k=HYBRID_K, metadata_filters=metadata_filters)
   327	    kw_docs = _redis_bm25_search(query, k=HYBRID_K, tenant=tenant)
   328	    fused = _rrf_fuse(vec_docs, kw_docs, rrf_k=RRF_K, final_k=k)
   329	    if use_rerank:
   330	        fused = _rerank(query, fused)
   331	    ms = round((time.time() - t0) * 1000)
   332	    log.info("hybrid_retrieve", tenant=tenant, q=query[:120], vec=len(vec_docs), kw=len(kw_docs), fused=len(fused), ms=ms)
   333	    return [d.to_payload() for d in fused]
   334	
   335	# --------- Warmup (beim App-Start aufrufen) ---------
   336	def prewarm() -> None:
   337	    """Lädt Redis/Qdrant/Embedding/Reranker einmalig in den Speicher."""
   338	    try: _maybe_bind_redis_index()
   339	    except Exception: pass
   340	    try: _get_qdrant()
   341	    except Exception: pass
   342	    try: _get_emb()
   343	    except Exception: pass
   344	    try: _get_reranker()
   345	    except Exception: pass

======================== NODE/EDGE-INDEX (grep) ========================
backend/app/services/langgraph/graph/orchestrator.py:12:    g.add_node("intake_validate", intake_validate)
backend/app/services/langgraph/graph/orchestrator.py:13:    g.add_node("calc_core", calc_core)
backend/app/services/langgraph/graph/orchestrator.py:14:    g.add_node("calc_advanced", calc_advanced)
backend/app/services/langgraph/graph/orchestrator.py:15:    g.add_node("rag_retrieve", rag_retrieve)
backend/app/services/langgraph/graph/orchestrator.py:16:    g.add_node("partner_only_filter", partner_only_filter)
backend/app/services/langgraph/graph/orchestrator.py:17:    g.add_node("rules_filter", rules_filter)
backend/app/services/langgraph/graph/orchestrator.py:18:    g.add_node("explain", explain)
backend/app/services/langgraph/graph/orchestrator.py:19:    g.add_node("decision_ready", decision_ready)
backend/app/services/langgraph/graph/orchestrator.py:20:    g.add_node("await_user_action", await_user_action)
backend/app/services/langgraph/graph/orchestrator.py:21:    g.add_node("generate_rfq_pdf", generate_rfq_pdf_node)
backend/app/services/langgraph/graph/orchestrator.py:22:    g.add_node("deliver_pdf", deliver_pdf)
backend/app/services/langgraph/graph/orchestrator.py:25:    g.add_edge("intake_validate", "calc_core")
backend/app/services/langgraph/graph/orchestrator.py:26:    g.add_edge("calc_core", "calc_advanced")
backend/app/services/langgraph/graph/orchestrator.py:27:    g.add_edge("calc_advanced", "rag_retrieve")
backend/app/services/langgraph/graph/orchestrator.py:28:    g.add_edge("rag_retrieve", "partner_only_filter")
backend/app/services/langgraph/graph/orchestrator.py:29:    g.add_edge("partner_only_filter", "rules_filter")
backend/app/services/langgraph/graph/orchestrator.py:30:    g.add_edge("rules_filter", "explain")
backend/app/services/langgraph/graph/orchestrator.py:31:    g.add_edge("explain", "decision_ready")
backend/app/services/langgraph/graph/orchestrator.py:32:    g.add_edge("decision_ready", "await_user_action")
backend/app/services/langgraph/graph/orchestrator.py:33:    g.add_edge("await_user_action", "generate_rfq_pdf")
backend/app/services/langgraph/graph/orchestrator.py:34:    g.add_edge("generate_rfq_pdf", "deliver_pdf")
backend/app/services/langgraph/graph/orchestrator.py:35:    g.add_edge("deliver_pdf", END)
backend/app/services/langgraph/graph/consult/build.py:206:    g.add_node("lite_router", lite_router_node)  # NEU
backend/app/services/langgraph/graph/consult/build.py:207:    g.add_node("smalltalk", smalltalk_node)      # NEU
backend/app/services/langgraph/graph/consult/build.py:209:    g.add_node("intake", intake_node)
backend/app/services/langgraph/graph/consult/build.py:210:    g.add_node("extract", _extract_node)
backend/app/services/langgraph/graph/consult/build.py:211:    g.add_node("domain_router", _domain_router_node)
backend/app/services/langgraph/graph/consult/build.py:212:    g.add_node("compute", _compute_node)
backend/app/services/langgraph/graph/consult/build.py:213:    g.add_node("calc_agent", calc_agent_node)
backend/app/services/langgraph/graph/consult/build.py:214:    g.add_node("ask_missing", ask_missing_node)
backend/app/services/langgraph/graph/consult/build.py:215:    g.add_node("validate", validate_node)
backend/app/services/langgraph/graph/consult/build.py:216:    g.add_node("prepare_query", _prepare_query_node)
backend/app/services/langgraph/graph/consult/build.py:217:    g.add_node("rag", run_rag_node)
backend/app/services/langgraph/graph/consult/build.py:218:    g.add_node("recommend", recommend_node)
backend/app/services/langgraph/graph/consult/build.py:219:    g.add_node("validate_answer", validate_answer)
backend/app/services/langgraph/graph/consult/build.py:220:    g.add_node("explain", explain_node)
backend/app/services/langgraph/graph/consult/build.py:221:    g.add_node("respond", _respond_node)
backend/app/services/langgraph/graph/consult/build.py:232:    g.add_edge("intake", "extract")
backend/app/services/langgraph/graph/consult/build.py:233:    g.add_edge("extract", "domain_router")
backend/app/services/langgraph/graph/consult/build.py:234:    g.add_edge("domain_router", "compute")
backend/app/services/langgraph/graph/consult/build.py:235:    g.add_edge("compute", "calc_agent")
backend/app/services/langgraph/graph/consult/build.py:236:    g.add_edge("calc_agent", "ask_missing")
backend/app/services/langgraph/graph/consult/build.py:244:    g.add_edge("validate", "prepare_query")
backend/app/services/langgraph/graph/consult/build.py:245:    g.add_edge("prepare_query", "rag")
backend/app/services/langgraph/graph/consult/build.py:250:    g.add_edge("recommend", "validate_answer")
backend/app/services/langgraph/graph/consult/build.py:251:    g.add_edge("validate_answer", "explain")
backend/app/services/langgraph/graph/consult/build.py:252:    g.add_edge("explain", "respond")
backend/app/services/langgraph/graph/consult/build.py:253:    g.add_edge("respond", END)
backend/app/services/langgraph/graph/supervisor_graph.py:81:    builder.add_node("router", router_node)
backend/app/services/langgraph/graph/supervisor_graph.py:82:    builder.add_node("chitchat", chitchat_node)
backend/app/services/langgraph/graph/supervisor_graph.py:83:    builder.add_node("consult", consult_node)
backend/app/services/langgraph/graph/supervisor_graph.py:92:    builder.add_edge("consult", END)
backend/app/services/langgraph/graph/supervisor_graph.py:93:    builder.add_edge("chitchat", END)
